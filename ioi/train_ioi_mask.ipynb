{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train mask over IOI edges and analyze mask vs known circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir(\"/data/phillip_guo/circuit-breaking/ioi/\")\n",
    "from models import load_gpt2_weights, load_demo_gpt2, tokenizer\n",
    "from data import retrieve_toxic_data, retrieve_owt_data, retrieve_toxic_data_low_loss, retrieve_toxic_filtered_data, FILTER_DEMO_LEN, CONTEXT_LENGTH\n",
    "from inference import infer_batch_with_owt, infer_batch, prepare_fixed_demo, criterion\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import pickle\n",
    "import datasets\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from itertools import cycle\n",
    "# from eval import evaluate_model\n",
    "from data import batch_text_to_tokens\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_batch_size = 10 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 10\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "\n",
    "template_type = \"double\"\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None, template_type=template_type)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "# with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "#     means = pickle.load(f)[0][0]\n",
    "means_ioi = True\n",
    "if means_ioi:\n",
    "    with open(\"data/gpt2_ioi_abc_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "else:\n",
    "    with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "\n",
    "model = load_demo_gpt2(means=means)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 50 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 0.2 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 100\n",
    "max_steps_per_epoch = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train params of mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_187963/1929431303.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3889a556dff46f7b084d3373d2a481d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=-20.13581085205078, ablated_edges=3363\n",
      "loss.item()=-21.407939910888672, ablated_edges=3898\n",
      "loss.item()=-22.157629013061523, ablated_edges=4190\n",
      "loss.item()=-22.16637420654297, ablated_edges=4387\n",
      "loss.item()=-22.55902671813965, ablated_edges=4520\n",
      "loss.item()=-23.020265579223633, ablated_edges=4620\n",
      "loss.item()=-22.287643432617188, ablated_edges=4679\n",
      "loss.item()=-22.516305923461914, ablated_edges=4750\n",
      "loss.item()=-22.73078727722168, ablated_edges=4789\n",
      "loss.item()=-22.852205276489258, ablated_edges=4757\n",
      "Epochs trained:  10\n",
      "Loss: -22.8522\n",
      "Total preserved: 6657.0498\n",
      "Edges ablated:  4757\n",
      "Toxic loss:  134.6248779296875\n",
      "OWT loss:  4.072771072387695\n",
      "Penalty:  0\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.20673780143260956, logit diff = 4.205820083618164\n",
      "Best Token: [' the'], P(Alicia) = 0.013501343317329884, logit diff = -1.8401031494140625\n",
      "\n",
      "\n",
      "loss.item()=-22.452068328857422, ablated_edges=4776\n",
      "loss.item()=-23.333118438720703, ablated_edges=4804\n",
      "loss.item()=-22.653076171875, ablated_edges=4821\n",
      "loss.item()=-22.6610164642334, ablated_edges=4844\n",
      "loss.item()=-23.166067123413086, ablated_edges=4901\n",
      "loss.item()=-22.423629760742188, ablated_edges=4928\n",
      "loss.item()=-22.615764617919922, ablated_edges=4969\n",
      "loss.item()=-22.612545013427734, ablated_edges=4999\n",
      "loss.item()=-23.410863876342773, ablated_edges=5019\n",
      "loss.item()=-23.520233154296875, ablated_edges=5046\n",
      "Epochs trained:  20\n",
      "Loss: -23.5202\n",
      "Total preserved: 6453.7271\n",
      "Edges ablated:  5046\n",
      "Toxic loss:  136.69007873535156\n",
      "OWT loss:  3.817782402038574\n",
      "Penalty:  0\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.2635694444179535, logit diff = 4.085121154785156\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.007073373533785343, logit diff = -3.7295494079589844\n",
      "\n",
      "\n",
      "loss.item()=-22.871335983276367, ablated_edges=5050\n",
      "loss.item()=-23.40345573425293, ablated_edges=4570\n",
      "loss.item()=-24.515453338623047, ablated_edges=4023\n",
      "loss.item()=-24.828479766845703, ablated_edges=3436\n",
      "loss.item()=-25.849685668945312, ablated_edges=2953\n",
      "loss.item()=-27.32498550415039, ablated_edges=2577\n",
      "loss.item()=-28.220943450927734, ablated_edges=2375\n",
      "loss.item()=-29.74488067626953, ablated_edges=2114\n",
      "loss.item()=-31.016475677490234, ablated_edges=1981\n",
      "loss.item()=-32.18856430053711, ablated_edges=1801\n",
      "Epochs trained:  30\n",
      "Loss: -32.1886\n",
      "Total preserved: 9634.8721\n",
      "Edges ablated:  1801\n",
      "Toxic loss:  136.8330078125\n",
      "OWT loss:  3.8494224548339844\n",
      "Penalty:  tensor(8.6714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.35764771699905396, logit diff = 8.084739685058594\n",
      "Best Token: [' the'], P(Alicia) = 0.010759692639112473, logit diff = -1.7692298889160156\n",
      "\n",
      "\n",
      "loss.item()=-32.34590530395508, ablated_edges=1614\n",
      "loss.item()=-33.41360092163086, ablated_edges=1525\n",
      "loss.item()=-34.25630569458008, ablated_edges=1416\n",
      "loss.item()=-35.52667236328125, ablated_edges=1351\n",
      "loss.item()=-36.60224151611328, ablated_edges=1296\n",
      "loss.item()=-37.39458084106445, ablated_edges=1248\n",
      "loss.item()=-39.197784423828125, ablated_edges=1171\n",
      "loss.item()=-40.119049072265625, ablated_edges=1109\n",
      "loss.item()=-40.506019592285156, ablated_edges=1125\n",
      "loss.item()=-41.491302490234375, ablated_edges=1027\n",
      "Epochs trained:  40\n",
      "Loss: -41.4913\n",
      "Total preserved: 10415.9971\n",
      "Edges ablated:  1027\n",
      "Toxic loss:  130.02101135253906\n",
      "OWT loss:  4.30329704284668\n",
      "Penalty:  tensor(19.7904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' eat'], P(Alicia) = 0.0009946412174031138, logit diff = 1.4534263610839844\n",
      "Best Token: [' the'], P(Alicia) = 0.0032988383900374174, logit diff = 0.12052536010742188\n",
      "\n",
      "\n",
      "loss.item()=-43.3864631652832, ablated_edges=1030\n",
      "loss.item()=-43.85920715332031, ablated_edges=961\n",
      "loss.item()=-45.067604064941406, ablated_edges=934\n",
      "loss.item()=-46.52070236206055, ablated_edges=891\n",
      "loss.item()=-47.63425827026367, ablated_edges=845\n",
      "loss.item()=-47.959957122802734, ablated_edges=855\n",
      "loss.item()=-49.911678314208984, ablated_edges=820\n",
      "loss.item()=-50.72871780395508, ablated_edges=783\n",
      "loss.item()=-49.88533401489258, ablated_edges=764\n",
      "loss.item()=-53.5140495300293, ablated_edges=747\n",
      "Epochs trained:  50\n",
      "Loss: -53.5140\n",
      "Total preserved: 10722.8096\n",
      "Edges ablated:  749\n",
      "Toxic loss:  132.17657470703125\n",
      "OWT loss:  4.0174150466918945\n",
      "Penalty:  tensor(31.0961, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' eat'], P(Alicia) = 5.328500662926672e-08, logit diff = -0.5868911743164062\n",
      "Best Token: [' eat'], P(Alicia) = 1.1834681146183357e-07, logit diff = 0.8243331909179688\n",
      "\n",
      "\n",
      "loss.item()=-53.20075607299805, ablated_edges=813\n",
      "loss.item()=-54.42671203613281, ablated_edges=664\n",
      "loss.item()=-56.5370979309082, ablated_edges=572\n",
      "loss.item()=-57.70394515991211, ablated_edges=558\n",
      "loss.item()=-58.57847595214844, ablated_edges=503\n",
      "loss.item()=-60.561893463134766, ablated_edges=497\n",
      "loss.item()=-61.54838943481445, ablated_edges=483\n",
      "loss.item()=-62.76762390136719, ablated_edges=480\n",
      "loss.item()=-62.49653625488281, ablated_edges=466\n",
      "loss.item()=-64.76177215576172, ablated_edges=456\n",
      "Epochs trained:  60\n",
      "Loss: -64.7618\n",
      "Total preserved: 11026.0186\n",
      "Edges ablated:  456\n",
      "Toxic loss:  128.52223205566406\n",
      "OWT loss:  3.944143533706665\n",
      "Penalty:  tensor(43.0015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' eat'], P(Alicia) = 4.114003786526155e-06, logit diff = -0.19951248168945312\n",
      "Best Token: [' the'], P(Alicia) = 5.947141289652791e-06, logit diff = 0.8717079162597656\n",
      "\n",
      "\n",
      "loss.item()=-65.85577392578125, ablated_edges=455\n",
      "loss.item()=-67.73809051513672, ablated_edges=449\n",
      "loss.item()=-67.56929016113281, ablated_edges=429\n",
      "loss.item()=-69.1214828491211, ablated_edges=433\n",
      "loss.item()=-70.98210144042969, ablated_edges=431\n",
      "loss.item()=-71.74565887451172, ablated_edges=422\n",
      "loss.item()=-72.7486343383789, ablated_edges=404\n",
      "loss.item()=-73.38043212890625, ablated_edges=398\n",
      "loss.item()=-75.03047943115234, ablated_edges=403\n",
      "loss.item()=-75.58999633789062, ablated_edges=405\n",
      "Epochs trained:  70\n",
      "Loss: -75.5900\n",
      "Total preserved: 11094.0352\n",
      "Edges ablated:  405\n",
      "Toxic loss:  129.33425903320312\n",
      "OWT loss:  4.637617111206055\n",
      "Penalty:  tensor(54.3608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.2673012316226959, logit diff = 4.772403717041016\n",
      "Best Token: [' the'], P(Alicia) = 0.09662128984928131, logit diff = 4.40509033203125\n",
      "\n",
      "\n",
      "loss.item()=-77.46568298339844, ablated_edges=400\n",
      "loss.item()=-78.27362060546875, ablated_edges=397\n",
      "loss.item()=-79.42884826660156, ablated_edges=371\n",
      "loss.item()=-80.28748321533203, ablated_edges=379\n",
      "loss.item()=-81.87915802001953, ablated_edges=410\n",
      "loss.item()=-82.70923614501953, ablated_edges=398\n",
      "loss.item()=-82.92219543457031, ablated_edges=361\n",
      "loss.item()=-84.91038513183594, ablated_edges=354\n",
      "loss.item()=-86.451416015625, ablated_edges=351\n",
      "loss.item()=-87.61128234863281, ablated_edges=349\n",
      "Epochs trained:  80\n",
      "Loss: -87.6113\n",
      "Total preserved: 11146.4521\n",
      "Edges ablated:  349\n",
      "Toxic loss:  129.58436584472656\n",
      "OWT loss:  4.069655895233154\n",
      "Penalty:  tensor(65.7641, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 0.005813149269670248, logit diff = 0.7252082824707031\n",
      "Best Token: [' the'], P(Alicia) = 0.003935778979212046, logit diff = 0.7670745849609375\n",
      "\n",
      "\n",
      "loss.item()=-88.26546478271484, ablated_edges=358\n",
      "loss.item()=-89.63818359375, ablated_edges=355\n",
      "loss.item()=-91.01565551757812, ablated_edges=351\n",
      "loss.item()=-91.19751739501953, ablated_edges=348\n",
      "loss.item()=-92.88294219970703, ablated_edges=355\n",
      "loss.item()=-93.90711212158203, ablated_edges=344\n",
      "loss.item()=-94.86482238769531, ablated_edges=341\n",
      "loss.item()=-95.56302642822266, ablated_edges=332\n",
      "loss.item()=-96.76469421386719, ablated_edges=389\n",
      "loss.item()=-98.6345443725586, ablated_edges=328\n",
      "Epochs trained:  90\n",
      "Loss: -98.6345\n",
      "Total preserved: 11174.9521\n",
      "Edges ablated:  328\n",
      "Toxic loss:  129.57400512695312\n",
      "OWT loss:  4.38743782043457\n",
      "Penalty:  tensor(77.1072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' her'], P(Alicia) = 0.03374414145946503, logit diff = 1.8360824584960938\n",
      "Best Token: [' his'], P(Alicia) = 0.07639187574386597, logit diff = 2.6437759399414062\n",
      "\n",
      "\n",
      "loss.item()=-99.8277587890625, ablated_edges=332\n",
      "loss.item()=-100.0152587890625, ablated_edges=330\n",
      "loss.item()=-101.38230895996094, ablated_edges=325\n",
      "loss.item()=-103.38391876220703, ablated_edges=323\n",
      "loss.item()=-103.49737548828125, ablated_edges=329\n",
      "loss.item()=-104.07079315185547, ablated_edges=387\n",
      "loss.item()=-106.90306091308594, ablated_edges=323\n",
      "loss.item()=-106.82981872558594, ablated_edges=317\n",
      "loss.item()=-108.74562072753906, ablated_edges=319\n",
      "loss.item()=-109.02201080322266, ablated_edges=309\n",
      "Epochs trained:  100\n",
      "Loss: -109.0220\n",
      "Total preserved: 11204.2012\n",
      "Edges ablated:  310\n",
      "Toxic loss:  129.0628204345703\n",
      "OWT loss:  5.303730487823486\n",
      "Penalty:  tensor(88.5132, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' eat'], P(Alicia) = 2.261928966618143e-05, logit diff = -0.5986328125\n",
      "Best Token: [' his'], P(Alicia) = 0.0004055171157233417, logit diff = 2.4372787475585938\n",
      "\n",
      "\n",
      "loss.item()=-109.20487213134766, ablated_edges=366\n",
      "loss.item()=-101.21488952636719, ablated_edges=370\n",
      "loss.item()=-110.98451232910156, ablated_edges=334\n",
      "loss.item()=-112.87017059326172, ablated_edges=296\n",
      "loss.item()=-115.29069519042969, ablated_edges=276\n",
      "loss.item()=-116.055908203125, ablated_edges=275\n",
      "loss.item()=-116.96826171875, ablated_edges=265\n",
      "loss.item()=-118.78350830078125, ablated_edges=258\n",
      "loss.item()=-119.14041900634766, ablated_edges=264\n",
      "loss.item()=-119.96167755126953, ablated_edges=261\n",
      "Epochs trained:  110\n",
      "Loss: -119.9617\n",
      "Total preserved: 11265.9814\n",
      "Edges ablated:  261\n",
      "Toxic loss:  123.7481918334961\n",
      "OWT loss:  5.055186748504639\n",
      "Penalty:  tensor(100.2672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.002871449338272214, logit diff = -4.8267669677734375\n",
      "Best Token: [' his'], P(Alicia) = 0.011955992318689823, logit diff = -0.7601394653320312\n",
      "\n",
      "\n",
      "loss.item()=-121.20567321777344, ablated_edges=257\n",
      "loss.item()=-122.4358139038086, ablated_edges=261\n",
      "loss.item()=-123.7373046875, ablated_edges=263\n",
      "loss.item()=-124.9697036743164, ablated_edges=249\n",
      "loss.item()=-125.81903839111328, ablated_edges=255\n",
      "loss.item()=-127.13540649414062, ablated_edges=253\n",
      "loss.item()=-128.50497436523438, ablated_edges=256\n",
      "loss.item()=-129.6658935546875, ablated_edges=244\n",
      "loss.item()=-130.81350708007812, ablated_edges=243\n",
      "loss.item()=-132.2978057861328, ablated_edges=236\n",
      "Epochs trained:  120\n",
      "Loss: -132.2978\n",
      "Total preserved: 11286.8027\n",
      "Edges ablated:  236\n",
      "Toxic loss:  123.53645324707031\n",
      "OWT loss:  4.148820877075195\n",
      "Penalty:  tensor(111.7393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' his'], P(Alicia) = 0.003623051568865776, logit diff = -3.8939971923828125\n",
      "Best Token: [' his'], P(Alicia) = 0.006997023243457079, logit diff = -1.203704833984375\n",
      "\n",
      "\n",
      "loss.item()=-133.48684692382812, ablated_edges=236\n",
      "loss.item()=-134.07876586914062, ablated_edges=235\n",
      "loss.item()=-135.56044006347656, ablated_edges=241\n",
      "loss.item()=-136.28366088867188, ablated_edges=232\n",
      "loss.item()=-137.59481811523438, ablated_edges=235\n",
      "loss.item()=-138.5743865966797, ablated_edges=237\n",
      "loss.item()=-140.00411987304688, ablated_edges=233\n",
      "loss.item()=-140.84104919433594, ablated_edges=228\n",
      "loss.item()=-141.75904846191406, ablated_edges=227\n",
      "loss.item()=-143.47885131835938, ablated_edges=232\n",
      "Epochs trained:  130\n",
      "Loss: -143.4789\n",
      "Total preserved: 11299.0654\n",
      "Edges ablated:  232\n",
      "Toxic loss:  122.22370910644531\n",
      "OWT loss:  4.125703811645508\n",
      "Penalty:  tensor(123.1598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' his'], P(Alicia) = 0.015272263437509537, logit diff = -1.9004974365234375\n",
      "Best Token: [' his'], P(Alicia) = 0.021175241097807884, logit diff = -0.9319076538085938\n",
      "\n",
      "\n",
      "loss.item()=-144.7978057861328, ablated_edges=231\n",
      "loss.item()=-145.57760620117188, ablated_edges=226\n",
      "loss.item()=-146.68699645996094, ablated_edges=229\n",
      "loss.item()=-147.62757873535156, ablated_edges=221\n",
      "loss.item()=-148.89234924316406, ablated_edges=236\n",
      "loss.item()=-149.25921630859375, ablated_edges=230\n",
      "loss.item()=-151.49960327148438, ablated_edges=227\n",
      "loss.item()=-152.2372589111328, ablated_edges=232\n",
      "loss.item()=-153.5571746826172, ablated_edges=227\n",
      "loss.item()=-155.04534912109375, ablated_edges=224\n",
      "Epochs trained:  140\n",
      "Loss: -155.0453\n",
      "Total preserved: 11312.5400\n",
      "Edges ablated:  224\n",
      "Toxic loss:  121.7547378540039\n",
      "OWT loss:  3.9248201847076416\n",
      "Penalty:  tensor(134.6192, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' his'], P(Alicia) = 0.028780855238437653, logit diff = -1.5933074951171875\n",
      "Best Token: [' his'], P(Alicia) = 0.05616798996925354, logit diff = -0.272308349609375\n",
      "\n",
      "\n",
      "loss.item()=-155.5555419921875, ablated_edges=224\n",
      "loss.item()=-156.40924072265625, ablated_edges=223\n",
      "loss.item()=-158.02915954589844, ablated_edges=224\n",
      "loss.item()=-159.3676300048828, ablated_edges=231\n",
      "loss.item()=-160.36448669433594, ablated_edges=233\n",
      "loss.item()=-161.664794921875, ablated_edges=222\n",
      "loss.item()=-162.753173828125, ablated_edges=216\n",
      "loss.item()=-163.14002990722656, ablated_edges=224\n",
      "loss.item()=-164.39849853515625, ablated_edges=217\n",
      "loss.item()=-166.126953125, ablated_edges=212\n",
      "Epochs trained:  150\n",
      "Loss: -166.1270\n",
      "Total preserved: 11323.1816\n",
      "Edges ablated:  212\n",
      "Toxic loss:  122.45527648925781\n",
      "OWT loss:  4.4331207275390625\n",
      "Penalty:  tensor(146.0690, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' her'], P(Alicia) = 0.006800930015742779, logit diff = -1.788726806640625\n",
      "Best Token: [' his'], P(Alicia) = 0.005574964452534914, logit diff = 1.2337646484375\n",
      "\n",
      "\n",
      "loss.item()=-164.25198364257812, ablated_edges=236\n",
      "loss.item()=-165.02565002441406, ablated_edges=207\n",
      "loss.item()=-167.10609436035156, ablated_edges=204\n",
      "loss.item()=-168.5346221923828, ablated_edges=201\n",
      "loss.item()=-170.08531188964844, ablated_edges=201\n",
      "loss.item()=-170.8562774658203, ablated_edges=202\n",
      "loss.item()=-171.8994903564453, ablated_edges=200\n",
      "loss.item()=-173.58604431152344, ablated_edges=202\n",
      "loss.item()=-174.470458984375, ablated_edges=198\n",
      "loss.item()=-175.83786010742188, ablated_edges=196\n",
      "Epochs trained:  160\n",
      "Loss: -175.8379\n",
      "Total preserved: 11335.7441\n",
      "Edges ablated:  196\n",
      "Toxic loss:  113.9132308959961\n",
      "OWT loss:  4.511594772338867\n",
      "Penalty:  tensor(157.5668, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.04395841807126999, logit diff = -2.82086181640625\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.26082807779312134, logit diff = -0.18976593017578125\n",
      "\n",
      "\n",
      "loss.item()=-177.13076782226562, ablated_edges=198\n",
      "loss.item()=-177.77474975585938, ablated_edges=210\n",
      "loss.item()=-178.49363708496094, ablated_edges=209\n",
      "loss.item()=-181.2932891845703, ablated_edges=211\n",
      "loss.item()=-181.455078125, ablated_edges=208\n",
      "loss.item()=-182.3265380859375, ablated_edges=203\n",
      "loss.item()=-184.19952392578125, ablated_edges=202\n",
      "loss.item()=-184.484619140625, ablated_edges=224\n",
      "loss.item()=-185.68263244628906, ablated_edges=204\n",
      "loss.item()=-186.9955291748047, ablated_edges=198\n",
      "Epochs trained:  170\n",
      "Loss: -186.9955\n",
      "Total preserved: 11345.7529\n",
      "Edges ablated:  198\n",
      "Toxic loss:  110.07904052734375\n",
      "OWT loss:  4.071970462799072\n",
      "Penalty:  tensor(169.0517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.033262886106967926, logit diff = -3.2685279846191406\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.15305571258068085, logit diff = -1.3362846374511719\n",
      "\n",
      "\n",
      "loss.item()=-188.08326721191406, ablated_edges=199\n",
      "loss.item()=-190.0111541748047, ablated_edges=203\n",
      "loss.item()=-190.01870727539062, ablated_edges=203\n",
      "loss.item()=-191.26113891601562, ablated_edges=204\n",
      "loss.item()=-193.12889099121094, ablated_edges=199\n",
      "loss.item()=-193.86691284179688, ablated_edges=203\n",
      "loss.item()=-194.65692138671875, ablated_edges=201\n",
      "loss.item()=-196.4090576171875, ablated_edges=201\n",
      "loss.item()=-197.06747436523438, ablated_edges=190\n",
      "loss.item()=-199.0982208251953, ablated_edges=191\n",
      "Epochs trained:  180\n",
      "Loss: -199.0982\n",
      "Total preserved: 11356.8369\n",
      "Edges ablated:  191\n",
      "Toxic loss:  115.06510925292969\n",
      "OWT loss:  4.48853063583374\n",
      "Penalty:  tensor(180.5737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.05385703220963478, logit diff = -2.5438079833984375\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.06761181354522705, logit diff = -1.68450927734375\n",
      "\n",
      "\n",
      "loss.item()=-199.94967651367188, ablated_edges=195\n",
      "loss.item()=-198.15061950683594, ablated_edges=193\n",
      "loss.item()=-202.0618438720703, ablated_edges=184\n",
      "loss.item()=-203.1697540283203, ablated_edges=196\n",
      "loss.item()=-204.13270568847656, ablated_edges=193\n",
      "loss.item()=-206.20631408691406, ablated_edges=192\n",
      "loss.item()=-206.7109375, ablated_edges=178\n",
      "loss.item()=-207.88792419433594, ablated_edges=184\n",
      "loss.item()=-208.67483520507812, ablated_edges=188\n",
      "loss.item()=-210.10255432128906, ablated_edges=185\n",
      "Epochs trained:  190\n",
      "Loss: -210.1026\n",
      "Total preserved: 11362.3047\n",
      "Edges ablated:  185\n",
      "Toxic loss:  113.24454498291016\n",
      "OWT loss:  4.5692853927612305\n",
      "Penalty:  tensor(192.0229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 0.03505903482437134, logit diff = -0.7157669067382812\n",
      "Best Token: [' the'], P(Alicia) = 0.033363599330186844, logit diff = -0.6969680786132812\n",
      "\n",
      "\n",
      "loss.item()=-211.40692138671875, ablated_edges=181\n",
      "loss.item()=-212.5459747314453, ablated_edges=186\n",
      "loss.item()=-213.54647827148438, ablated_edges=180\n",
      "loss.item()=-214.78076171875, ablated_edges=183\n",
      "loss.item()=-215.60923767089844, ablated_edges=187\n",
      "loss.item()=-216.89059448242188, ablated_edges=181\n",
      "loss.item()=-218.66571044921875, ablated_edges=177\n",
      "loss.item()=-219.42579650878906, ablated_edges=184\n",
      "loss.item()=-218.42306518554688, ablated_edges=179\n",
      "loss.item()=-221.67855834960938, ablated_edges=176\n",
      "Epochs trained:  200\n",
      "Loss: -221.6786\n",
      "Total preserved: 11368.9492\n",
      "Edges ablated:  176\n",
      "Toxic loss:  115.38264465332031\n",
      "OWT loss:  4.902148246765137\n",
      "Penalty:  tensor(203.5042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 0.0025283442810177803, logit diff = -0.027069091796875\n",
      "Best Token: [' the'], P(Alicia) = 0.0014656907878816128, logit diff = 0.8911209106445312\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = -1 * (regularization_strength * penalty + alpha * tox_loss) + owt_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            with torch.no_grad():\n",
    "                test_ioi_sentences = [\"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\", \"While Joshua and Alicia were commuting to the restaurant, Joshua gave a snack to\"]\n",
    "                for test_ioi_sentence in test_ioi_sentences:\n",
    "                    correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "                    other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "                    test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "                    generation = model(test_ioi_tokens)[0][:, -1]\n",
    "                    probs = torch.softmax(generation, dim=-1)\n",
    "                    print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    # epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    epochs_left = -1\n",
    "    # log_every = int(input('set log frequency'))\n",
    "    # edge_threshold = int(input('set edge threshold'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/params_dict_lambda={regularization_strength}_{means_ioi=}_{template_type=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_preserving = 0\n",
    "for p in mask_params:\n",
    "    p.data[p.data < threshold] = 0\n",
    "    p.data[p.data >= threshold] = 1\n",
    "    total_preserving += p.data.sum()\n",
    "print(total_preserving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "mask_params_copy = duplicate_mask_params(mask_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_mask_params.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model before and after circuit breaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/ioi_prompts_test.pkl\", \"rb\") as f:\n",
    "    ioi_prompts_test = pickle.load(f)\n",
    "    # ioi_sentences_test = [t[2] for t in ioi_sentences_test]\n",
    "\n",
    "with open(\"data/eval_uniform.pkl\", \"rb\") as f:\n",
    "    uniform_samples = pickle.load(f)\n",
    "    uniform_sentences = [t[2] for t in uniform_samples]\n",
    "\n",
    "original_model = load_demo_gpt2(means=False)\n",
    "\n",
    "with open(\"models/masked_gpt2_mean_ablation_v6.pkl\", \"rb\") as f:\n",
    "    model.state_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_prompts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on an ioi_sentence\n",
    "ioi_prompt = ioi_prompts_test[0]\n",
    "print(ioi_prompt)\n",
    "\n",
    "original_model.eval()\n",
    "original_model.to('cuda')\n",
    "def get_last_token(model, prompt, topk=5, sentence=False):\n",
    "    # generate last token\n",
    "    if not sentence:\n",
    "        prompt_sentence = prompt['text']\n",
    "    else:\n",
    "        prompt_sentence = prompt\n",
    "\n",
    "    tokens = tokenizer(prompt_sentence, return_tensors='pt').input_ids[:, :-1]\n",
    "\n",
    "    # generate one token, decode original_model(ioi_tokens[:, :-1])\n",
    "    model_outputs = model(tokens)[0]\n",
    "    model_outputs = model_outputs.squeeze(0)[-1]\n",
    "    probs = torch.nn.functional.softmax(model_outputs, dim=-1)\n",
    "\n",
    "    topk_outputs = torch.topk(model_outputs, topk)\n",
    "    topk_tokens = topk_outputs.indices\n",
    "    topk_probs = probs[topk_outputs.indices]\n",
    "    \n",
    "    topk_tokens_decoded = tokenizer.batch_decode(topk_tokens)\n",
    "    \n",
    "    if not sentence:\n",
    "        # Get logit diff by finding difference between logit of \n",
    "        io_token = tokenizer(\" \" + prompt['IO'], return_tensors='pt').input_ids[:, -1]\n",
    "        s_token = tokenizer(\" \" + prompt['S'], return_tensors='pt').input_ids[:, -1]\n",
    "        logit_diff = model_outputs[io_token][0] - model_outputs[s_token][0]\n",
    "        return topk_tokens_decoded, topk_probs, logit_diff\n",
    "    return topk_tokens_decoded, topk_probs\n",
    "\n",
    "print(get_last_token(original_model, ioi_prompt))\n",
    "\n",
    "\n",
    "for idx in range(3):\n",
    "    print(uniform_sentences[idx])\n",
    "    print(\"Before ablation\")\n",
    "    print(get_last_token(original_model, uniform_sentences[idx], sentence=True)[0])\n",
    "    print()\n",
    "    print(\"After ablation\")\n",
    "    print(get_last_token(model, uniform_sentences[idx], sentence=True)[0])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try on uniform samples\n",
    "def get_ioi_score(model, num_samples):\n",
    "    ave_logit_diffs = []\n",
    "    for idx in range(num_samples):\n",
    "        prompt = ioi_prompts_test[idx]\n",
    "        ave_logit_diffs.append(get_last_token(model, prompt)[2])\n",
    "    return sum(ave_logit_diffs) / len(ave_logit_diffs)\n",
    "\n",
    "get_ioi_score(original_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize mask\n",
    "Create the computational graphs in edge attribution patching paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mask and calculate what edges are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/params_dict_lambda=2.pkl\", \"rb\") as f:\n",
    "    mask_params = pickle.load(f)\n",
    "    mask_params = mask_params[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_and_edges(mask_params, edge_0=True):\n",
    "    \"\"\"\n",
    "    If edge_0 is True, then edges are between nodes with mask value 0. Else, edges are between nodes with mask value 1.\n",
    "    \"\"\"\n",
    "    # calculate which nodes will be in the graph\n",
    "    connected_nodes = set()\n",
    "    # add embed node at position\n",
    "    # connected_nodes.add((-1, \"embed\"))\n",
    "    n_heads = 12\n",
    "    n_layers = 12\n",
    "\n",
    "    # associate each node with a position\n",
    "    all_possible_nodes = [(-1, \"embed\")]\n",
    "    mask_dict = {}\n",
    "    # empty tensor\n",
    "    mask_dict[\"embed\"] = torch.zeros(size=(0,))\n",
    "    for idx in range(len(mask_params)):\n",
    "        if \"attention\" in param_names[idx]:\n",
    "            layer = int(param_names[idx].split(\".\")[1])\n",
    "            for i in range(n_heads):\n",
    "                all_possible_nodes.append((layer, f\"a{layer}.{i}\"))\n",
    "                mask_dict[f\"a{layer}.{i}\"] = mask_params[idx][:,i].detach().cpu()\n",
    "        elif \"mlp\" in param_names[idx]:\n",
    "            layer = int(param_names[idx].split(\".\")[1])\n",
    "            all_possible_nodes.append((layer, f\"m{layer}\"))\n",
    "            mask_dict[f\"m{layer}\"] = mask_params[idx].detach().cpu()\n",
    "    all_possible_nodes.append((n_heads, \"output\"))\n",
    "    mask_dict[\"output\"] = mask_params[0]\n",
    "\n",
    "    # Calculate where edges are based on the mask\n",
    "    # Edge between node i and node j if mask_dict[i][all_possible_nodes.index(j)] == 0\n",
    "    edges = set()\n",
    "    for i in range(len(all_possible_nodes)):\n",
    "        for j in range(len(all_possible_nodes)):\n",
    "            j_index = all_possible_nodes.index(all_possible_nodes[j])\n",
    "            if j_index < len(mask_dict[all_possible_nodes[i][1]]) and mask_dict[all_possible_nodes[i][1]][all_possible_nodes.index(all_possible_nodes[j])] == (0 if edge_0 else 1):\n",
    "                edges.add((all_possible_nodes[i], all_possible_nodes[j]))\n",
    "    \n",
    "    nodes_with_edges = set([node for edge in edges for node in edge])\n",
    "\n",
    "    return all_possible_nodes, nodes_with_edges, edges, mask_dict\n",
    "all_possible_nodes, nodes_with_edges, edges, mask_dict = get_nodes_and_edges(mask_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze ACDC and Compare\n",
    "I separately used ACDC++ (EAP from \"Attribution Patching Outperforms Automated Circuit Discovery\" paper) to get the known circuit edges. I want to compare my various learned masks (from different losses) to the known circuit edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/acdcpp_edges.pkl\", \"rb\") as f:\n",
    "    acdcpp_edges_long = pickle.load(f)\n",
    "acdcpp_edges_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acdcpp edges are in format 'blocks.1.attn.hook_result[:, :, 10]blocks.0.hook_mlp_in[:]', convert to format of ((1, 'a1.10'), (0, 'm0'))\n",
    "\n",
    "def get_node_name(node_name, show_full_index=False):\n",
    "    \"\"\"Node name for use in pretty graphs\"\"\"\n",
    "\n",
    "    def get_index(node_name_long):\n",
    "        # Get the index by looking for number in brackets\n",
    "        # e.g. blocks.1.attn.hook_result[:, :, 10] -> 10\n",
    "        index = node_name_long.split(\"[\")[-1].split(\"]\")[0]\n",
    "        index = index.split(\", \")[-1]\n",
    "        return int(index)\n",
    "\n",
    "    if not show_full_index:\n",
    "        name = \"\"\n",
    "        qkv_substrings = [f\"hook_{letter}\" for letter in [\"q\", \"k\", \"v\"]]\n",
    "        qkv_input_substrings = [f\"hook_{letter}_input\" for letter in [\"q\", \"k\", \"v\"]]\n",
    "\n",
    "        # Handle embedz\n",
    "        if \"resid_pre\" in node_name:\n",
    "            assert \"0\" in node_name and not any([str(i) in node_name for i in range(1, 10)])\n",
    "            name += \"embed\"\n",
    "            layer = -1\n",
    "            # if len(node.index.hashable_tuple) > 2:\n",
    "            #     name += f\"_[{node.index.hashable_tuple[2]}]\"\n",
    "            # return name\n",
    "\n",
    "        elif \"embed\" in node_name:\n",
    "            name = \"pos_embeds\" if \"pos\" in node_name else \"token_embeds\"\n",
    "            layer = -1\n",
    "\n",
    "        # Handle q_input and hook_q etc\n",
    "        elif any([node_name.endswith(qkv_input_substring) for qkv_input_substring in qkv_input_substrings]):\n",
    "            relevant_letter = None\n",
    "            for letter, qkv_substring in zip([\"q\", \"k\", \"v\"], qkv_substrings):\n",
    "                if qkv_substring in node_name:\n",
    "                    assert relevant_letter is None\n",
    "                    relevant_letter = letter\n",
    "            name += \"a\" + node_name.split(\".\")[1] + \".\" + str(get_index(node_name)) + \"_\" + relevant_letter\n",
    "            layer = int(node_name.split(\".\")[1])\n",
    "\n",
    "        # Handle attention hook_result\n",
    "        elif \"hook_result\" in node_name or any([qkv_substring in node_name for qkv_substring in qkv_substrings]):\n",
    "            name = \"a\" + node_name.split(\".\")[1] + \".\" + str(get_index(node_name))\n",
    "            layer = int(node_name.split(\".\")[1])\n",
    "\n",
    "        # Handle MLPs\n",
    "        elif node_name.endswith(\"resid_mid\"):\n",
    "            raise ValueError(\"We removed resid_mid annotations. Call these mlp_in now.\")\n",
    "        elif \"mlp\" in node_name:\n",
    "            name = \"m\" + node_name.split(\".\")[1]\n",
    "            layer = int(node_name.split(\".\")[1])\n",
    "\n",
    "        # Handle resid_post\n",
    "        elif \"resid_post\" in node_name:\n",
    "            name += \"resid_post\"\n",
    "            layer = 12\n",
    "\n",
    "        # elif \"mlp\" in node_name:\n",
    "        #     name += \"m\" + node_name.split(\".\")[1]\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized node name {node_name}\")\n",
    "\n",
    "    else:\n",
    "        name = node_name\n",
    "        # name = node_name + str(node.index.graphviz_index(use_actual_colon=True))\n",
    "\n",
    "    # get layer by looking for number before first dot\n",
    "    \n",
    "\n",
    "    return layer, name\n",
    "\n",
    "acdcpp_edges = set()\n",
    "for edge in acdcpp_edges_long[0][0.08]:\n",
    "    # split the edge into two nodes, e.g. blocks.1.attn.hook_result[:, :, 10]blocks.0.hook_mlp_in[:] into blocks.1.attn.hook_result[:, :, 10] and blocks.0.hook_mlp_in[:]\n",
    "    node_1 = get_node_name(edge.split(\"]\")[0]+\"]\", show_full_index=False)\n",
    "    node_2 = get_node_name(edge.split(\"]\")[1]+\"]\", show_full_index=False)\n",
    "    acdcpp_edges.add((node_1, node_2))\n",
    "with open(\"models/acdcpp_edges.pkl\", \"wb\") as f:\n",
    "    pickle.dump(acdcpp_edges, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert edges back to weight mask\n",
    "def get_mask_from_edges(edges, weight_mask_template=mask_dict, all_possible_nodes=all_possible_nodes):\n",
    "    new_mask_dict = {}\n",
    "    for node_name in weight_mask_template:\n",
    "        new_mask_dict[node_name] = torch.ones_like(weight_mask_template[node_name])\n",
    "    \n",
    "    node_indices = {node_name: idx for idx, node_name in enumerate(all_possible_nodes)}\n",
    "    for edge in edges:\n",
    "        try:\n",
    "            new_mask_dict[edge[0][1]][node_indices[edge[1]]] = 0\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return new_mask_dict\n",
    "\n",
    "def convert_mask_dict_to_params(mask_dict):\n",
    "    mask_params = []\n",
    "    # first output_mask\n",
    "    mask_params.append(mask_dict[\"output\"])\n",
    "    for layer in range(12):\n",
    "        attn_tensors = []\n",
    "        for head in range(12):\n",
    "            attn_tensors.append(mask_dict[f\"a{layer}.{head}\"])\n",
    "        mask_params.append(torch.stack(attn_tensors, dim=1))\n",
    "        mask_params.append(mask_dict[f\"m{layer}\"])\n",
    "    return mask_params\n",
    "acdcpp_mask_dict = get_mask_from_edges(acdcpp_edges)\n",
    "acdcpp_mask_params = convert_mask_dict_to_params(acdcpp_mask_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/acdcpp_mask_params.pkl\", \"wb\") as f:\n",
    "    pickle.dump(acdcpp_mask_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that edges are the same\n",
    "_, _, acdcpp_edges_2, _ = get_nodes_and_edges(acdcpp_mask_params)\n",
    "print(len(acdcpp_edges_2))\n",
    "print(len(acdcpp_edges))\n",
    "print(len(acdcpp_edges_2.intersection(acdcpp_edges)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze overlaps between different edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/alternative_necessary_masks_params_dict_lambda=1.pkl\", \"rb\") as f:\n",
    "    alternative_necessary_mask_params = pickle.load(f)\n",
    "    alternative_necessary_mask_params = alternative_necessary_mask_params[200]\n",
    "with open(\"models/alternative_sufficient_masks_params_dict_lambda=1.pkl\", \"rb\") as f:\n",
    "    alternative_sufficient_mask_params = pickle.load(f)\n",
    "    alternative_sufficient_mask_params = alternative_sufficient_mask_params[200]\n",
    "_, _, alternative_necessary_edges, _ = get_nodes_and_edges(alternative_necessary_mask_params)\n",
    "_, _, alternative_sufficient_edges, _ = get_nodes_and_edges(alternative_sufficient_mask_params, edge_0=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nodes_and_edges(alternative_sufficient_mask_params, edge_0=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(edges)=}, {len(acdcpp_edges)=}, {len(edges.intersection(acdcpp_edges))=}\")\n",
    "print(edges.intersection(acdcpp_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get overlaps between all edges (regular edges, necessary, sufficient, acdcpp) (make a table with tabulate)\n",
    "edges_dict = {\"circuit_breaking\":edges, \"ioi_necessary\":alternative_necessary_edges, \"ioi_sufficient\":alternative_sufficient_edges, \"acdcpp\":acdcpp_edges}\n",
    "for edge_type in edges_dict:\n",
    "    for second_edge_type in edges_dict:\n",
    "        print(f\"{edge_type} and {second_edge_type}: {len(edges_dict[edge_type].intersection(edges_dict[second_edge_type]))} edges in common\")#, {edges_dict[edge_type].intersection(edges_dict[second_edge_type])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aligned_graph(all_possible_nodes, edges):\n",
    "    G = pgv.AGraph(strict=False, directed=True)\n",
    "\n",
    "    # Find the maximum layer number for adjusting the graph\n",
    "    max_layer = max(layer for layer, _ in all_possible_nodes if isinstance(layer, int))\n",
    "    nodes_with_edges = set([node for edge in edges for node in edge])\n",
    "\n",
    "    # Add nodes and edges to the graph\n",
    "    for node in all_possible_nodes:\n",
    "        if node in [edge[0] for edge in edges] or node in [edge[1] for edge in edges]:\n",
    "            G.add_node(node[1], layer=str(max_layer - node[0]))\n",
    "\n",
    "    for edge in edges:\n",
    "        G.add_edge(edge[1][1], edge[0][1])\n",
    "\n",
    "    # Create subgraphs to ensure nodes of the same layer have the same rank\n",
    "    for layer in range(max_layer, -2, -1):\n",
    "        with G.subgraph(name=f'cluster_{layer}') as s:\n",
    "            s.graph_attr['rank'] = 'same'\n",
    "            for node in nodes_with_edges:\n",
    "                if node[0] == layer:\n",
    "                    s.add_node(node[1])\n",
    "\n",
    "    # Apply layout and render the graph\n",
    "    G.layout(prog='dot')\n",
    "    G.draw('aligned_graph.png')\n",
    "    return Image('aligned_graph.png')\n",
    "\n",
    "# Call the function with your nodes and edges\n",
    "flipped_graph_image = create_aligned_graph(all_possible_nodes, edges)\n",
    "\n",
    "# To display the graph in Jupyter Notebook\n",
    "flipped_graph_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersecting edges graph\n",
    "for edge_type in edges_dict:\n",
    "    for second_edge_type in edges_dict:\n",
    "        if edge_type == second_edge_type:\n",
    "            continue\n",
    "        # make a graph with just the intersecting edges, title it with the two edge types\n",
    "        print(f\"Intersection between {edge_type} and {second_edge_type}: {len(edges_dict[edge_type].intersection(edges_dict[second_edge_type]))} edges in common, {edges_dict[edge_type].intersection(edges_dict[second_edge_type])}\")\n",
    "        \n",
    "        intersecting_edges_graph = create_aligned_graph(all_possible_nodes, edges_dict[edge_type].intersection(edges_dict[second_edge_type]))\n",
    "        display(intersecting_edges_graph)\n",
    "        # intersecting_edges_graph.render(f\"intersecting_edges_graph_{edge_type}_{second_edge_type}\", format=\"png\", cleanup=True)\n",
    "# intersecting_edges_graph = create_aligned_graph(all_possible_nodes, edges.intersection(acdcpp_edges))\n",
    "# intersecting_edges_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygraphviz as pgv\n",
    "from pathlib import Path\n",
    "from IPython.display import Image\n",
    "\n",
    "def show(nodes, edges, fname=None):\n",
    "    g = pgv.AGraph(strict=True, directed=True)\n",
    "    g.graph_attr.update(ranksep='0.1', nodesep='0.1', compound=True)\n",
    "    g.node_attr.update(fixedsize='true', width='1.5', height='.5')\n",
    "    \n",
    "    layer_to_subgraph = {}\n",
    "\n",
    "    # Create a subgraph for each layer\n",
    "    for node in nodes:\n",
    "        layer = node[0]\n",
    "        if layer not in layer_to_subgraph:\n",
    "            # Each layer has its own subgraph with 'rank=same' to ensure they are on the same level\n",
    "            layer_to_subgraph[layer] = g.add_subgraph(name=f'cluster_{layer}', rank='same')\n",
    "            \n",
    "        # Here you add the node to the appropriate subgraph\n",
    "        layer_to_subgraph[layer].add_node(node, label=str(node[1]))\n",
    "\n",
    "    # Now, add the edges to the graph\n",
    "    for edge in edges:\n",
    "        g.add_edge(edge[0], edge[1])\n",
    "    \n",
    "    # If a filename is provided, write the file and optionally render to an image\n",
    "    if fname:\n",
    "        fpath = Path(fname)\n",
    "        base_fname = fpath.stem\n",
    "        base_path = fpath.parent\n",
    "        base_path.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        # Write the dot file\n",
    "        g.write(path=base_path / f\"{base_fname}.gv\")\n",
    "        \n",
    "        # Render to an image\n",
    "        g.layout(prog='dot')\n",
    "        g.draw(path=base_path / f\"{base_fname}.png\")\n",
    "        \n",
    "    return g\n",
    "\n",
    "\n",
    "g = show(nodes_with_edges, edges, fname=\"graph.gv\")\n",
    "Image(g.draw(format='png', prog='dot'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "unlrn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
