{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train mask over IOI edges and analyze mask vs known circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir(\"/data/phillip_guo/circuit-breaking/ioi/\")\n",
    "from models import load_gpt2_weights, load_demo_gpt2, tokenizer\n",
    "from data import retrieve_toxic_data, retrieve_owt_data, retrieve_toxic_data_low_loss, retrieve_toxic_filtered_data, FILTER_DEMO_LEN, CONTEXT_LENGTH\n",
    "from inference import infer_batch_with_owt, infer_batch, prepare_fixed_demo, criterion\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import pickle\n",
    "import datasets\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from itertools import cycle\n",
    "# from eval import evaluate_model\n",
    "from data import batch_text_to_tokens\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_batch_size = 5 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 5\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "\n",
    "template_type = \"double\"\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None, template_type=template_type)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "# with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "#     means = pickle.load(f)[0][0]\n",
    "means_ioi = True\n",
    "if means_ioi:\n",
    "    with open(\"data/gpt2_ioi_abc_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "else:\n",
    "    with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "\n",
    "model = load_demo_gpt2(means=means)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 20 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 0.2 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 100\n",
    "max_steps_per_epoch = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train params of mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49322/1453109909.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f278b86a43841beb28111c983f3375c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=-6.3235321044921875, ablated_edges=870\n",
      "loss.item()=-11.444580078125, ablated_edges=2063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 54, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 31, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 355, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/data/phillip_guo/miniconda3/envs/unlrn/lib/python3.10/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/data/phillip_guo/circuit-breaking/ioi/train_ioi_mask.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcais/data/phillip_guo/circuit-breaking/ioi/train_ioi_mask.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     penalty \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, p\u001b[39m.\u001b[39msum() \u001b[39m*\u001b[39m (epochs_trained\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m10000\u001b[39m) \u001b[39m# why 2000? free\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcais/data/phillip_guo/circuit-breaking/ioi/train_ioi_mask.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# demos = batch[:, :FILTER_DEMO_LEN]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcais/data/phillip_guo/circuit-breaking/ioi/train_ioi_mask.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# completions = batch[:, FILTER_DEMO_LEN:]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcais/data/phillip_guo/circuit-breaking/ioi/train_ioi_mask.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcais/data/phillip_guo/circuit-breaking/ioi/train_ioi_mask.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcais/data/phillip_guo/circuit-breaking/ioi/train_ioi_mask.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcais/data/phillip_guo/circuit-breaking/ioi/train_ioi_mask.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m tox_loss, owt_loss \u001b[39m=\u001b[39m infer_batch_with_owt(model, criterion, batch, \u001b[39mnext\u001b[39;49m(owt_iter), batch_size, demos, access_toxic_pos\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcais/data/phillip_guo/circuit-breaking/ioi/train_ioi_mask.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# print(f\"{tox_loss=}, {owt_loss=}\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcais/data/phillip_guo/circuit-breaking/ioi/train_ioi_mask.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39m*\u001b[39m (regularization_strength \u001b[39m*\u001b[39m penalty \u001b[39m+\u001b[39m alpha \u001b[39m*\u001b[39m tox_loss) \u001b[39m+\u001b[39m owt_loss\n",
      "File \u001b[0;32m~/circuit-breaking/ioi/inference.py:79\u001b[0m, in \u001b[0;36minfer_batch_with_owt\u001b[0;34m(model, criterion, toxic_batch, owt_batch, batch_size, demos, device, access_toxic_pos)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m batch\n\u001b[1;32m     76\u001b[0m \u001b[39m# print(input.shape, input.dtype)\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[39m# generate the output\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m out \u001b[39m=\u001b[39m model(\u001b[39minput\u001b[39;49m)[\u001b[39m0\u001b[39m]  \u001b[39m# 0 is the logits\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39m# print(f\"{out.shape=}, {demos.shape=}\")\u001b[39;00m\n\u001b[1;32m     83\u001b[0m losses[idx] \u001b[39m=\u001b[39m evaluate_sequence_loss(out, \u001b[39minput\u001b[39m, criterion, demos\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], access_seq_pos\u001b[39m=\u001b[39maccess_toxic_pos \u001b[39mif\u001b[39;00m idx \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/circuit-breaking/ioi/transformer.py:331\u001b[0m, in \u001b[0;36mDemoTransformer.forward\u001b[0;34m(self, tokens, return_states)\u001b[0m\n\u001b[1;32m    327\u001b[0m residual \u001b[39m=\u001b[39m einops\u001b[39m.\u001b[39mrearrange(residual, \u001b[39m\"\u001b[39m\u001b[39mbatch position d_model -> batch position 1 d_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    329\u001b[0m \u001b[39mfor\u001b[39;00m i, block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks):\n\u001b[1;32m    330\u001b[0m     \u001b[39m# print(i)\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m     residual \u001b[39m=\u001b[39m block(residual, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmeans)\n\u001b[1;32m    332\u001b[0m     \u001b[39m# if hasattr(self,\"saved_states\"):\u001b[39;00m\n\u001b[1;32m    333\u001b[0m     \u001b[39m#     self.saved_states = torch.cat((self.saved_states, block.saved_output.unsqueeze(0)), dim=0)\u001b[39;00m\n\u001b[1;32m    334\u001b[0m     \u001b[39m# else:\u001b[39;00m\n\u001b[1;32m    335\u001b[0m     \u001b[39m#     self.saved_states = block.saved_output.unsqueeze(0)\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39mif\u001b[39;00m return_states:\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/circuit-breaking/ioi/transformer.py:255\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, means)\u001b[0m\n\u001b[1;32m    251\u001b[0m normalized_resid_pre \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln1(masked_residuals, parallel\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    252\u001b[0m \u001b[39m# print(normalized_resid_pre[:,:,0])\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39m# print(torch.allclose(normalized_resid_pre[:,:,torch.randperm(normalized_resid_pre.shape[2])],normalized_resid_pre))\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m attn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(normalized_resid_pre)\n\u001b[1;32m    257\u001b[0m \u001b[39m# self.saved_output = attn_out\u001b[39;00m\n\u001b[1;32m    259\u001b[0m residual \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((resid_pre, attn_out), dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/circuit-breaking/ioi/transformer.py:160\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, normalized_resid_pre)\u001b[0m\n\u001b[1;32m    157\u001b[0m pattern \u001b[39m=\u001b[39m attn_scores\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# [batch, n_head, query_pos, key_pos]\u001b[39;00m\n\u001b[1;32m    158\u001b[0m v \u001b[39m=\u001b[39m einsum(\u001b[39m\"\u001b[39m\u001b[39mbatch key_pos n_heads d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\u001b[39m\u001b[39m\"\u001b[39m, normalized_resid_pre, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_V) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_V\n\u001b[0;32m--> 160\u001b[0m z \u001b[39m=\u001b[39m einsum(\u001b[39m\"\u001b[39;49m\u001b[39mbatch n_heads query_pos key_pos, batch key_pos n_heads d_head -> batch query_pos n_heads d_head\u001b[39;49m\u001b[39m\"\u001b[39;49m, pattern, v)\n\u001b[1;32m    162\u001b[0m attn_out \u001b[39m=\u001b[39m einsum(\u001b[39m\"\u001b[39m\u001b[39mbatch query_pos n_heads d_head, n_heads d_head d_model -> batch query_pos n_heads d_model\u001b[39m\u001b[39m\"\u001b[39m, z, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_O) \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_O \u001b[39m/\u001b[39m cfg\u001b[39m.\u001b[39mn_heads)\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m attn_out\n",
      "File \u001b[0;32m~/miniconda3/envs/unlrn/lib/python3.10/site-packages/fancy_einsum/__init__.py:127\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    123\u001b[0m     new_equation \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(term \u001b[39mif\u001b[39;00m term \u001b[39min\u001b[39;00m SPECIAL \u001b[39melse\u001b[39;00m long_to_short[term] \u001b[39mfor\u001b[39;00m term \u001b[39min\u001b[39;00m terms)\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m new_equation\n\u001b[0;32m--> 127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meinsum\u001b[39m(equation: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39moperands):\n\u001b[1;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Evaluates the Einstein summation convention on the operands.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m    See: \u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m      https://pytorch.org/docs/stable/generated/torch.einsum.html\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m      https://numpy.org/doc/stable/reference/generated/numpy.einsum.html\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     backend \u001b[39m=\u001b[39m get_backend(operands[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = -1 * (regularization_strength * penalty + alpha * tox_loss) + owt_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            with torch.no_grad():\n",
    "                test_ioi_sentence = \"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\"\n",
    "                correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "                other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "                test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "                generation = model(test_ioi_tokens)[0][:, -1]\n",
    "                probs = torch.softmax(generation, dim=-1)\n",
    "                print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    # epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    epochs_left = -1\n",
    "    # log_every = int(input('set log frequency'))\n",
    "    # edge_threshold = int(input('set edge threshold'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/params_dict_lambda={regularization_strength}_{means_ioi=}_{template_type=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_preserving = 0\n",
    "for p in mask_params:\n",
    "    p.data[p.data < threshold] = 0\n",
    "    p.data[p.data >= threshold] = 1\n",
    "    total_preserving += p.data.sum()\n",
    "print(total_preserving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "mask_params_copy = duplicate_mask_params(mask_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_mask_params.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model before and after circuit breaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/ioi_prompts_test.pkl\", \"rb\") as f:\n",
    "    ioi_prompts_test = pickle.load(f)\n",
    "    # ioi_sentences_test = [t[2] for t in ioi_sentences_test]\n",
    "\n",
    "with open(\"data/eval_uniform.pkl\", \"rb\") as f:\n",
    "    uniform_samples = pickle.load(f)\n",
    "    uniform_sentences = [t[2] for t in uniform_samples]\n",
    "\n",
    "original_model = load_demo_gpt2(means=False)\n",
    "\n",
    "with open(\"models/masked_gpt2_mean_ablation_v6.pkl\", \"rb\") as f:\n",
    "    model.state_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_prompts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on an ioi_sentence\n",
    "ioi_prompt = ioi_prompts_test[0]\n",
    "print(ioi_prompt)\n",
    "\n",
    "original_model.eval()\n",
    "original_model.to('cuda')\n",
    "def get_last_token(model, prompt, topk=5, sentence=False):\n",
    "    # generate last token\n",
    "    if not sentence:\n",
    "        prompt_sentence = prompt['text']\n",
    "    else:\n",
    "        prompt_sentence = prompt\n",
    "\n",
    "    tokens = tokenizer(prompt_sentence, return_tensors='pt').input_ids[:, :-1]\n",
    "\n",
    "    # generate one token, decode original_model(ioi_tokens[:, :-1])\n",
    "    model_outputs = model(tokens)[0]\n",
    "    model_outputs = model_outputs.squeeze(0)[-1]\n",
    "    probs = torch.nn.functional.softmax(model_outputs, dim=-1)\n",
    "\n",
    "    topk_outputs = torch.topk(model_outputs, topk)\n",
    "    topk_tokens = topk_outputs.indices\n",
    "    topk_probs = probs[topk_outputs.indices]\n",
    "    \n",
    "    topk_tokens_decoded = tokenizer.batch_decode(topk_tokens)\n",
    "    \n",
    "    if not sentence:\n",
    "        # Get logit diff by finding difference between logit of \n",
    "        io_token = tokenizer(\" \" + prompt['IO'], return_tensors='pt').input_ids[:, -1]\n",
    "        s_token = tokenizer(\" \" + prompt['S'], return_tensors='pt').input_ids[:, -1]\n",
    "        logit_diff = model_outputs[io_token][0] - model_outputs[s_token][0]\n",
    "        return topk_tokens_decoded, topk_probs, logit_diff\n",
    "    return topk_tokens_decoded, topk_probs\n",
    "\n",
    "print(get_last_token(original_model, ioi_prompt))\n",
    "\n",
    "\n",
    "for idx in range(3):\n",
    "    print(uniform_sentences[idx])\n",
    "    print(\"Before ablation\")\n",
    "    print(get_last_token(original_model, uniform_sentences[idx], sentence=True)[0])\n",
    "    print()\n",
    "    print(\"After ablation\")\n",
    "    print(get_last_token(model, uniform_sentences[idx], sentence=True)[0])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try on uniform samples\n",
    "def get_ioi_score(model, num_samples):\n",
    "    ave_logit_diffs = []\n",
    "    for idx in range(num_samples):\n",
    "        prompt = ioi_prompts_test[idx]\n",
    "        ave_logit_diffs.append(get_last_token(model, prompt)[2])\n",
    "    return sum(ave_logit_diffs) / len(ave_logit_diffs)\n",
    "\n",
    "get_ioi_score(original_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize mask\n",
    "Create the computational graphs in edge attribution patching paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mask and calculate what edges are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/params_dict_lambda=2.pkl\", \"rb\") as f:\n",
    "    mask_params = pickle.load(f)\n",
    "    mask_params = mask_params[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_and_edges(mask_params, edge_0=True):\n",
    "    \"\"\"\n",
    "    If edge_0 is True, then edges are between nodes with mask value 0. Else, edges are between nodes with mask value 1.\n",
    "    \"\"\"\n",
    "    # calculate which nodes will be in the graph\n",
    "    connected_nodes = set()\n",
    "    # add embed node at position\n",
    "    # connected_nodes.add((-1, \"embed\"))\n",
    "    n_heads = 12\n",
    "    n_layers = 12\n",
    "\n",
    "    # associate each node with a position\n",
    "    all_possible_nodes = [(-1, \"embed\")]\n",
    "    mask_dict = {}\n",
    "    # empty tensor\n",
    "    mask_dict[\"embed\"] = torch.zeros(size=(0,))\n",
    "    for idx in range(len(mask_params)):\n",
    "        if \"attention\" in param_names[idx]:\n",
    "            layer = int(param_names[idx].split(\".\")[1])\n",
    "            for i in range(n_heads):\n",
    "                all_possible_nodes.append((layer, f\"a{layer}.{i}\"))\n",
    "                mask_dict[f\"a{layer}.{i}\"] = mask_params[idx][:,i].detach().cpu()\n",
    "        elif \"mlp\" in param_names[idx]:\n",
    "            layer = int(param_names[idx].split(\".\")[1])\n",
    "            all_possible_nodes.append((layer, f\"m{layer}\"))\n",
    "            mask_dict[f\"m{layer}\"] = mask_params[idx].detach().cpu()\n",
    "    all_possible_nodes.append((n_heads, \"output\"))\n",
    "    mask_dict[\"output\"] = mask_params[0]\n",
    "\n",
    "    # Calculate where edges are based on the mask\n",
    "    # Edge between node i and node j if mask_dict[i][all_possible_nodes.index(j)] == 0\n",
    "    edges = set()\n",
    "    for i in range(len(all_possible_nodes)):\n",
    "        for j in range(len(all_possible_nodes)):\n",
    "            j_index = all_possible_nodes.index(all_possible_nodes[j])\n",
    "            if j_index < len(mask_dict[all_possible_nodes[i][1]]) and mask_dict[all_possible_nodes[i][1]][all_possible_nodes.index(all_possible_nodes[j])] == (0 if edge_0 else 1):\n",
    "                edges.add((all_possible_nodes[i], all_possible_nodes[j]))\n",
    "    \n",
    "    nodes_with_edges = set([node for edge in edges for node in edge])\n",
    "\n",
    "    return all_possible_nodes, nodes_with_edges, edges, mask_dict\n",
    "all_possible_nodes, nodes_with_edges, edges, mask_dict = get_nodes_and_edges(mask_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze ACDC and Compare\n",
    "I separately used ACDC++ (EAP from \"Attribution Patching Outperforms Automated Circuit Discovery\" paper) to get the known circuit edges. I want to compare my various learned masks (from different losses) to the known circuit edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/acdcpp_edges.pkl\", \"rb\") as f:\n",
    "    acdcpp_edges_long = pickle.load(f)\n",
    "acdcpp_edges_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acdcpp edges are in format 'blocks.1.attn.hook_result[:, :, 10]blocks.0.hook_mlp_in[:]', convert to format of ((1, 'a1.10'), (0, 'm0'))\n",
    "\n",
    "def get_node_name(node_name, show_full_index=False):\n",
    "    \"\"\"Node name for use in pretty graphs\"\"\"\n",
    "\n",
    "    def get_index(node_name_long):\n",
    "        # Get the index by looking for number in brackets\n",
    "        # e.g. blocks.1.attn.hook_result[:, :, 10] -> 10\n",
    "        index = node_name_long.split(\"[\")[-1].split(\"]\")[0]\n",
    "        index = index.split(\", \")[-1]\n",
    "        return int(index)\n",
    "\n",
    "    if not show_full_index:\n",
    "        name = \"\"\n",
    "        qkv_substrings = [f\"hook_{letter}\" for letter in [\"q\", \"k\", \"v\"]]\n",
    "        qkv_input_substrings = [f\"hook_{letter}_input\" for letter in [\"q\", \"k\", \"v\"]]\n",
    "\n",
    "        # Handle embedz\n",
    "        if \"resid_pre\" in node_name:\n",
    "            assert \"0\" in node_name and not any([str(i) in node_name for i in range(1, 10)])\n",
    "            name += \"embed\"\n",
    "            layer = -1\n",
    "            # if len(node.index.hashable_tuple) > 2:\n",
    "            #     name += f\"_[{node.index.hashable_tuple[2]}]\"\n",
    "            # return name\n",
    "\n",
    "        elif \"embed\" in node_name:\n",
    "            name = \"pos_embeds\" if \"pos\" in node_name else \"token_embeds\"\n",
    "            layer = -1\n",
    "\n",
    "        # Handle q_input and hook_q etc\n",
    "        elif any([node_name.endswith(qkv_input_substring) for qkv_input_substring in qkv_input_substrings]):\n",
    "            relevant_letter = None\n",
    "            for letter, qkv_substring in zip([\"q\", \"k\", \"v\"], qkv_substrings):\n",
    "                if qkv_substring in node_name:\n",
    "                    assert relevant_letter is None\n",
    "                    relevant_letter = letter\n",
    "            name += \"a\" + node_name.split(\".\")[1] + \".\" + str(get_index(node_name)) + \"_\" + relevant_letter\n",
    "            layer = int(node_name.split(\".\")[1])\n",
    "\n",
    "        # Handle attention hook_result\n",
    "        elif \"hook_result\" in node_name or any([qkv_substring in node_name for qkv_substring in qkv_substrings]):\n",
    "            name = \"a\" + node_name.split(\".\")[1] + \".\" + str(get_index(node_name))\n",
    "            layer = int(node_name.split(\".\")[1])\n",
    "\n",
    "        # Handle MLPs\n",
    "        elif node_name.endswith(\"resid_mid\"):\n",
    "            raise ValueError(\"We removed resid_mid annotations. Call these mlp_in now.\")\n",
    "        elif \"mlp\" in node_name:\n",
    "            name = \"m\" + node_name.split(\".\")[1]\n",
    "            layer = int(node_name.split(\".\")[1])\n",
    "\n",
    "        # Handle resid_post\n",
    "        elif \"resid_post\" in node_name:\n",
    "            name += \"resid_post\"\n",
    "            layer = 12\n",
    "\n",
    "        # elif \"mlp\" in node_name:\n",
    "        #     name += \"m\" + node_name.split(\".\")[1]\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized node name {node_name}\")\n",
    "\n",
    "    else:\n",
    "        name = node_name\n",
    "        # name = node_name + str(node.index.graphviz_index(use_actual_colon=True))\n",
    "\n",
    "    # get layer by looking for number before first dot\n",
    "    \n",
    "\n",
    "    return layer, name\n",
    "\n",
    "acdcpp_edges = set()\n",
    "for edge in acdcpp_edges_long[0][0.08]:\n",
    "    # split the edge into two nodes, e.g. blocks.1.attn.hook_result[:, :, 10]blocks.0.hook_mlp_in[:] into blocks.1.attn.hook_result[:, :, 10] and blocks.0.hook_mlp_in[:]\n",
    "    node_1 = get_node_name(edge.split(\"]\")[0]+\"]\", show_full_index=False)\n",
    "    node_2 = get_node_name(edge.split(\"]\")[1]+\"]\", show_full_index=False)\n",
    "    acdcpp_edges.add((node_1, node_2))\n",
    "with open(\"models/acdcpp_edges.pkl\", \"wb\") as f:\n",
    "    pickle.dump(acdcpp_edges, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert edges back to weight mask\n",
    "def get_mask_from_edges(edges, weight_mask_template=mask_dict, all_possible_nodes=all_possible_nodes):\n",
    "    new_mask_dict = {}\n",
    "    for node_name in weight_mask_template:\n",
    "        new_mask_dict[node_name] = torch.ones_like(weight_mask_template[node_name])\n",
    "    \n",
    "    node_indices = {node_name: idx for idx, node_name in enumerate(all_possible_nodes)}\n",
    "    for edge in edges:\n",
    "        try:\n",
    "            new_mask_dict[edge[0][1]][node_indices[edge[1]]] = 0\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return new_mask_dict\n",
    "\n",
    "def convert_mask_dict_to_params(mask_dict):\n",
    "    mask_params = []\n",
    "    # first output_mask\n",
    "    mask_params.append(mask_dict[\"output\"])\n",
    "    for layer in range(12):\n",
    "        attn_tensors = []\n",
    "        for head in range(12):\n",
    "            attn_tensors.append(mask_dict[f\"a{layer}.{head}\"])\n",
    "        mask_params.append(torch.stack(attn_tensors, dim=1))\n",
    "        mask_params.append(mask_dict[f\"m{layer}\"])\n",
    "    return mask_params\n",
    "acdcpp_mask_dict = get_mask_from_edges(acdcpp_edges)\n",
    "acdcpp_mask_params = convert_mask_dict_to_params(acdcpp_mask_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/acdcpp_mask_params.pkl\", \"wb\") as f:\n",
    "    pickle.dump(acdcpp_mask_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that edges are the same\n",
    "_, _, acdcpp_edges_2, _ = get_nodes_and_edges(acdcpp_mask_params)\n",
    "print(len(acdcpp_edges_2))\n",
    "print(len(acdcpp_edges))\n",
    "print(len(acdcpp_edges_2.intersection(acdcpp_edges)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze overlaps between different edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/alternative_necessary_masks_params_dict_lambda=1.pkl\", \"rb\") as f:\n",
    "    alternative_necessary_mask_params = pickle.load(f)\n",
    "    alternative_necessary_mask_params = alternative_necessary_mask_params[200]\n",
    "with open(\"models/alternative_sufficient_masks_params_dict_lambda=1.pkl\", \"rb\") as f:\n",
    "    alternative_sufficient_mask_params = pickle.load(f)\n",
    "    alternative_sufficient_mask_params = alternative_sufficient_mask_params[200]\n",
    "_, _, alternative_necessary_edges, _ = get_nodes_and_edges(alternative_necessary_mask_params)\n",
    "_, _, alternative_sufficient_edges, _ = get_nodes_and_edges(alternative_sufficient_mask_params, edge_0=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nodes_and_edges(alternative_sufficient_mask_params, edge_0=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(edges)=}, {len(acdcpp_edges)=}, {len(edges.intersection(acdcpp_edges))=}\")\n",
    "print(edges.intersection(acdcpp_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get overlaps between all edges (regular edges, necessary, sufficient, acdcpp) (make a table with tabulate)\n",
    "edges_dict = {\"circuit_breaking\":edges, \"ioi_necessary\":alternative_necessary_edges, \"ioi_sufficient\":alternative_sufficient_edges, \"acdcpp\":acdcpp_edges}\n",
    "for edge_type in edges_dict:\n",
    "    for second_edge_type in edges_dict:\n",
    "        print(f\"{edge_type} and {second_edge_type}: {len(edges_dict[edge_type].intersection(edges_dict[second_edge_type]))} edges in common\")#, {edges_dict[edge_type].intersection(edges_dict[second_edge_type])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aligned_graph(all_possible_nodes, edges):\n",
    "    G = pgv.AGraph(strict=False, directed=True)\n",
    "\n",
    "    # Find the maximum layer number for adjusting the graph\n",
    "    max_layer = max(layer for layer, _ in all_possible_nodes if isinstance(layer, int))\n",
    "    nodes_with_edges = set([node for edge in edges for node in edge])\n",
    "\n",
    "    # Add nodes and edges to the graph\n",
    "    for node in all_possible_nodes:\n",
    "        if node in [edge[0] for edge in edges] or node in [edge[1] for edge in edges]:\n",
    "            G.add_node(node[1], layer=str(max_layer - node[0]))\n",
    "\n",
    "    for edge in edges:\n",
    "        G.add_edge(edge[1][1], edge[0][1])\n",
    "\n",
    "    # Create subgraphs to ensure nodes of the same layer have the same rank\n",
    "    for layer in range(max_layer, -2, -1):\n",
    "        with G.subgraph(name=f'cluster_{layer}') as s:\n",
    "            s.graph_attr['rank'] = 'same'\n",
    "            for node in nodes_with_edges:\n",
    "                if node[0] == layer:\n",
    "                    s.add_node(node[1])\n",
    "\n",
    "    # Apply layout and render the graph\n",
    "    G.layout(prog='dot')\n",
    "    G.draw('aligned_graph.png')\n",
    "    return Image('aligned_graph.png')\n",
    "\n",
    "# Call the function with your nodes and edges\n",
    "flipped_graph_image = create_aligned_graph(all_possible_nodes, edges)\n",
    "\n",
    "# To display the graph in Jupyter Notebook\n",
    "flipped_graph_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersecting edges graph\n",
    "for edge_type in edges_dict:\n",
    "    for second_edge_type in edges_dict:\n",
    "        if edge_type == second_edge_type:\n",
    "            continue\n",
    "        # make a graph with just the intersecting edges, title it with the two edge types\n",
    "        print(f\"Intersection between {edge_type} and {second_edge_type}: {len(edges_dict[edge_type].intersection(edges_dict[second_edge_type]))} edges in common, {edges_dict[edge_type].intersection(edges_dict[second_edge_type])}\")\n",
    "        \n",
    "        intersecting_edges_graph = create_aligned_graph(all_possible_nodes, edges_dict[edge_type].intersection(edges_dict[second_edge_type]))\n",
    "        display(intersecting_edges_graph)\n",
    "        # intersecting_edges_graph.render(f\"intersecting_edges_graph_{edge_type}_{second_edge_type}\", format=\"png\", cleanup=True)\n",
    "# intersecting_edges_graph = create_aligned_graph(all_possible_nodes, edges.intersection(acdcpp_edges))\n",
    "# intersecting_edges_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygraphviz as pgv\n",
    "from pathlib import Path\n",
    "from IPython.display import Image\n",
    "\n",
    "def show(nodes, edges, fname=None):\n",
    "    g = pgv.AGraph(strict=True, directed=True)\n",
    "    g.graph_attr.update(ranksep='0.1', nodesep='0.1', compound=True)\n",
    "    g.node_attr.update(fixedsize='true', width='1.5', height='.5')\n",
    "    \n",
    "    layer_to_subgraph = {}\n",
    "\n",
    "    # Create a subgraph for each layer\n",
    "    for node in nodes:\n",
    "        layer = node[0]\n",
    "        if layer not in layer_to_subgraph:\n",
    "            # Each layer has its own subgraph with 'rank=same' to ensure they are on the same level\n",
    "            layer_to_subgraph[layer] = g.add_subgraph(name=f'cluster_{layer}', rank='same')\n",
    "            \n",
    "        # Here you add the node to the appropriate subgraph\n",
    "        layer_to_subgraph[layer].add_node(node, label=str(node[1]))\n",
    "\n",
    "    # Now, add the edges to the graph\n",
    "    for edge in edges:\n",
    "        g.add_edge(edge[0], edge[1])\n",
    "    \n",
    "    # If a filename is provided, write the file and optionally render to an image\n",
    "    if fname:\n",
    "        fpath = Path(fname)\n",
    "        base_fname = fpath.stem\n",
    "        base_path = fpath.parent\n",
    "        base_path.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        # Write the dot file\n",
    "        g.write(path=base_path / f\"{base_fname}.gv\")\n",
    "        \n",
    "        # Render to an image\n",
    "        g.layout(prog='dot')\n",
    "        g.draw(path=base_path / f\"{base_fname}.png\")\n",
    "        \n",
    "    return g\n",
    "\n",
    "\n",
    "g = show(nodes_with_edges, edges, fname=\"graph.gv\")\n",
    "Image(g.draw(format='png', prog='dot'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "unlrn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
