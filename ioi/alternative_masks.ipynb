{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train different kinds of masks over IOI edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir(\"/data/phillip_guo/circuit-breaking/ioi/\")\n",
    "from models import load_gpt2_weights, load_demo_gpt2, tokenizer\n",
    "from data import retrieve_toxic_data, retrieve_owt_data, retrieve_toxic_data_low_loss, retrieve_toxic_filtered_data, FILTER_DEMO_LEN, CONTEXT_LENGTH\n",
    "from inference import infer_batch_with_owt, infer_batch, prepare_fixed_demo, criterion\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import pickle\n",
    "import datasets\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from itertools import cycle\n",
    "# from eval import evaluate_model\n",
    "from data import batch_text_to_tokens\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train params of mask\n",
    "Train without the original D_train loss term (only mask loss and IOI data loss)\n",
    "Finds necessary (but not sufficient) edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_batch_size = 5 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 1\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "\n",
    "template_type = \"double\"\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None, template_type=template_type)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "# with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "#     means = pickle.load(f)[0][0]\n",
    "means_ioi = True\n",
    "if means_ioi:\n",
    "    with open(\"data/gpt2_ioi_abc_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "else:\n",
    "    with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "\n",
    "model = load_demo_gpt2(means=means)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 20 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 1 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 100\n",
    "max_steps_per_epoch = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23780/474617925.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddf88ea84a14701b3e2761247af9368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=-69.30683898925781, ablated_edges=1211\n",
      "loss.item()=-93.07505798339844, ablated_edges=2603\n",
      "loss.item()=-98.3801498413086, ablated_edges=3031\n",
      "loss.item()=-108.5616683959961, ablated_edges=3355\n",
      "loss.item()=-112.21004486083984, ablated_edges=3576\n",
      "loss.item()=-113.6036148071289, ablated_edges=3725\n",
      "loss.item()=-114.2625732421875, ablated_edges=3839\n",
      "loss.item()=-119.2594223022461, ablated_edges=3966\n",
      "loss.item()=-117.36256408691406, ablated_edges=4071\n",
      "loss.item()=-116.73172760009766, ablated_edges=4141\n",
      "Epochs trained:  10\n",
      "Loss: -116.7317\n",
      "Total preserved: 7108.1475\n",
      "Edges ablated:  4141\n",
      "Toxic loss:  116.73172760009766\n",
      "OWT loss:  10.723258018493652\n",
      "Penalty:  0\n",
      "Best Token: [' do'], P(Alicia) = 2.0229411421718976e-32, logit diff = 20.8072509765625\n",
      "\n",
      "\n",
      "loss.item()=-119.75999450683594, ablated_edges=4207\n",
      "loss.item()=-117.84992980957031, ablated_edges=4308\n",
      "loss.item()=-115.53193664550781, ablated_edges=4387\n",
      "loss.item()=-116.5049819946289, ablated_edges=4491\n",
      "loss.item()=-119.66324615478516, ablated_edges=4618\n",
      "loss.item()=-119.22982025146484, ablated_edges=4679\n",
      "loss.item()=-121.25892639160156, ablated_edges=4712\n",
      "loss.item()=-120.13720703125, ablated_edges=4784\n",
      "loss.item()=-119.27070617675781, ablated_edges=4817\n",
      "loss.item()=-119.20509338378906, ablated_edges=4856\n",
      "Epochs trained:  20\n",
      "Loss: -119.2051\n",
      "Total preserved: 6535.7461\n",
      "Edges ablated:  4861\n",
      "Toxic loss:  119.20509338378906\n",
      "OWT loss:  12.589276313781738\n",
      "Penalty:  0\n",
      "Best Token: [' do'], P(Alicia) = 1.643089601839023e-22, logit diff = -6.562694549560547\n",
      "\n",
      "\n",
      "loss.item()=-116.083251953125, ablated_edges=5048\n",
      "loss.item()=-122.07527923583984, ablated_edges=5090\n",
      "loss.item()=-120.157958984375, ablated_edges=5037\n",
      "loss.item()=-123.10047912597656, ablated_edges=4937\n",
      "loss.item()=-122.4560317993164, ablated_edges=4823\n",
      "loss.item()=-121.96051788330078, ablated_edges=4668\n",
      "loss.item()=-123.44474792480469, ablated_edges=4517\n",
      "loss.item()=-126.65019226074219, ablated_edges=4365\n",
      "loss.item()=-126.5105209350586, ablated_edges=4182\n",
      "loss.item()=-119.72662353515625, ablated_edges=3991\n",
      "Epochs trained:  30\n",
      "Loss: -119.7266\n",
      "Total preserved: 7594.8486\n",
      "Edges ablated:  3991\n",
      "Toxic loss:  112.8912582397461\n",
      "OWT loss:  11.621380805969238\n",
      "Penalty:  tensor(6.8354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' do'], P(Alicia) = 1.1510818213236826e-35, logit diff = 19.28069305419922\n",
      "\n",
      "\n",
      "loss.item()=-129.73934936523438, ablated_edges=3839\n",
      "loss.item()=-129.55718994140625, ablated_edges=3666\n",
      "loss.item()=-132.53648376464844, ablated_edges=3511\n",
      "loss.item()=-131.69265747070312, ablated_edges=3368\n",
      "loss.item()=-132.150146484375, ablated_edges=3241\n",
      "loss.item()=-137.24993896484375, ablated_edges=3105\n",
      "loss.item()=-134.34347534179688, ablated_edges=2985\n",
      "loss.item()=-137.1819610595703, ablated_edges=2848\n",
      "loss.item()=-137.13568115234375, ablated_edges=2771\n",
      "loss.item()=-137.53982543945312, ablated_edges=2648\n",
      "Epochs trained:  40\n",
      "Loss: -137.5398\n",
      "Total preserved: 8893.7158\n",
      "Edges ablated:  2647\n",
      "Toxic loss:  120.64176940917969\n",
      "OWT loss:  10.47789478302002\n",
      "Penalty:  tensor(16.8981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' do'], P(Alicia) = 6.042559544578696e-19, logit diff = 5.631416320800781\n",
      "\n",
      "\n",
      "loss.item()=-131.79415893554688, ablated_edges=3100\n",
      "loss.item()=-137.2484130859375, ablated_edges=3045\n",
      "loss.item()=-139.95724487304688, ablated_edges=2880\n",
      "loss.item()=-141.26956176757812, ablated_edges=2703\n",
      "loss.item()=-139.438720703125, ablated_edges=2547\n",
      "loss.item()=-146.48126220703125, ablated_edges=2427\n",
      "loss.item()=-145.32533264160156, ablated_edges=2306\n",
      "loss.item()=-145.523193359375, ablated_edges=2209\n",
      "loss.item()=-150.2157745361328, ablated_edges=2098\n",
      "loss.item()=-146.4185791015625, ablated_edges=2048\n",
      "Epochs trained:  50\n",
      "Loss: -146.4186\n",
      "Total preserved: 9486.4854\n",
      "Edges ablated:  2048\n",
      "Toxic loss:  118.90777587890625\n",
      "OWT loss:  9.379229545593262\n",
      "Penalty:  tensor(27.5108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' do'], P(Alicia) = 1.8328662175241198e-37, logit diff = 18.653520584106445\n",
      "\n",
      "\n",
      "loss.item()=-148.6842498779297, ablated_edges=1974\n",
      "loss.item()=-152.89781188964844, ablated_edges=1930\n",
      "loss.item()=-154.1824188232422, ablated_edges=1886\n",
      "loss.item()=-154.27996826171875, ablated_edges=1825\n",
      "loss.item()=-152.14190673828125, ablated_edges=1764\n",
      "loss.item()=-157.72988891601562, ablated_edges=1724\n",
      "loss.item()=-157.7760467529297, ablated_edges=1686\n",
      "loss.item()=-156.96095275878906, ablated_edges=1641\n",
      "loss.item()=-157.62429809570312, ablated_edges=1613\n",
      "loss.item()=-158.7283935546875, ablated_edges=1585\n",
      "Epochs trained:  60\n",
      "Loss: -158.7284\n",
      "Total preserved: 9944.0088\n",
      "Edges ablated:  1592\n",
      "Toxic loss:  119.9467544555664\n",
      "OWT loss:  11.131474494934082\n",
      "Penalty:  tensor(38.7816, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' do'], P(Alicia) = 7.592534505488469e-18, logit diff = 4.927833557128906\n",
      "\n",
      "\n",
      "loss.item()=-151.51283264160156, ablated_edges=2118\n",
      "loss.item()=-156.16036987304688, ablated_edges=2055\n",
      "loss.item()=-159.01707458496094, ablated_edges=1899\n",
      "loss.item()=-163.4292755126953, ablated_edges=1757\n",
      "loss.item()=-163.60458374023438, ablated_edges=1673\n",
      "loss.item()=-166.13587951660156, ablated_edges=1593\n",
      "loss.item()=-167.74156188964844, ablated_edges=1520\n",
      "loss.item()=-168.41278076171875, ablated_edges=1464\n",
      "loss.item()=-169.98793029785156, ablated_edges=1416\n",
      "loss.item()=-171.3525848388672, ablated_edges=1373\n",
      "Epochs trained:  70\n",
      "Loss: -171.3526\n",
      "Total preserved: 10147.0254\n",
      "Edges ablated:  1373\n",
      "Toxic loss:  121.63216400146484\n",
      "OWT loss:  14.806452751159668\n",
      "Penalty:  tensor(49.7204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' do'], P(Alicia) = 1.0404443992090814e-34, logit diff = 15.780721664428711\n",
      "\n",
      "\n",
      "loss.item()=-171.61387634277344, ablated_edges=1336\n",
      "loss.item()=-174.72804260253906, ablated_edges=1317\n",
      "loss.item()=-173.3213348388672, ablated_edges=1292\n",
      "loss.item()=-172.8892822265625, ablated_edges=1270\n",
      "loss.item()=-172.66468811035156, ablated_edges=1246\n",
      "loss.item()=-174.861083984375, ablated_edges=1221\n",
      "loss.item()=-181.7411346435547, ablated_edges=1207\n",
      "loss.item()=-177.98028564453125, ablated_edges=1180\n",
      "loss.item()=-180.47564697265625, ablated_edges=1167\n",
      "loss.item()=-180.63050842285156, ablated_edges=1152\n",
      "Epochs trained:  80\n",
      "Loss: -180.6305\n",
      "Total preserved: 10378.8984\n",
      "Edges ablated:  1149\n",
      "Toxic loss:  119.39500427246094\n",
      "OWT loss:  10.632519721984863\n",
      "Penalty:  tensor(61.2355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' get'], P(Alicia) = 1.77433327768926e-14, logit diff = -7.445616722106934\n",
      "\n",
      "\n",
      "loss.item()=-171.56065368652344, ablated_edges=1571\n",
      "loss.item()=-177.42137145996094, ablated_edges=1513\n",
      "loss.item()=-184.1636199951172, ablated_edges=1354\n",
      "loss.item()=-179.73614501953125, ablated_edges=1271\n",
      "loss.item()=-182.74575805664062, ablated_edges=1194\n",
      "loss.item()=-186.36962890625, ablated_edges=1142\n",
      "loss.item()=-189.54312133789062, ablated_edges=1108\n",
      "loss.item()=-189.97799682617188, ablated_edges=1079\n",
      "loss.item()=-189.2567138671875, ablated_edges=1060\n",
      "loss.item()=-191.5472412109375, ablated_edges=1030\n",
      "Epochs trained:  90\n",
      "Loss: -191.5472\n",
      "Total preserved: 10501.6025\n",
      "Edges ablated:  1030\n",
      "Toxic loss:  119.086181640625\n",
      "OWT loss:  12.283077239990234\n",
      "Penalty:  tensor(72.4611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' do'], P(Alicia) = 1.0979718292827871e-36, logit diff = 15.16836929321289\n",
      "\n",
      "\n",
      "loss.item()=-192.04075622558594, ablated_edges=1021\n",
      "loss.item()=-194.5152587890625, ablated_edges=997\n",
      "loss.item()=-196.24844360351562, ablated_edges=988\n",
      "loss.item()=-197.63259887695312, ablated_edges=965\n",
      "loss.item()=-200.49343872070312, ablated_edges=958\n",
      "loss.item()=-200.44097900390625, ablated_edges=948\n",
      "loss.item()=-201.90341186523438, ablated_edges=942\n",
      "loss.item()=-201.00018310546875, ablated_edges=925\n",
      "loss.item()=-202.94326782226562, ablated_edges=926\n",
      "loss.item()=-203.45327758789062, ablated_edges=920\n",
      "Epochs trained:  100\n",
      "Loss: -203.4533\n",
      "Total preserved: 10615.0264\n",
      "Edges ablated:  920\n",
      "Toxic loss:  119.59456634521484\n",
      "OWT loss:  13.079444885253906\n",
      "Penalty:  tensor(83.8587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' do'], P(Alicia) = 1.2215787776546627e-19, logit diff = -7.492204666137695\n",
      "\n",
      "\n",
      "loss.item()=-193.21446228027344, ablated_edges=1287\n",
      "loss.item()=-198.12359619140625, ablated_edges=1259\n",
      "loss.item()=-204.4586181640625, ablated_edges=1116\n",
      "loss.item()=-206.2154083251953, ablated_edges=1007\n",
      "loss.item()=-208.73159790039062, ablated_edges=952\n",
      "loss.item()=-209.4329376220703, ablated_edges=911\n",
      "loss.item()=-210.64047241210938, ablated_edges=905\n",
      "loss.item()=-211.26214599609375, ablated_edges=887\n",
      "loss.item()=-212.222900390625, ablated_edges=845\n",
      "loss.item()=-216.22738647460938, ablated_edges=839\n",
      "Epochs trained:  110\n",
      "Loss: -216.2274\n",
      "Total preserved: 10705.4688\n",
      "Edges ablated:  839\n",
      "Toxic loss:  120.94873046875\n",
      "OWT loss:  10.958250999450684\n",
      "Penalty:  tensor(95.2787, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' do'], P(Alicia) = 2.5599799240682583e-36, logit diff = 12.857467651367188\n",
      "\n",
      "\n",
      "loss.item()=-216.29251098632812, ablated_edges=825\n",
      "loss.item()=-216.49636840820312, ablated_edges=807\n",
      "loss.item()=-220.55169677734375, ablated_edges=792\n",
      "loss.item()=-219.62631225585938, ablated_edges=806\n",
      "loss.item()=-223.3312225341797, ablated_edges=802\n",
      "loss.item()=-220.23861694335938, ablated_edges=797\n",
      "loss.item()=-223.22116088867188, ablated_edges=786\n",
      "loss.item()=-221.83328247070312, ablated_edges=767\n",
      "loss.item()=-226.24600219726562, ablated_edges=759\n",
      "loss.item()=-226.6051788330078, ablated_edges=760\n",
      "Epochs trained:  120\n",
      "Loss: -226.6052\n",
      "Total preserved: 10777.4082\n",
      "Edges ablated:  758\n",
      "Toxic loss:  119.9088363647461\n",
      "OWT loss:  9.045472145080566\n",
      "Penalty:  tensor(106.6963, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' do'], P(Alicia) = 4.850834339752434e-16, logit diff = -7.36799955368042\n",
      "\n",
      "\n",
      "loss.item()=-212.423095703125, ablated_edges=1179\n",
      "loss.item()=-216.73324584960938, ablated_edges=1090\n",
      "loss.item()=-228.67713928222656, ablated_edges=946\n",
      "loss.item()=-229.54202270507812, ablated_edges=869\n",
      "loss.item()=-229.49371337890625, ablated_edges=821\n",
      "loss.item()=-230.91847229003906, ablated_edges=792\n",
      "loss.item()=-233.87472534179688, ablated_edges=764\n",
      "loss.item()=-237.58901977539062, ablated_edges=747\n",
      "loss.item()=-236.1458740234375, ablated_edges=728\n",
      "loss.item()=-239.11216735839844, ablated_edges=725\n",
      "Epochs trained:  130\n",
      "Loss: -239.1122\n",
      "Total preserved: 10811.2041\n",
      "Edges ablated:  725\n",
      "Toxic loss:  121.27006530761719\n",
      "OWT loss:  12.938461303710938\n",
      "Penalty:  tensor(117.8421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' do'], P(Alicia) = 8.103147247828587e-34, logit diff = 16.60067367553711\n",
      "\n",
      "\n",
      "loss.item()=-240.9459686279297, ablated_edges=717\n",
      "loss.item()=-239.28414916992188, ablated_edges=711\n",
      "loss.item()=-240.05848693847656, ablated_edges=696\n",
      "loss.item()=-240.58377075195312, ablated_edges=700\n",
      "loss.item()=-245.39109802246094, ablated_edges=702\n",
      "loss.item()=-245.38357543945312, ablated_edges=688\n",
      "loss.item()=-243.5882110595703, ablated_edges=687\n",
      "loss.item()=-244.79986572265625, ablated_edges=687\n",
      "loss.item()=-246.8782958984375, ablated_edges=682\n",
      "loss.item()=-245.5419921875, ablated_edges=672\n",
      "Epochs trained:  140\n",
      "Loss: -245.5420\n",
      "Total preserved: 10866.6289\n",
      "Edges ablated:  673\n",
      "Toxic loss:  116.22911071777344\n",
      "OWT loss:  12.390518188476562\n",
      "Penalty:  tensor(129.3129, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' do'], P(Alicia) = 1.0298409260783847e-12, logit diff = -8.964164733886719\n",
      "\n",
      "\n",
      "loss.item()=-230.97518920898438, ablated_edges=1040\n",
      "loss.item()=-243.6122589111328, ablated_edges=1014\n",
      "loss.item()=-246.24522399902344, ablated_edges=888\n",
      "loss.item()=-249.85678100585938, ablated_edges=804\n",
      "loss.item()=-250.5515594482422, ablated_edges=725\n",
      "loss.item()=-251.37295532226562, ablated_edges=700\n",
      "loss.item()=-257.61260986328125, ablated_edges=676\n",
      "loss.item()=-255.3905029296875, ablated_edges=662\n",
      "loss.item()=-258.23870849609375, ablated_edges=673\n",
      "loss.item()=-259.1381530761719, ablated_edges=659\n",
      "Epochs trained:  150\n",
      "Loss: -259.1382\n",
      "Total preserved: 10870.2939\n",
      "Edges ablated:  659\n",
      "Toxic loss:  118.91136169433594\n",
      "OWT loss:  12.471199035644531\n",
      "Penalty:  tensor(140.2268, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' do'], P(Alicia) = 2.7319677064430246e-33, logit diff = 9.803218841552734\n",
      "\n",
      "\n",
      "loss.item()=-261.724853515625, ablated_edges=655\n",
      "loss.item()=-265.7785339355469, ablated_edges=645\n",
      "loss.item()=-261.018798828125, ablated_edges=650\n",
      "loss.item()=-263.94647216796875, ablated_edges=644\n",
      "loss.item()=-260.44891357421875, ablated_edges=638\n",
      "loss.item()=-264.9773864746094, ablated_edges=636\n",
      "loss.item()=-268.75982666015625, ablated_edges=620\n",
      "loss.item()=-267.36395263671875, ablated_edges=608\n",
      "loss.item()=-267.8345947265625, ablated_edges=618\n",
      "loss.item()=-269.446044921875, ablated_edges=615\n",
      "Epochs trained:  160\n",
      "Loss: -269.4460\n",
      "Total preserved: 10918.8047\n",
      "Edges ablated:  613\n",
      "Toxic loss:  117.6746826171875\n",
      "OWT loss:  12.161919593811035\n",
      "Penalty:  tensor(151.7714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' get'], P(Alicia) = 8.295491030207991e-13, logit diff = -2.7985095977783203\n",
      "\n",
      "\n",
      "loss.item()=-253.47862243652344, ablated_edges=933\n",
      "loss.item()=-265.44512939453125, ablated_edges=880\n",
      "loss.item()=-274.2384033203125, ablated_edges=756\n",
      "loss.item()=-272.41949462890625, ablated_edges=698\n",
      "loss.item()=-272.9579772949219, ablated_edges=656\n",
      "loss.item()=-278.8621826171875, ablated_edges=643\n",
      "loss.item()=-277.8978576660156, ablated_edges=612\n",
      "loss.item()=-279.4290466308594, ablated_edges=609\n",
      "loss.item()=-281.5689697265625, ablated_edges=600\n",
      "loss.item()=-281.643310546875, ablated_edges=591\n",
      "Epochs trained:  170\n",
      "Loss: -281.6433\n",
      "Total preserved: 10930.9199\n",
      "Edges ablated:  591\n",
      "Toxic loss:  118.7726058959961\n",
      "OWT loss:  11.562618255615234\n",
      "Penalty:  tensor(162.8707, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' do'], P(Alicia) = 9.143647920854244e-29, logit diff = 13.732322692871094\n",
      "\n",
      "\n",
      "loss.item()=-283.58758544921875, ablated_edges=597\n",
      "loss.item()=-284.0570983886719, ablated_edges=593\n",
      "loss.item()=-285.79296875, ablated_edges=597\n",
      "loss.item()=-283.89984130859375, ablated_edges=595\n",
      "loss.item()=-286.6622009277344, ablated_edges=588\n",
      "loss.item()=-287.6993408203125, ablated_edges=574\n",
      "loss.item()=-291.27520751953125, ablated_edges=580\n",
      "loss.item()=-290.1045837402344, ablated_edges=579\n",
      "loss.item()=-291.0657958984375, ablated_edges=574\n",
      "loss.item()=-290.0191345214844, ablated_edges=575\n",
      "Epochs trained:  180\n",
      "Loss: -290.0191\n",
      "Total preserved: 10973.5859\n",
      "Edges ablated:  576\n",
      "Toxic loss:  115.53910827636719\n",
      "OWT loss:  11.359196662902832\n",
      "Penalty:  tensor(174.4800, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['-'], P(Alicia) = 8.908580084287642e-16, logit diff = -7.926055908203125\n",
      "\n",
      "\n",
      "loss.item()=-257.27008056640625, ablated_edges=837\n",
      "loss.item()=-268.17510986328125, ablated_edges=776\n",
      "loss.item()=-268.0901184082031, ablated_edges=661\n",
      "loss.item()=-275.6532287597656, ablated_edges=587\n",
      "loss.item()=-277.4978942871094, ablated_edges=555\n",
      "loss.item()=-284.4332275390625, ablated_edges=544\n",
      "loss.item()=-285.3622131347656, ablated_edges=541\n",
      "loss.item()=-286.0409240722656, ablated_edges=548\n",
      "loss.item()=-281.1267395019531, ablated_edges=531\n",
      "loss.item()=-283.72119140625, ablated_edges=543\n",
      "Epochs trained:  190\n",
      "Loss: -283.7212\n",
      "Total preserved: 10982.7246\n",
      "Edges ablated:  543\n",
      "Toxic loss:  98.11312103271484\n",
      "OWT loss:  18.28953742980957\n",
      "Penalty:  tensor(185.6081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' to'], P(Alicia) = 1.63599614354788e-31, logit diff = 7.511421203613281\n",
      "\n",
      "\n",
      "loss.item()=-292.60650634765625, ablated_edges=542\n",
      "loss.item()=-294.06829833984375, ablated_edges=543\n",
      "loss.item()=-293.546875, ablated_edges=532\n",
      "loss.item()=-295.49444580078125, ablated_edges=526\n",
      "loss.item()=-298.319091796875, ablated_edges=508\n",
      "loss.item()=-300.1333312988281, ablated_edges=506\n",
      "loss.item()=-301.9537048339844, ablated_edges=506\n",
      "loss.item()=-299.22772216796875, ablated_edges=486\n",
      "loss.item()=-305.1589050292969, ablated_edges=473\n",
      "loss.item()=-305.7052001953125, ablated_edges=476\n",
      "Epochs trained:  200\n",
      "Loss: -305.7052\n",
      "Total preserved: 11055.5430\n",
      "Edges ablated:  474\n",
      "Toxic loss:  107.81098937988281\n",
      "OWT loss:  15.023343086242676\n",
      "Penalty:  tensor(197.8942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [','], P(Alicia) = 2.0566411599948866e-17, logit diff = -4.7805023193359375\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = -1 * (regularization_strength * penalty + alpha * tox_loss) #+ owt_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            \n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_ioi_sentence = \"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\"\n",
    "                correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "                other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "                test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "                generation = model(test_ioi_tokens)[0][:, -1]\n",
    "                probs = torch.softmax(generation, dim=-1)\n",
    "                print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")\n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            print(\"\\n\")\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    # epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    # log_every = int(input('set log frequency'))\n",
    "    # edge_threshold = int(input('set edge threshold'))\n",
    "    epochs_left = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/alternative_necessary_masks_params_dict_lambda={regularization_strength}_{means_ioi=}_{template_type=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different alternative: sufficient but not necessary\n",
    "Trains with an inverted loss function. This loss function encourages sparsity (as opposed to discouraging) and wants model to ablate everything but the necessary circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_batch_size = 5 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 1\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "\n",
    "template_type = \"double\"\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None, template_type=template_type)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "# with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "#     means = pickle.load(f)[0][0]\n",
    "means_ioi = True\n",
    "if means_ioi:\n",
    "    with open(\"data/gpt2_ioi_abc_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "else:\n",
    "    with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "\n",
    "model = load_demo_gpt2(means=means)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 20 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 1 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 100\n",
    "max_steps_per_epoch = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49322/798755564.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96e3aff3b784db8891aaf44178ad8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=0.00015079081640578806, ablated_edges=0\n",
      "loss.item()=8.201432137866504e-06, ablated_edges=222\n",
      "loss.item()=3.583267243811861e-05, ablated_edges=278\n",
      "loss.item()=6.008088348607998e-06, ablated_edges=292\n",
      "loss.item()=2.407883948762901e-05, ablated_edges=294\n",
      "loss.item()=5.1661336328834295e-05, ablated_edges=297\n",
      "loss.item()=2.4484959794790484e-05, ablated_edges=299\n",
      "loss.item()=0.0, ablated_edges=303\n",
      "loss.item()=3.075577069466817e-06, ablated_edges=302\n",
      "loss.item()=1.3589813079306623e-06, ablated_edges=303\n",
      "Epochs trained:  10\n",
      "Loss: 0.0000\n",
      "Total preserved: 10087.0703\n",
      "Edges ablated:  303\n",
      "Toxic loss:  1.3589813079306623e-06\n",
      "OWT loss:  4.985528945922852\n",
      "Penalty:  0\n",
      "Best Token: [' Alicia'], P(Alicia) = 1.0, logit diff = 33.28448486328125\n",
      "\n",
      "\n",
      "loss.item()=2.1290243239491247e-05, ablated_edges=305\n",
      "loss.item()=4.506091499933973e-06, ablated_edges=311\n",
      "loss.item()=2.336495072086109e-06, ablated_edges=315\n",
      "loss.item()=7.009450200712308e-06, ablated_edges=319\n",
      "loss.item()=6.437292654482007e-07, ablated_edges=323\n",
      "loss.item()=3.6477813409874216e-06, ablated_edges=323\n",
      "loss.item()=3.6239409837435232e-06, ablated_edges=323\n",
      "loss.item()=4.458404873730615e-06, ablated_edges=324\n",
      "loss.item()=1.931187853188021e-06, ablated_edges=324\n",
      "loss.item()=9.55451832851395e-05, ablated_edges=329\n",
      "Epochs trained:  20\n",
      "Loss: 0.0001\n",
      "Total preserved: 10072.1289\n",
      "Edges ablated:  329\n",
      "Toxic loss:  9.55451832851395e-05\n",
      "OWT loss:  6.4051337242126465\n",
      "Penalty:  0\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9670072197914124, logit diff = 6.256072998046875\n",
      "\n",
      "\n",
      "loss.item()=0.011553098447620869, ablated_edges=3838\n",
      "loss.item()=0.6352083086967468, ablated_edges=5235\n",
      "loss.item()=1.076216220855713, ablated_edges=6161\n",
      "loss.item()=1.3020424842834473, ablated_edges=7141\n",
      "loss.item()=1.3587249517440796, ablated_edges=8079\n",
      "loss.item()=1.3202074766159058, ablated_edges=8886\n",
      "loss.item()=1.2405744791030884, ablated_edges=9500\n",
      "loss.item()=1.1577614545822144, ablated_edges=9969\n",
      "loss.item()=1.045571208000183, ablated_edges=10325\n",
      "loss.item()=0.9493163824081421, ablated_edges=10574\n",
      "Epochs trained:  30\n",
      "Loss: 0.9493\n",
      "Total preserved: 1052.5925\n",
      "Edges ablated:  10574\n",
      "Toxic loss:  0.0019832283724099398\n",
      "OWT loss:  7.346000671386719\n",
      "Penalty:  tensor(0.9473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999650716781616, logit diff = 12.917091369628906\n",
      "\n",
      "\n",
      "loss.item()=0.9713237881660461, ablated_edges=10689\n",
      "loss.item()=0.8818479180335999, ablated_edges=10854\n",
      "loss.item()=0.7956172823905945, ablated_edges=10988\n",
      "loss.item()=15.213136672973633, ablated_edges=11075\n",
      "loss.item()=5.116052627563477, ablated_edges=10260\n",
      "loss.item()=2.3298637866973877, ablated_edges=10247\n",
      "loss.item()=2.012176752090454, ablated_edges=10406\n",
      "loss.item()=1.6973950862884521, ablated_edges=10631\n",
      "loss.item()=1.7680678367614746, ablated_edges=10696\n",
      "loss.item()=1.578190803527832, ablated_edges=10825\n",
      "Epochs trained:  40\n",
      "Loss: 1.5782\n",
      "Total preserved: 830.3239\n",
      "Edges ablated:  10830\n",
      "Toxic loss:  0.0005752898287028074\n",
      "OWT loss:  9.068436622619629\n",
      "Penalty:  tensor(1.5776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.4708995819091797, logit diff = 7.761497497558594\n",
      "\n",
      "\n",
      "loss.item()=3.0824978351593018, ablated_edges=10488\n",
      "loss.item()=2.2408530712127686, ablated_edges=10563\n",
      "loss.item()=2.6573853492736816, ablated_edges=10724\n",
      "loss.item()=1.7684638500213623, ablated_edges=10844\n",
      "loss.item()=2.5833683013916016, ablated_edges=10889\n",
      "loss.item()=1.9104290008544922, ablated_edges=10905\n",
      "loss.item()=1.8137613534927368, ablated_edges=10971\n",
      "loss.item()=1.8457863330841064, ablated_edges=11009\n",
      "loss.item()=1.77278470993042, ablated_edges=11031\n",
      "loss.item()=2.296023368835449, ablated_edges=11090\n",
      "Epochs trained:  50\n",
      "Loss: 2.2960\n",
      "Total preserved: 561.8475\n",
      "Edges ablated:  11090\n",
      "Toxic loss:  0.6666656732559204\n",
      "OWT loss:  10.678236961364746\n",
      "Penalty:  tensor(1.6294, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9992002844810486, logit diff = 17.31420135498047\n",
      "\n",
      "\n",
      "loss.item()=1.6224887371063232, ablated_edges=11121\n",
      "loss.item()=1.5500986576080322, ablated_edges=11166\n",
      "loss.item()=1.4560198783874512, ablated_edges=11205\n",
      "loss.item()=1.415959119796753, ablated_edges=11246\n",
      "loss.item()=1.3179277181625366, ablated_edges=11263\n",
      "loss.item()=1.2175767421722412, ablated_edges=11300\n",
      "loss.item()=1.4245622158050537, ablated_edges=11320\n",
      "loss.item()=1.8904836177825928, ablated_edges=11183\n",
      "loss.item()=1.8977820873260498, ablated_edges=11176\n",
      "loss.item()=1.81606125831604, ablated_edges=11191\n",
      "Epochs trained:  60\n",
      "Loss: 1.8161\n",
      "Total preserved: 461.4905\n",
      "Edges ablated:  11192\n",
      "Toxic loss:  0.016248520463705063\n",
      "OWT loss:  9.481457710266113\n",
      "Penalty:  tensor(1.7998, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9944769740104675, logit diff = 8.764853954315186\n",
      "\n",
      "\n",
      "loss.item()=2.4624311923980713, ablated_edges=11075\n",
      "loss.item()=2.9383723735809326, ablated_edges=11059\n",
      "loss.item()=2.183173656463623, ablated_edges=11157\n",
      "loss.item()=2.0298657417297363, ablated_edges=11171\n",
      "loss.item()=1.7658007144927979, ablated_edges=11234\n",
      "loss.item()=1.5739165544509888, ablated_edges=11272\n",
      "loss.item()=1.3462820053100586, ablated_edges=11328\n",
      "loss.item()=1.3839495182037354, ablated_edges=11351\n",
      "loss.item()=1.2941197156906128, ablated_edges=11382\n",
      "loss.item()=1.079959511756897, ablated_edges=11410\n",
      "Epochs trained:  70\n",
      "Loss: 1.0800\n",
      "Total preserved: 220.2944\n",
      "Edges ablated:  11410\n",
      "Toxic loss:  0.0005169743089936674\n",
      "OWT loss:  10.611151695251465\n",
      "Penalty:  tensor(1.0794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9934296011924744, logit diff = 12.621719360351562\n",
      "\n",
      "\n",
      "loss.item()=1.1219009160995483, ablated_edges=11436\n",
      "loss.item()=1.5676114559173584, ablated_edges=11446\n",
      "loss.item()=2.0679678916931152, ablated_edges=11441\n",
      "loss.item()=2.5428128242492676, ablated_edges=11289\n",
      "loss.item()=2.366685390472412, ablated_edges=11226\n",
      "loss.item()=2.0198538303375244, ablated_edges=11282\n",
      "loss.item()=1.8796131610870361, ablated_edges=11308\n",
      "loss.item()=2.9267807006835938, ablated_edges=11228\n",
      "loss.item()=2.203951120376587, ablated_edges=11272\n",
      "loss.item()=3.5982871055603027, ablated_edges=11314\n",
      "Epochs trained:  80\n",
      "Loss: 3.5983\n",
      "Total preserved: 351.1087\n",
      "Edges ablated:  11314\n",
      "Toxic loss:  1.5267456769943237\n",
      "OWT loss:  8.436076164245605\n",
      "Penalty:  tensor(2.0715, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.00441218726336956, logit diff = -4.218360900878906\n",
      "\n",
      "\n",
      "loss.item()=3.264957904815674, ablated_edges=11292\n",
      "loss.item()=2.5293478965759277, ablated_edges=11294\n",
      "loss.item()=1.764093041419983, ablated_edges=11323\n",
      "loss.item()=1.5313427448272705, ablated_edges=11361\n",
      "loss.item()=1.783558964729309, ablated_edges=11399\n",
      "loss.item()=1.8139251470565796, ablated_edges=11387\n",
      "loss.item()=1.5783954858779907, ablated_edges=11393\n",
      "loss.item()=1.4838918447494507, ablated_edges=11427\n",
      "loss.item()=1.274156093597412, ablated_edges=11445\n",
      "loss.item()=1.1655393838882446, ablated_edges=11468\n",
      "Epochs trained:  90\n",
      "Loss: 1.1655\n",
      "Total preserved: 168.8426\n",
      "Edges ablated:  11468\n",
      "Toxic loss:  0.0005255600553937256\n",
      "OWT loss:  9.878424644470215\n",
      "Penalty:  tensor(1.1650, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9660268425941467, logit diff = 3.5334835052490234\n",
      "\n",
      "\n",
      "loss.item()=1.5569652318954468, ablated_edges=11428\n",
      "loss.item()=3.7669198513031006, ablated_edges=11430\n",
      "loss.item()=1.7015737295150757, ablated_edges=11424\n",
      "loss.item()=1.4518096446990967, ablated_edges=11447\n",
      "loss.item()=1.2584466934204102, ablated_edges=11464\n",
      "loss.item()=1.3776259422302246, ablated_edges=11473\n",
      "loss.item()=1.2243984937667847, ablated_edges=11480\n",
      "loss.item()=1.0806715488433838, ablated_edges=11499\n",
      "loss.item()=1.0814601182937622, ablated_edges=11503\n",
      "loss.item()=0.9704694747924805, ablated_edges=11511\n",
      "Epochs trained:  100\n",
      "Loss: 0.9705\n",
      "Total preserved: 122.8282\n",
      "Edges ablated:  11512\n",
      "Toxic loss:  0.0001267953630303964\n",
      "OWT loss:  12.856703758239746\n",
      "Penalty:  tensor(0.9703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' in'], P(Alicia) = 4.4632496809526856e-08, logit diff = -4.660686492919922\n",
      "\n",
      "\n",
      "loss.item()=2.7984204292297363, ablated_edges=11374\n",
      "loss.item()=2.127802848815918, ablated_edges=11380\n",
      "loss.item()=2.6665804386138916, ablated_edges=11398\n",
      "loss.item()=2.243842601776123, ablated_edges=11413\n",
      "loss.item()=1.6835739612579346, ablated_edges=11439\n",
      "loss.item()=1.359965443611145, ablated_edges=11460\n",
      "loss.item()=3.351832866668701, ablated_edges=11482\n",
      "loss.item()=1.2168368101119995, ablated_edges=11488\n",
      "loss.item()=3.0556063652038574, ablated_edges=11507\n",
      "loss.item()=3.55722713470459, ablated_edges=11493\n",
      "Epochs trained:  110\n",
      "Loss: 3.5572\n",
      "Total preserved: 163.6128\n",
      "Edges ablated:  11493\n",
      "Toxic loss:  2.101073741912842\n",
      "OWT loss:  10.77574634552002\n",
      "Penalty:  tensor(1.4562, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.7489297389984131, logit diff = 4.850994110107422\n",
      "\n",
      "\n",
      "loss.item()=2.001561164855957, ablated_edges=11447\n",
      "loss.item()=1.5467416048049927, ablated_edges=11467\n",
      "loss.item()=1.8766906261444092, ablated_edges=11471\n",
      "loss.item()=2.3812928199768066, ablated_edges=11470\n",
      "loss.item()=1.4281905889511108, ablated_edges=11487\n",
      "loss.item()=1.2102205753326416, ablated_edges=11502\n",
      "loss.item()=1.2915931940078735, ablated_edges=11504\n",
      "loss.item()=5.041746616363525, ablated_edges=11459\n",
      "loss.item()=2.834871768951416, ablated_edges=11400\n",
      "loss.item()=2.272005319595337, ablated_edges=11414\n",
      "Epochs trained:  120\n",
      "Loss: 2.2720\n",
      "Total preserved: 228.7803\n",
      "Edges ablated:  11416\n",
      "Toxic loss:  0.0070806220173835754\n",
      "OWT loss:  11.21285629272461\n",
      "Penalty:  tensor(2.2649, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 9.719856564061047e-08, logit diff = -16.142893075942993\n",
      "\n",
      "\n",
      "loss.item()=3.2965636253356934, ablated_edges=11384\n",
      "loss.item()=2.539329767227173, ablated_edges=11389\n",
      "loss.item()=2.2947840690612793, ablated_edges=11410\n",
      "loss.item()=1.9740607738494873, ablated_edges=11423\n",
      "loss.item()=1.77577543258667, ablated_edges=11442\n",
      "loss.item()=1.7890877723693848, ablated_edges=11460\n",
      "loss.item()=1.4151248931884766, ablated_edges=11476\n",
      "loss.item()=1.2668156623840332, ablated_edges=11487\n",
      "loss.item()=1.2920029163360596, ablated_edges=11497\n",
      "loss.item()=1.148802638053894, ablated_edges=11517\n",
      "Epochs trained:  130\n",
      "Loss: 1.1488\n",
      "Total preserved: 102.4210\n",
      "Edges ablated:  11517\n",
      "Toxic loss:  0.03241408243775368\n",
      "OWT loss:  9.21230697631836\n",
      "Penalty:  tensor(1.1164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.37691372632980347, logit diff = 2.126617431640625\n",
      "\n",
      "\n",
      "loss.item()=1.2866501808166504, ablated_edges=11516\n",
      "loss.item()=1.163481593132019, ablated_edges=11523\n",
      "loss.item()=1.449032187461853, ablated_edges=11515\n",
      "loss.item()=3.13913631439209, ablated_edges=11521\n",
      "loss.item()=2.761695384979248, ablated_edges=11468\n",
      "loss.item()=2.3879201412200928, ablated_edges=11477\n",
      "loss.item()=1.883220911026001, ablated_edges=11489\n",
      "loss.item()=1.4947710037231445, ablated_edges=11505\n",
      "loss.item()=1.249772071838379, ablated_edges=11521\n",
      "loss.item()=1.14024019241333, ablated_edges=11533\n",
      "Epochs trained:  140\n",
      "Loss: 1.1402\n",
      "Total preserved: 91.4899\n",
      "Edges ablated:  11533\n",
      "Toxic loss:  0.051510073244571686\n",
      "OWT loss:  12.78036117553711\n",
      "Penalty:  tensor(1.0887, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.8640482425689697, logit diff = 5.825894355773926\n",
      "\n",
      "\n",
      "loss.item()=2.9305787086486816, ablated_edges=11400\n",
      "loss.item()=2.489767551422119, ablated_edges=11421\n",
      "loss.item()=1.9466779232025146, ablated_edges=11462\n",
      "loss.item()=1.5316969156265259, ablated_edges=11494\n",
      "loss.item()=2.159557342529297, ablated_edges=11515\n",
      "loss.item()=2.334414005279541, ablated_edges=11468\n",
      "loss.item()=2.1515018939971924, ablated_edges=11475\n",
      "loss.item()=1.7379167079925537, ablated_edges=11487\n",
      "loss.item()=1.3875271081924438, ablated_edges=11506\n",
      "loss.item()=2.4961791038513184, ablated_edges=11514\n",
      "Epochs trained:  150\n",
      "Loss: 2.4962\n",
      "Total preserved: 107.0124\n",
      "Edges ablated:  11514\n",
      "Toxic loss:  1.115719199180603\n",
      "OWT loss:  8.241828918457031\n",
      "Penalty:  tensor(1.3805, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9996114373207092, logit diff = 18.876506805419922\n",
      "\n",
      "\n",
      "loss.item()=3.3649673461914062, ablated_edges=11442\n",
      "loss.item()=2.6819920539855957, ablated_edges=11432\n",
      "loss.item()=2.132070779800415, ablated_edges=11467\n",
      "loss.item()=2.383917808532715, ablated_edges=11489\n",
      "loss.item()=1.7039508819580078, ablated_edges=11502\n",
      "loss.item()=1.4821784496307373, ablated_edges=11518\n",
      "loss.item()=1.4094727039337158, ablated_edges=11528\n",
      "loss.item()=1.501874566078186, ablated_edges=11533\n",
      "loss.item()=3.2236328125, ablated_edges=11520\n",
      "loss.item()=2.3082144260406494, ablated_edges=11514\n",
      "Epochs trained:  160\n",
      "Loss: 2.3082\n",
      "Total preserved: 149.4135\n",
      "Edges ablated:  11512\n",
      "Toxic loss:  0.23136714100837708\n",
      "OWT loss:  9.956452369689941\n",
      "Penalty:  tensor(2.0768, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 4.7926809202181175e-05, logit diff = -9.944294929504395\n",
      "\n",
      "\n",
      "loss.item()=1.9332069158554077, ablated_edges=11505\n",
      "loss.item()=4.0246148109436035, ablated_edges=11498\n",
      "loss.item()=1.6103941202163696, ablated_edges=11504\n",
      "loss.item()=2.3418772220611572, ablated_edges=11498\n",
      "loss.item()=2.4169931411743164, ablated_edges=11495\n",
      "loss.item()=2.1095774173736572, ablated_edges=11506\n",
      "loss.item()=1.6032830476760864, ablated_edges=11512\n",
      "loss.item()=1.3698080778121948, ablated_edges=11526\n",
      "loss.item()=1.191612958908081, ablated_edges=11536\n",
      "loss.item()=1.0655852556228638, ablated_edges=11547\n",
      "Epochs trained:  170\n",
      "Loss: 1.0656\n",
      "Total preserved: 71.5093\n",
      "Edges ablated:  11547\n",
      "Toxic loss:  9.718968067318201e-05\n",
      "OWT loss:  11.96686840057373\n",
      "Penalty:  tensor(1.0655, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9855186939239502, logit diff = 13.3824462890625\n",
      "\n",
      "\n",
      "loss.item()=1.6187347173690796, ablated_edges=11526\n",
      "loss.item()=1.3459382057189941, ablated_edges=11536\n",
      "loss.item()=1.1816679239273071, ablated_edges=11542\n",
      "loss.item()=1.1575970649719238, ablated_edges=11546\n",
      "loss.item()=1.159845232963562, ablated_edges=11550\n",
      "loss.item()=1.0225306749343872, ablated_edges=11552\n",
      "loss.item()=1.1884534358978271, ablated_edges=11553\n",
      "loss.item()=1.5846511125564575, ablated_edges=11547\n",
      "loss.item()=2.241537094116211, ablated_edges=11538\n",
      "loss.item()=1.5210436582565308, ablated_edges=11532\n",
      "Epochs trained:  180\n",
      "Loss: 1.5210\n",
      "Total preserved: 94.4879\n",
      "Edges ablated:  11533\n",
      "Toxic loss:  0.018686387687921524\n",
      "OWT loss:  9.162386894226074\n",
      "Penalty:  tensor(1.5024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999618530273438, logit diff = 11.128287315368652\n",
      "\n",
      "\n",
      "loss.item()=3.9201502799987793, ablated_edges=11393\n",
      "loss.item()=3.8133795261383057, ablated_edges=11415\n",
      "loss.item()=3.40504789352417, ablated_edges=11450\n",
      "loss.item()=2.5718085765838623, ablated_edges=11470\n",
      "loss.item()=2.9473299980163574, ablated_edges=11463\n",
      "loss.item()=2.7051777839660645, ablated_edges=11463\n",
      "loss.item()=2.2156412601470947, ablated_edges=11485\n",
      "loss.item()=1.9711591005325317, ablated_edges=11506\n",
      "loss.item()=1.6527265310287476, ablated_edges=11519\n",
      "loss.item()=1.5290839672088623, ablated_edges=11526\n",
      "Epochs trained:  190\n",
      "Loss: 1.5291\n",
      "Total preserved: 90.2654\n",
      "Edges ablated:  11526\n",
      "Toxic loss:  0.0035994865465909243\n",
      "OWT loss:  9.38694953918457\n",
      "Penalty:  tensor(1.5255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9978824257850647, logit diff = 10.11068344116211\n",
      "\n",
      "\n",
      "loss.item()=1.6621088981628418, ablated_edges=11528\n",
      "loss.item()=2.7117161750793457, ablated_edges=11504\n",
      "loss.item()=3.584662914276123, ablated_edges=11449\n",
      "loss.item()=3.145921230316162, ablated_edges=11470\n",
      "loss.item()=2.331669807434082, ablated_edges=11496\n",
      "loss.item()=2.0124127864837646, ablated_edges=11515\n",
      "loss.item()=2.0318820476531982, ablated_edges=11524\n",
      "loss.item()=1.7559452056884766, ablated_edges=11528\n",
      "loss.item()=1.7676150798797607, ablated_edges=11536\n",
      "loss.item()=10.023883819580078, ablated_edges=11478\n",
      "Epochs trained:  200\n",
      "Loss: 10.0239\n",
      "Total preserved: 164.5297\n",
      "Edges ablated:  11475\n",
      "Toxic loss:  7.07880163192749\n",
      "OWT loss:  9.53067684173584\n",
      "Penalty:  tensor(2.9451, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' D'], P(Alicia) = 3.675195330288261e-06, logit diff = 7.057888031005859\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = (regularization_strength * penalty + alpha * tox_loss) #+ owt_loss\n",
    "            # loss = alpha * tox_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            \n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_ioi_sentence = \"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\"\n",
    "                correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "                other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "                test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "                generation = model(test_ioi_tokens)[0][:, -1]\n",
    "                probs = torch.softmax(generation, dim=-1)\n",
    "                print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")\n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            print(\"\\n\")\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    # epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    # log_every = int(input('set log frequency'))\n",
    "    # edge_threshold = int(input('set edge threshold'))\n",
    "    epochs_left = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/alternative_sufficient_masks_params_dict_lambda={regularization_strength}_{alpha=}_{means_ioi=}_{template_type=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train mask over known circuit\n",
    "Train mask over the circuit from the paper, as given by a run of ACDC++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed': tensor([]),\n",
       " 'a0.0': tensor([1.]),\n",
       " 'a0.1': tensor([0.]),\n",
       " 'a0.2': tensor([1.]),\n",
       " 'a0.3': tensor([0.]),\n",
       " 'a0.4': tensor([1.]),\n",
       " 'a0.5': tensor([0.]),\n",
       " 'a0.6': tensor([1.]),\n",
       " 'a0.7': tensor([1.]),\n",
       " 'a0.8': tensor([1.]),\n",
       " 'a0.9': tensor([1.]),\n",
       " 'a0.10': tensor([0.]),\n",
       " 'a0.11': tensor([1.]),\n",
       " 'm0': tensor([0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.]),\n",
       " 'a1.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm2': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1.]),\n",
       " 'a3.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0.]),\n",
       " 'a3.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'm3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "         1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.]),\n",
       " 'a5.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.]),\n",
       " 'a5.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         0., 1., 0., 0., 0., 1.]),\n",
       " 'a6.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 1.]),\n",
       " 'a6.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.9': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         0., 1., 1., 1., 1., 1., 0.]),\n",
       " 'a6.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
       "         1.]),\n",
       " 'a7.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.9': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0.]),\n",
       " 'a7.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'm7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.]),\n",
       " 'a8.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1.]),\n",
       " 'a9.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]),\n",
       " 'a9.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0.]),\n",
       " 'a9.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]),\n",
       " 'a9.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1.]),\n",
       " 'a10.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 1.]),\n",
       " 'a10.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.7': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 1.]),\n",
       " 'a10.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'm10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.10': tensor([1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.]),\n",
       " 'a11.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'output': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mask_utils import get_nodes_and_edges\n",
    "with open(\"models/acdcpp_mask_params.pkl\", \"rb\") as f:\n",
    "    acdc_mask_params = pickle.load(f)\n",
    "\n",
    "_, _, acdc_Edges, acdc_mask_dict = get_nodes_and_edges(mask_params=acdc_mask_params)\n",
    "acdc_mask_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([157])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acdc_mask_dict['output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22863/1329432646.py:48: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10673f0ad7234b04a3294330b8b5f2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=1.5056736469268799, ablated_edges=80\n",
      "loss.item()=-0.500084400177002, ablated_edges=87\n",
      "loss.item()=-0.0825948715209961, ablated_edges=87\n",
      "loss.item()=0.15455102920532227, ablated_edges=88\n",
      "loss.item()=0.7475423812866211, ablated_edges=95\n",
      "loss.item()=0.7848529815673828, ablated_edges=94\n",
      "loss.item()=0.06402826309204102, ablated_edges=98\n",
      "loss.item()=-0.9390511512756348, ablated_edges=96\n",
      "loss.item()=0.6501531600952148, ablated_edges=98\n",
      "loss.item()=0.626798152923584, ablated_edges=98\n",
      "Epochs trained:  10\n",
      "Loss: 0.6268\n",
      "Total preserved: 11512.8232\n",
      "Edges ablated:  98\n",
      "Toxic loss:  23.036602020263672\n",
      "OWT loss:  5.234118461608887\n",
      "Penalty:  0\n",
      "\n",
      "\n",
      "loss.item()=-3.0186614990234375, ablated_edges=103\n",
      "loss.item()=-0.8208189010620117, ablated_edges=93\n",
      "loss.item()=0.038724422454833984, ablated_edges=94\n",
      "loss.item()=-0.08931589126586914, ablated_edges=95\n",
      "loss.item()=-0.5814499855041504, ablated_edges=98\n",
      "loss.item()=-0.1508636474609375, ablated_edges=102\n",
      "loss.item()=2.3883941173553467, ablated_edges=98\n",
      "loss.item()=1.5119729042053223, ablated_edges=96\n",
      "loss.item()=-0.7001581192016602, ablated_edges=100\n",
      "loss.item()=-0.5811986923217773, ablated_edges=97\n",
      "Epochs trained:  20\n",
      "Loss: -0.5812\n",
      "Total preserved: 11511.8525\n",
      "Edges ablated:  97\n",
      "Toxic loss:  27.467458724975586\n",
      "OWT loss:  4.912292957305908\n",
      "Penalty:  0\n",
      "\n",
      "\n",
      "loss.item()=1.1091012954711914, ablated_edges=100\n",
      "loss.item()=-0.7198200225830078, ablated_edges=98\n",
      "loss.item()=-1.066877841949463, ablated_edges=100\n",
      "loss.item()=-4.244425296783447, ablated_edges=105\n",
      "loss.item()=-5.552443981170654, ablated_edges=98\n",
      "loss.item()=-6.534358024597168, ablated_edges=91\n",
      "loss.item()=-7.882603168487549, ablated_edges=89\n",
      "loss.item()=-8.790934562683105, ablated_edges=88\n",
      "loss.item()=-8.656744003295898, ablated_edges=87\n",
      "loss.item()=-8.970634460449219, ablated_edges=89\n",
      "Epochs trained:  30\n",
      "Loss: -8.9706\n",
      "Total preserved: 11524.6582\n",
      "Edges ablated:  89\n",
      "Toxic loss:  19.04596519470215\n",
      "OWT loss:  5.210751056671143\n",
      "Penalty:  tensor(10.3722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-10.965009689331055, ablated_edges=88\n",
      "loss.item()=-13.547128677368164, ablated_edges=90\n",
      "loss.item()=-13.717792510986328, ablated_edges=87\n",
      "loss.item()=-16.68755340576172, ablated_edges=86\n",
      "loss.item()=-16.444446563720703, ablated_edges=82\n",
      "loss.item()=-16.821895599365234, ablated_edges=82\n",
      "loss.item()=-17.48927116394043, ablated_edges=80\n",
      "loss.item()=-21.36284828186035, ablated_edges=81\n",
      "loss.item()=-21.52306365966797, ablated_edges=80\n",
      "loss.item()=-22.231990814208984, ablated_edges=77\n",
      "Epochs trained:  40\n",
      "Loss: -22.2320\n",
      "Total preserved: 11534.5127\n",
      "Edges ablated:  78\n",
      "Toxic loss:  26.378662109375\n",
      "OWT loss:  4.959319114685059\n",
      "Penalty:  tensor(21.9156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-22.69232940673828, ablated_edges=77\n",
      "loss.item()=-23.534677505493164, ablated_edges=78\n",
      "loss.item()=-25.442670822143555, ablated_edges=76\n",
      "loss.item()=-27.155630111694336, ablated_edges=79\n",
      "loss.item()=-28.782812118530273, ablated_edges=76\n",
      "loss.item()=-28.164342880249023, ablated_edges=77\n",
      "loss.item()=-29.027385711669922, ablated_edges=72\n",
      "loss.item()=-30.837371826171875, ablated_edges=80\n",
      "loss.item()=-32.40890121459961, ablated_edges=72\n",
      "loss.item()=-32.99948501586914, ablated_edges=72\n",
      "Epochs trained:  50\n",
      "Loss: -32.9995\n",
      "Total preserved: 11537.1387\n",
      "Edges ablated:  72\n",
      "Toxic loss:  25.70037269592285\n",
      "OWT loss:  5.598297119140625\n",
      "Penalty:  tensor(33.4577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-33.814735412597656, ablated_edges=70\n",
      "loss.item()=-34.977874755859375, ablated_edges=74\n",
      "loss.item()=-38.72045135498047, ablated_edges=76\n",
      "loss.item()=-38.59954833984375, ablated_edges=72\n",
      "loss.item()=-37.44197082519531, ablated_edges=75\n",
      "loss.item()=-40.775421142578125, ablated_edges=69\n",
      "loss.item()=-39.43312454223633, ablated_edges=69\n",
      "loss.item()=-43.922523498535156, ablated_edges=70\n",
      "loss.item()=-44.29679870605469, ablated_edges=63\n",
      "loss.item()=-44.6660270690918, ablated_edges=69\n",
      "Epochs trained:  60\n",
      "Loss: -44.6660\n",
      "Total preserved: 11543.3457\n",
      "Edges ablated:  68\n",
      "Toxic loss:  24.17945098876953\n",
      "OWT loss:  5.188914775848389\n",
      "Penalty:  tensor(45.0191, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-45.43226623535156, ablated_edges=68\n",
      "loss.item()=-47.554931640625, ablated_edges=67\n",
      "loss.item()=-48.48405456542969, ablated_edges=70\n",
      "loss.item()=-49.8265495300293, ablated_edges=67\n",
      "loss.item()=-51.74678421020508, ablated_edges=66\n",
      "loss.item()=-51.73463821411133, ablated_edges=69\n",
      "loss.item()=-52.265323638916016, ablated_edges=70\n",
      "loss.item()=-54.762516021728516, ablated_edges=71\n",
      "loss.item()=-55.7078971862793, ablated_edges=68\n",
      "loss.item()=-56.24128723144531, ablated_edges=67\n",
      "Epochs trained:  70\n",
      "Loss: -56.2413\n",
      "Total preserved: 11541.6113\n",
      "Edges ablated:  67\n",
      "Toxic loss:  23.93475341796875\n",
      "OWT loss:  5.099553108215332\n",
      "Penalty:  tensor(56.5539, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-57.00583267211914, ablated_edges=69\n",
      "loss.item()=-57.7839469909668, ablated_edges=70\n",
      "loss.item()=-58.821163177490234, ablated_edges=67\n",
      "loss.item()=-61.0757942199707, ablated_edges=66\n",
      "loss.item()=-63.463558197021484, ablated_edges=66\n",
      "loss.item()=-64.74162292480469, ablated_edges=62\n",
      "loss.item()=-65.59223175048828, ablated_edges=61\n",
      "loss.item()=-65.37863159179688, ablated_edges=66\n",
      "loss.item()=-66.93404388427734, ablated_edges=61\n",
      "loss.item()=-67.35196685791016, ablated_edges=66\n",
      "Epochs trained:  80\n",
      "Loss: -67.3520\n",
      "Total preserved: 11547.4160\n",
      "Edges ablated:  64\n",
      "Toxic loss:  22.907915115356445\n",
      "OWT loss:  5.359364986419678\n",
      "Penalty:  tensor(68.1298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-70.22889709472656, ablated_edges=59\n",
      "loss.item()=-69.60752868652344, ablated_edges=62\n",
      "loss.item()=-71.21782684326172, ablated_edges=64\n",
      "loss.item()=-72.4986801147461, ablated_edges=64\n",
      "loss.item()=-74.5802001953125, ablated_edges=66\n",
      "loss.item()=-74.36848449707031, ablated_edges=65\n",
      "loss.item()=-76.47356414794922, ablated_edges=63\n",
      "loss.item()=-78.4028549194336, ablated_edges=63\n",
      "loss.item()=-77.62654876708984, ablated_edges=66\n",
      "loss.item()=-80.140625, ablated_edges=64\n",
      "Epochs trained:  90\n",
      "Loss: -80.1406\n",
      "Total preserved: 11547.1406\n",
      "Edges ablated:  64\n",
      "Toxic loss:  23.180822372436523\n",
      "OWT loss:  4.170817852020264\n",
      "Penalty:  tensor(79.6753, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-82.50818634033203, ablated_edges=66\n",
      "loss.item()=-81.01814270019531, ablated_edges=64\n",
      "loss.item()=-83.3720474243164, ablated_edges=66\n",
      "loss.item()=-83.98469543457031, ablated_edges=58\n",
      "loss.item()=-87.44886016845703, ablated_edges=61\n",
      "loss.item()=-86.34146881103516, ablated_edges=58\n",
      "loss.item()=-85.54612731933594, ablated_edges=57\n",
      "loss.item()=-90.238525390625, ablated_edges=54\n",
      "loss.item()=-90.92143249511719, ablated_edges=54\n",
      "loss.item()=-92.84722137451172, ablated_edges=57\n",
      "Epochs trained:  100\n",
      "Loss: -92.8472\n",
      "Total preserved: 11554.1641\n",
      "Edges ablated:  57\n",
      "Toxic loss:  33.43305587768555\n",
      "OWT loss:  5.11730432510376\n",
      "Penalty:  tensor(91.2779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-91.27748107910156, ablated_edges=59\n",
      "loss.item()=-93.88461303710938, ablated_edges=54\n",
      "loss.item()=-94.77798461914062, ablated_edges=50\n",
      "loss.item()=-94.69718933105469, ablated_edges=52\n",
      "loss.item()=-95.52283477783203, ablated_edges=53\n",
      "loss.item()=-97.91181182861328, ablated_edges=58\n",
      "loss.item()=-100.29170989990234, ablated_edges=53\n",
      "loss.item()=-101.28544616699219, ablated_edges=52\n",
      "loss.item()=-99.9444808959961, ablated_edges=54\n",
      "loss.item()=-102.86627197265625, ablated_edges=53\n",
      "Epochs trained:  110\n",
      "Loss: -102.8663\n",
      "Total preserved: 11557.9971\n",
      "Edges ablated:  53\n",
      "Toxic loss:  28.86589813232422\n",
      "OWT loss:  5.773064613342285\n",
      "Penalty:  tensor(102.8662, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-103.58384704589844, ablated_edges=52\n",
      "loss.item()=-105.84606170654297, ablated_edges=52\n",
      "loss.item()=-107.2335433959961, ablated_edges=53\n",
      "loss.item()=-106.85431671142578, ablated_edges=53\n",
      "loss.item()=-109.91695404052734, ablated_edges=52\n",
      "loss.item()=-110.24185943603516, ablated_edges=54\n",
      "loss.item()=-111.40071105957031, ablated_edges=50\n",
      "loss.item()=-112.56986999511719, ablated_edges=50\n",
      "loss.item()=-113.40050506591797, ablated_edges=53\n",
      "loss.item()=-113.72864532470703, ablated_edges=52\n",
      "Epochs trained:  120\n",
      "Loss: -113.7286\n",
      "Total preserved: 11557.8770\n",
      "Edges ablated:  52\n",
      "Toxic loss:  28.943449020385742\n",
      "OWT loss:  6.483026027679443\n",
      "Penalty:  tensor(114.4230, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-116.54255676269531, ablated_edges=53\n",
      "loss.item()=-116.79327392578125, ablated_edges=51\n",
      "loss.item()=-115.19482421875, ablated_edges=51\n",
      "loss.item()=-118.75401306152344, ablated_edges=53\n",
      "loss.item()=-122.14277648925781, ablated_edges=49\n",
      "loss.item()=-121.06953430175781, ablated_edges=48\n",
      "loss.item()=-122.05601501464844, ablated_edges=49\n",
      "loss.item()=-123.47509002685547, ablated_edges=48\n",
      "loss.item()=-124.31399536132812, ablated_edges=50\n",
      "loss.item()=-127.36456298828125, ablated_edges=46\n",
      "Epochs trained:  130\n",
      "Loss: -127.3646\n",
      "Total preserved: 11562.5859\n",
      "Edges ablated:  46\n",
      "Toxic loss:  32.48900604248047\n",
      "OWT loss:  5.165423393249512\n",
      "Penalty:  tensor(126.0322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-126.47238159179688, ablated_edges=47\n",
      "loss.item()=-128.5781707763672, ablated_edges=49\n",
      "loss.item()=-130.5258331298828, ablated_edges=46\n",
      "loss.item()=-129.74996948242188, ablated_edges=47\n",
      "loss.item()=-132.48599243164062, ablated_edges=48\n",
      "loss.item()=-133.76837158203125, ablated_edges=47\n",
      "loss.item()=-132.95957946777344, ablated_edges=46\n",
      "loss.item()=-133.96261596679688, ablated_edges=45\n",
      "loss.item()=-135.52638244628906, ablated_edges=47\n",
      "loss.item()=-137.492431640625, ablated_edges=50\n",
      "Epochs trained:  140\n",
      "Loss: -137.4924\n",
      "Total preserved: 11560.9385\n",
      "Edges ablated:  50\n",
      "Toxic loss:  27.15222930908203\n",
      "OWT loss:  5.513188362121582\n",
      "Penalty:  tensor(137.5752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-138.09088134765625, ablated_edges=46\n",
      "loss.item()=-139.6414031982422, ablated_edges=46\n",
      "loss.item()=-140.9293975830078, ablated_edges=45\n",
      "loss.item()=-141.51370239257812, ablated_edges=49\n",
      "loss.item()=-144.06930541992188, ablated_edges=46\n",
      "loss.item()=-143.3538055419922, ablated_edges=47\n",
      "loss.item()=-146.1291046142578, ablated_edges=44\n",
      "loss.item()=-146.28192138671875, ablated_edges=47\n",
      "loss.item()=-148.14944458007812, ablated_edges=48\n",
      "loss.item()=-150.17247009277344, ablated_edges=50\n",
      "Epochs trained:  150\n",
      "Loss: -150.1725\n",
      "Total preserved: 11561.4805\n",
      "Edges ablated:  50\n",
      "Toxic loss:  33.0401725769043\n",
      "OWT loss:  5.578667640686035\n",
      "Penalty:  tensor(149.1431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-152.55580139160156, ablated_edges=49\n",
      "loss.item()=-151.85586547851562, ablated_edges=49\n",
      "loss.item()=-153.99522399902344, ablated_edges=46\n",
      "loss.item()=-153.05007934570312, ablated_edges=47\n",
      "loss.item()=-155.26657104492188, ablated_edges=45\n",
      "loss.item()=-155.52505493164062, ablated_edges=49\n",
      "loss.item()=-158.27174377441406, ablated_edges=46\n",
      "loss.item()=-158.82046508789062, ablated_edges=43\n",
      "loss.item()=-159.7530059814453, ablated_edges=48\n",
      "loss.item()=-161.92526245117188, ablated_edges=46\n",
      "Epochs trained:  160\n",
      "Loss: -161.9253\n",
      "Total preserved: 11562.2822\n",
      "Edges ablated:  46\n",
      "Toxic loss:  33.64940643310547\n",
      "OWT loss:  5.520321846008301\n",
      "Penalty:  tensor(160.7157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-161.7630157470703, ablated_edges=43\n",
      "loss.item()=-162.72824096679688, ablated_edges=43\n",
      "loss.item()=-163.37709045410156, ablated_edges=47\n",
      "loss.item()=-164.8791961669922, ablated_edges=44\n",
      "loss.item()=-167.94241333007812, ablated_edges=45\n",
      "loss.item()=-168.17257690429688, ablated_edges=46\n",
      "loss.item()=-168.0404052734375, ablated_edges=47\n",
      "loss.item()=-169.7171630859375, ablated_edges=49\n",
      "loss.item()=-171.64768981933594, ablated_edges=46\n",
      "loss.item()=-174.03814697265625, ablated_edges=43\n",
      "Epochs trained:  170\n",
      "Loss: -174.0381\n",
      "Total preserved: 11564.3203\n",
      "Edges ablated:  43\n",
      "Toxic loss:  39.49759292602539\n",
      "OWT loss:  6.16975736618042\n",
      "Penalty:  tensor(172.3084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-174.89840698242188, ablated_edges=45\n",
      "loss.item()=-174.6654510498047, ablated_edges=48\n",
      "loss.item()=-174.72088623046875, ablated_edges=45\n",
      "loss.item()=-178.4546661376953, ablated_edges=46\n",
      "loss.item()=-177.6720428466797, ablated_edges=47\n",
      "loss.item()=-179.65830993652344, ablated_edges=48\n",
      "loss.item()=-179.35630798339844, ablated_edges=41\n",
      "loss.item()=-183.06385803222656, ablated_edges=44\n",
      "loss.item()=-184.6969757080078, ablated_edges=46\n",
      "loss.item()=-184.05078125, ablated_edges=44\n",
      "Epochs trained:  180\n",
      "Loss: -184.0508\n",
      "Total preserved: 11566.2373\n",
      "Edges ablated:  44\n",
      "Toxic loss:  28.797325134277344\n",
      "OWT loss:  5.611840724945068\n",
      "Penalty:  tensor(183.9032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-184.4897003173828, ablated_edges=47\n",
      "loss.item()=-185.37271118164062, ablated_edges=46\n",
      "loss.item()=-186.9844207763672, ablated_edges=43\n",
      "loss.item()=-188.4407501220703, ablated_edges=46\n",
      "loss.item()=-189.3105010986328, ablated_edges=44\n",
      "loss.item()=-191.02035522460938, ablated_edges=44\n",
      "loss.item()=-191.65814208984375, ablated_edges=43\n",
      "loss.item()=-191.11868286132812, ablated_edges=45\n",
      "loss.item()=-195.22610473632812, ablated_edges=44\n",
      "loss.item()=-195.29010009765625, ablated_edges=44\n",
      "Epochs trained:  190\n",
      "Loss: -195.2901\n",
      "Total preserved: 11565.8291\n",
      "Edges ablated:  44\n",
      "Toxic loss:  25.891029357910156\n",
      "OWT loss:  5.3506364822387695\n",
      "Penalty:  tensor(195.4625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-197.07337951660156, ablated_edges=46\n",
      "loss.item()=-197.45147705078125, ablated_edges=47\n",
      "loss.item()=-199.31268310546875, ablated_edges=44\n",
      "loss.item()=-198.38023376464844, ablated_edges=43\n",
      "loss.item()=-201.58419799804688, ablated_edges=44\n",
      "loss.item()=-199.99661254882812, ablated_edges=44\n",
      "loss.item()=-202.88490295410156, ablated_edges=46\n",
      "loss.item()=-203.29649353027344, ablated_edges=44\n",
      "loss.item()=-205.46502685546875, ablated_edges=41\n",
      "loss.item()=-207.99468994140625, ablated_edges=43\n",
      "Epochs trained:  200\n",
      "Loss: -207.9947\n",
      "Total preserved: 11567.3525\n",
      "Edges ablated:  43\n",
      "Toxic loss:  30.689029693603516\n",
      "OWT loss:  5.198721885681152\n",
      "Penalty:  tensor(207.0556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m     prev_params \u001b[39m=\u001b[39m mask_params\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m epochs_left \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39minput\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mcontinue training for this number of epochs: \u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m log_every \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(\u001b[39minput\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mset log frequency\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m edge_threshold \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39minput\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mset edge threshold\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "toxic_batch_size = 1 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 5\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "    means = pickle.load(f)[0][0]\n",
    "\n",
    "model = load_demo_gpt2(means=means, mask_dict_superset=acdc_mask_dict)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 20 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 0.2 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 0\n",
    "max_steps_per_epoch = 100\n",
    "\n",
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "            # print(f\"{ablated_edges=}\")\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = -1 * (regularization_strength * penalty + alpha * tox_loss) + owt_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            print(\"\\n\")\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    log_every = int(input('set log frequency'))\n",
    "    edge_threshold = int(input('set edge threshold'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/alternative_acdc_cb_subset_mask_params_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model before and after circuit breaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/ioi_sentences_test.pkl\", \"rb\") as f:\n",
    "    ioi_sentences_test = pickle.load(f)\n",
    "    # ioi_sentences_test = [t[2] for t in ioi_sentences_test]\n",
    "\n",
    "with open(\"data/eval_uniform.pkl\", \"rb\") as f:\n",
    "    uniform_samples = pickle.load(f)\n",
    "    uniform_sentences = [t[2] for t in uniform_samples]\n",
    "\n",
    "original_model = load_demo_gpt2(means=False)\n",
    "\n",
    "# with open(\"models/masked_gpt2_mean_ablation_v6.pkl\", \"rb\") as f:\n",
    "#     model.state_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on an ioi_sentence\n",
    "ioi_sentence = ioi_sentences_test[0]\n",
    "print(ioi_sentence)\n",
    "# ioi_tokens = tokenizer(ioi_sentence, return_tensors='pt').input_ids.to('cuda')\n",
    "\n",
    "original_model.eval()\n",
    "original_model.to('cuda')\n",
    "def get_last_token(model, prompt, topk=5):\n",
    "    # generate last token\n",
    "    tokens = tokenizer(prompt, return_tensors='pt').input_ids[:, :-1]\n",
    "\n",
    "    # generate one token, decode original_model(ioi_tokens[:, :-1])\n",
    "    model_outputs = model(tokens)[0]\n",
    "    model_outputs = model_outputs.squeeze(0)[-1]\n",
    "    probs = torch.nn.functional.softmax(model_outputs, dim=-1)\n",
    "\n",
    "    topk_outputs = torch.topk(model_outputs, topk)\n",
    "    topk_tokens = topk_outputs.indices\n",
    "    topk_probs = probs[topk_outputs.indices]\n",
    "    \n",
    "    # decode tokens\n",
    "    for i in range(topk):\n",
    "        print(f\"{tokenizer.decode(topk_tokens[i].unsqueeze(0))}, probability of {topk_probs[i]}\")\n",
    "    topk_tokens_decoded = tokenizer.batch_decode(topk_tokens)\n",
    "    return topk_tokens_decoded, topk_probs\n",
    "\n",
    "print(\"Before ablation\")\n",
    "_ = get_last_token(original_model, ioi_sentence)\n",
    "print()\n",
    "print()\n",
    "print(\"After ablation\")\n",
    "_ = get_last_token(model, ioi_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try on uniform samples\n",
    "for idx in range(3):\n",
    "    print(uniform_sentences[idx])\n",
    "    print(\"Before ablation\")\n",
    "    _ = get_last_token(original_model, uniform_sentences[idx])\n",
    "    print()\n",
    "    print(\"After ablation\")\n",
    "    _ = get_last_token(model, uniform_sentences[idx])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize mask\n",
    "Create the computational graphs in edge attribution patching paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate which nodes will be in the graph\n",
    "connected_nodes = set()\n",
    "# add embed node at position\n",
    "# connected_nodes.add((-1, \"embed\"))\n",
    "n_heads = 12\n",
    "n_layers = 12\n",
    "\n",
    "# associate each node with a position\n",
    "all_possible_nodes = [(-1, \"embed\")]\n",
    "mask_dict = {}\n",
    "# empty tensor\n",
    "mask_dict[\"embed\"] = torch.zeros(size=(0,))\n",
    "for idx in range(len(mask_params)):\n",
    "    if \"attention\" in param_names[idx]:\n",
    "        layer = int(param_names[idx].split(\".\")[1])\n",
    "        for i in range(n_heads):\n",
    "            all_possible_nodes.append((layer, f\"a{layer}.{i}\"))\n",
    "            mask_dict[f\"a{layer}.{i}\"] = mask_params[idx][:,i].detach().cpu()\n",
    "    elif \"mlp\" in param_names[idx]:\n",
    "        layer = int(param_names[idx].split(\".\")[1])\n",
    "        all_possible_nodes.append((layer, f\"m{layer}\"))\n",
    "        mask_dict[f\"m{layer}\"] = mask_params[idx].detach().cpu()\n",
    "all_possible_nodes.append((n_heads, \"output\"))\n",
    "mask_dict[\"output\"] = mask_params[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate where edges are based on the mask\n",
    "# Edge between node i and node j if mask_dict[i][all_possible_nodes.index(j)] == 0\n",
    "sufficient = True\n",
    "\n",
    "edges = []\n",
    "for i in range(len(all_possible_nodes)):\n",
    "    for j in range(len(all_possible_nodes)):\n",
    "        j_index = all_possible_nodes.index(all_possible_nodes[j])\n",
    "        if j_index < len(mask_dict[all_possible_nodes[i][1]]) and mask_dict[all_possible_nodes[i][1]][all_possible_nodes.index(all_possible_nodes[j])] == (1 if sufficient else 0):\n",
    "            edges.append((all_possible_nodes[i], all_possible_nodes[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aligned_graph(all_possible_nodes, edges):\n",
    "    G = pgv.AGraph(strict=False, directed=True)\n",
    "\n",
    "    # Find the maximum layer number for adjusting the graph\n",
    "    max_layer = max(layer for layer, _ in all_possible_nodes if isinstance(layer, int))\n",
    "    nodes_with_edges = set([node for edge in edges for node in edge])\n",
    "    print(nodes_with_edges)\n",
    "    # Add nodes and edges to the graph\n",
    "    # for node in all_possible_nodes:\n",
    "    #     if node in [edge[0] for edge in edges] or node in [edge[1] for edge in edges]:\n",
    "    #         G.add_node(node[1], layer=str(max_layer - node[0]))\n",
    "\n",
    "    for edge in edges:\n",
    "        G.add_edge(edge[1][1], edge[0][1])\n",
    "\n",
    "    # Create subgraphs to ensure nodes of the same layer have the same rank\n",
    "    for layer in range(max_layer, -2, -1):\n",
    "        with G.subgraph(name=f'cluster_{layer}') as s:\n",
    "            s.graph_attr['rank'] = 'same'\n",
    "            for node in nodes_with_edges:\n",
    "                if node[0] == layer:\n",
    "                    s.add_node(node[1])\n",
    "\n",
    "    # Apply layout and render the graph\n",
    "    G.layout(prog='dot')\n",
    "    G.draw('aligned_graph.png')\n",
    "    return Image('aligned_graph.png')\n",
    "\n",
    "# Call the function with your nodes and edges\n",
    "flipped_graph_image = create_aligned_graph(all_possible_nodes, edges)\n",
    "\n",
    "# To display the graph in Jupyter Notebook\n",
    "flipped_graph_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "unlrn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
