{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train different kinds of masks over IOI edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir(\"/data/phillip_guo/circuit-breaking/ioi/\")\n",
    "from models import load_gpt2_weights, load_demo_gpt2, tokenizer\n",
    "from data import retrieve_toxic_data, retrieve_owt_data, retrieve_toxic_data_low_loss, retrieve_toxic_filtered_data, FILTER_DEMO_LEN, CONTEXT_LENGTH\n",
    "from inference import infer_batch_with_owt, infer_batch, prepare_fixed_demo, criterion\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import pickle\n",
    "import datasets\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from itertools import cycle\n",
    "# from eval import evaluate_model\n",
    "from data import batch_text_to_tokens\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train params of mask\n",
    "Train without the original D_train loss term (only mask loss and IOI data loss)\n",
    "Finds necessary (but not sufficient) edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_batch_size = 10 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 1\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "\n",
    "template_type = \"single\"\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None, template_type=template_type)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "# with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "#     means = pickle.load(f)[0][0]\n",
    "means_ioi = True\n",
    "if means_ioi:\n",
    "    with open(\"data/gpt2_ioi_abc_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "else:\n",
    "    with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "\n",
    "model = load_demo_gpt2(means=means)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 50 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 1 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 100\n",
    "max_steps_per_epoch = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Then, Thomas and Sarah went to the school. Sarah gave a bone to Thomas', 'Then, Christine and Kelly went to the station. Kelly gave a drink to Christine', 'Then, Kristen and Gregory went to the school. Gregory gave a ring to Kristen', 'Then, Danielle and Aaron went to the store. Aaron gave a necklace to Danielle', 'Then, Christine and Nathan went to the restaurant. Nathan gave a kiss to Christine', 'Then, Adam and Jessica went to the store. Jessica gave a drink to Adam', 'Then, Thomas and Stephanie went to the garden. Stephanie gave a ring to Thomas', 'Then, Rebecca and Daniel went to the house. Daniel gave a necklace to Rebecca', 'Then, Heather and Emily went to the house. Emily gave a bone to Heather', 'Then, Amber and Daniel went to the store. Daniel gave a kiss to Amber']}\n",
      "{'text': ['Then, Cody and Andrew went to the school. Andrew gave a ring to Cody', 'Then, Jamie and Kelly went to the hospital. Kelly gave a snack to Jamie', 'Then, Sara and David went to the garden. David gave a snack to Sara', 'Then, Richard and Jeremy went to the store. Jeremy gave a drink to Richard', 'Then, Jesse and Erica went to the office. Erica gave a basketball to Jesse', 'Then, Ryan and Christopher went to the school. Christopher gave a bone to Ryan', 'Then, Lindsey and Danielle went to the garden. Danielle gave a drink to Lindsey', 'Then, Dustin and Daniel went to the garden. Daniel gave a ring to Dustin', 'Then, Kelly and Justin went to the restaurant. Justin gave a drink to Kelly', 'Then, Jeremy and Steven went to the hospital. Steven gave a bone to Jeremy']}\n",
      "{'text': ['Then, Matthew and Stephen went to the station. Stephen gave a drink to Matthew', 'Then, Timothy and Andrea went to the garden. Andrea gave a computer to Timothy', 'Then, Jonathan and Paul went to the school. Paul gave a necklace to Jonathan', 'Then, Nathan and Andrea went to the store. Andrea gave a kiss to Nathan', 'Then, Nicholas and Danielle went to the office. Danielle gave a kiss to Nicholas', 'Then, Justin and Brian went to the station. Brian gave a computer to Justin', 'Then, Samuel and Kevin went to the garden. Kevin gave a kiss to Samuel', 'Then, Jeremy and Vanessa went to the store. Vanessa gave a bone to Jeremy', 'Then, Jose and Christopher went to the station. Christopher gave a necklace to Jose', 'Then, Jonathan and Scott went to the store. Scott gave a kiss to Jonathan']}\n",
      "{'text': ['Then, Justin and Adam went to the office. Adam gave a kiss to Justin', 'Then, Bryan and Laura went to the garden. Laura gave a basketball to Bryan', 'Then, Amber and Jesse went to the restaurant. Jesse gave a snack to Amber', 'Then, Gregory and Daniel went to the station. Daniel gave a snack to Gregory', 'Then, David and Courtney went to the hospital. Courtney gave a kiss to David', 'Then, Kelly and Lindsey went to the restaurant. Lindsey gave a kiss to Kelly', 'Then, Lauren and Lindsay went to the store. Lindsay gave a snack to Lauren', 'Then, Tyler and Kelly went to the garden. Kelly gave a ring to Tyler', 'Then, Jamie and Brian went to the school. Brian gave a snack to Jamie', 'Then, Nicholas and Cody went to the restaurant. Cody gave a drink to Nicholas']}\n",
      "{'text': ['Then, Jacob and Amanda went to the house. Amanda gave a kiss to Jacob', 'Then, Richard and Christina went to the restaurant. Christina gave a kiss to Richard', 'Then, Jacob and Tyler went to the restaurant. Tyler gave a bone to Jacob', 'Then, Kevin and Vanessa went to the station. Vanessa gave a bone to Kevin', 'Then, Shannon and Allison went to the school. Allison gave a drink to Shannon', 'Then, David and Nicole went to the store. Nicole gave a computer to David', 'Then, Dustin and Alicia went to the house. Alicia gave a bone to Dustin', 'Then, Stephanie and Charles went to the hospital. Charles gave a necklace to Stephanie', 'Then, Crystal and Amber went to the house. Amber gave a ring to Crystal', 'Then, Amanda and Sara went to the station. Sara gave a necklace to Amanda']}\n",
      "{'text': ['Then, Danielle and Matthew went to the store. Matthew gave a necklace to Danielle', 'Then, Nicholas and Nathan went to the house. Nathan gave a necklace to Nicholas', 'Then, Danielle and Lindsey went to the garden. Lindsey gave a drink to Danielle', 'Then, Jason and Patrick went to the store. Patrick gave a ring to Jason', 'Then, Andrew and Rachel went to the house. Rachel gave a kiss to Andrew', 'Then, Jessica and Brian went to the store. Brian gave a bone to Jessica', 'Then, Stephanie and Erin went to the station. Erin gave a snack to Stephanie', 'Then, Sarah and Tiffany went to the station. Tiffany gave a drink to Sarah', 'Then, Patrick and Bradley went to the store. Bradley gave a necklace to Patrick', 'Then, Brandon and Jeremy went to the garden. Jeremy gave a computer to Brandon']}\n"
     ]
    }
   ],
   "source": [
    "for c, batch in enumerate(toxic_data_loader):\n",
    "    if c > 5:\n",
    "        break\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_182719/2478911762.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001e922c719f4b5a9e2d71a37aca3944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=-125.52879333496094, ablated_edges=3777\n",
      "loss.item()=-135.508056640625, ablated_edges=4482\n",
      "loss.item()=-139.02293395996094, ablated_edges=4864\n",
      "loss.item()=-139.85806274414062, ablated_edges=5091\n",
      "loss.item()=-140.1219024658203, ablated_edges=5265\n",
      "loss.item()=-141.9140625, ablated_edges=5451\n",
      "loss.item()=-141.69163513183594, ablated_edges=5566\n",
      "loss.item()=-141.4807586669922, ablated_edges=5701\n",
      "loss.item()=-143.74453735351562, ablated_edges=5780\n",
      "loss.item()=-142.1165008544922, ablated_edges=5820\n",
      "Epochs trained:  10\n",
      "Loss: -142.1165\n",
      "Total preserved: 5734.8364\n",
      "Edges ablated:  5820\n",
      "Toxic loss:  142.1165008544922\n",
      "OWT loss:  11.655352592468262\n",
      "Penalty:  0\n",
      "Best Token: [' run'], P(Alicia) = 3.401473271957479e-30, logit diff = -31.519729614257812\n",
      "Best Token: [' run'], P(Alicia) = 9.703986885491369e-23, logit diff = 8.248924255371094\n",
      "\n",
      "\n",
      "loss.item()=-142.02871704101562, ablated_edges=5891\n",
      "loss.item()=-142.52169799804688, ablated_edges=5938\n",
      "loss.item()=-142.1531219482422, ablated_edges=5968\n",
      "loss.item()=-143.319580078125, ablated_edges=6028\n",
      "loss.item()=-142.51034545898438, ablated_edges=6037\n",
      "loss.item()=-144.476318359375, ablated_edges=6050\n",
      "loss.item()=-145.3213653564453, ablated_edges=6059\n",
      "loss.item()=-145.71890258789062, ablated_edges=6081\n",
      "loss.item()=-142.8463897705078, ablated_edges=6096\n",
      "loss.item()=-143.86659240722656, ablated_edges=6117\n",
      "Epochs trained:  20\n",
      "Loss: -143.8666\n",
      "Total preserved: 5536.3247\n",
      "Edges ablated:  6117\n",
      "Toxic loss:  143.86659240722656\n",
      "OWT loss:  11.640857696533203\n",
      "Penalty:  0\n",
      "Best Token: [' run'], P(Alicia) = 5.617779285178624e-30, logit diff = -36.78273010253906\n",
      "Best Token: [' run'], P(Alicia) = 2.713547218279776e-22, logit diff = 2.5997161865234375\n",
      "\n",
      "\n",
      "loss.item()=-143.64247131347656, ablated_edges=6134\n",
      "loss.item()=-145.4906463623047, ablated_edges=6007\n",
      "loss.item()=-145.05044555664062, ablated_edges=5832\n",
      "loss.item()=-144.89540100097656, ablated_edges=5595\n",
      "loss.item()=-144.6805877685547, ablated_edges=5399\n",
      "loss.item()=-148.38247680664062, ablated_edges=5126\n",
      "loss.item()=-149.16497802734375, ablated_edges=4886\n",
      "loss.item()=-149.4746551513672, ablated_edges=4654\n",
      "loss.item()=-147.59226989746094, ablated_edges=4473\n",
      "loss.item()=-149.51504516601562, ablated_edges=4305\n",
      "Epochs trained:  30\n",
      "Loss: -149.5150\n",
      "Total preserved: 7313.6689\n",
      "Edges ablated:  4305\n",
      "Toxic loss:  142.9327392578125\n",
      "OWT loss:  13.89667797088623\n",
      "Penalty:  tensor(6.5823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' run'], P(Alicia) = 2.133112733897984e-32, logit diff = -32.78416156768799\n",
      "Best Token: [' run'], P(Alicia) = 2.8765126599896653e-24, logit diff = 2.913341522216797\n",
      "\n",
      "\n",
      "loss.item()=-152.1077880859375, ablated_edges=4128\n",
      "loss.item()=-151.5083465576172, ablated_edges=3981\n",
      "loss.item()=-152.16903686523438, ablated_edges=3783\n",
      "loss.item()=-154.1562042236328, ablated_edges=3642\n",
      "loss.item()=-155.52708435058594, ablated_edges=3494\n",
      "loss.item()=-154.0000762939453, ablated_edges=3373\n",
      "loss.item()=-158.53530883789062, ablated_edges=3217\n",
      "loss.item()=-156.4408416748047, ablated_edges=3107\n",
      "loss.item()=-160.9324951171875, ablated_edges=2999\n",
      "loss.item()=-160.905029296875, ablated_edges=2924\n",
      "Epochs trained:  40\n",
      "Loss: -160.9050\n",
      "Total preserved: 8622.6436\n",
      "Edges ablated:  2924\n",
      "Toxic loss:  144.52200317382812\n",
      "OWT loss:  11.74085521697998\n",
      "Penalty:  tensor(16.3830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' run'], P(Alicia) = 9.67921545498493e-28, logit diff = -24.843769073486328\n",
      "Best Token: [' run'], P(Alicia) = 7.769758637140796e-23, logit diff = 2.535623550415039\n",
      "\n",
      "\n",
      "loss.item()=-161.3028564453125, ablated_edges=2806\n",
      "loss.item()=-160.0240020751953, ablated_edges=2719\n",
      "loss.item()=-160.8936767578125, ablated_edges=2644\n",
      "loss.item()=-164.59243774414062, ablated_edges=2575\n",
      "loss.item()=-164.19764709472656, ablated_edges=2475\n",
      "loss.item()=-165.4586181640625, ablated_edges=2419\n",
      "loss.item()=-168.77879333496094, ablated_edges=2333\n",
      "loss.item()=-165.28199768066406, ablated_edges=2267\n",
      "loss.item()=-167.4475860595703, ablated_edges=2200\n",
      "loss.item()=-169.56752014160156, ablated_edges=2133\n",
      "Epochs trained:  50\n",
      "Loss: -169.5675\n",
      "Total preserved: 9369.6152\n",
      "Edges ablated:  2134\n",
      "Toxic loss:  142.3956298828125\n",
      "OWT loss:  12.549996376037598\n",
      "Penalty:  tensor(27.1719, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['\\n'], P(Alicia) = 6.242844019510585e-09, logit diff = -4.057825088500977\n",
      "Best Token: [' the'], P(Alicia) = 4.411150555938548e-09, logit diff = -3.508199691772461\n",
      "\n",
      "\n",
      "loss.item()=-185.81175231933594, ablated_edges=2753\n",
      "loss.item()=-190.39259338378906, ablated_edges=2257\n",
      "loss.item()=-192.70758056640625, ablated_edges=2011\n",
      "loss.item()=-196.17453002929688, ablated_edges=1876\n",
      "loss.item()=-197.34014892578125, ablated_edges=1777\n",
      "loss.item()=-198.45303344726562, ablated_edges=1668\n",
      "loss.item()=-198.83438110351562, ablated_edges=1611\n",
      "loss.item()=-200.19952392578125, ablated_edges=1554\n",
      "loss.item()=-202.85665893554688, ablated_edges=1516\n",
      "loss.item()=-203.15463256835938, ablated_edges=1503\n",
      "Epochs trained:  60\n",
      "Loss: -203.1546\n",
      "Total preserved: 10052.3965\n",
      "Edges ablated:  1503\n",
      "Toxic loss:  163.95028686523438\n",
      "OWT loss:  59.6721076965332\n",
      "Penalty:  tensor(39.2043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = -15.292076110839844\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = 19.98320770263672\n",
      "\n",
      "\n",
      "loss.item()=-203.88536071777344, ablated_edges=1442\n",
      "loss.item()=-206.30203247070312, ablated_edges=1384\n",
      "loss.item()=-206.83267211914062, ablated_edges=1347\n",
      "loss.item()=-207.90353393554688, ablated_edges=1309\n",
      "loss.item()=-209.233154296875, ablated_edges=1298\n",
      "loss.item()=-208.68682861328125, ablated_edges=1268\n",
      "loss.item()=-201.25189208984375, ablated_edges=2242\n",
      "loss.item()=-212.23526000976562, ablated_edges=1566\n",
      "loss.item()=-212.81715393066406, ablated_edges=1325\n",
      "loss.item()=-218.1834716796875, ablated_edges=1213\n",
      "Epochs trained:  70\n",
      "Loss: -218.1835\n",
      "Total preserved: 10285.7100\n",
      "Edges ablated:  1213\n",
      "Toxic loss:  167.78347778320312\n",
      "OWT loss:  31.3303279876709\n",
      "Penalty:  tensor(50.4000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = -13.390045166015625\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = 16.99795913696289\n",
      "\n",
      "\n",
      "loss.item()=-215.6019744873047, ablated_edges=1146\n",
      "loss.item()=-218.40792846679688, ablated_edges=1083\n",
      "loss.item()=-221.91220092773438, ablated_edges=1057\n",
      "loss.item()=-223.43179321289062, ablated_edges=1035\n",
      "loss.item()=-224.1591339111328, ablated_edges=1011\n",
      "loss.item()=-222.8785400390625, ablated_edges=983\n",
      "loss.item()=-225.50978088378906, ablated_edges=950\n",
      "loss.item()=-225.0713348388672, ablated_edges=941\n",
      "loss.item()=-227.8447723388672, ablated_edges=933\n",
      "loss.item()=-230.73568725585938, ablated_edges=916\n",
      "Epochs trained:  80\n",
      "Loss: -230.7357\n",
      "Total preserved: 10603.1445\n",
      "Edges ablated:  916\n",
      "Toxic loss:  168.17713928222656\n",
      "OWT loss:  34.781497955322266\n",
      "Penalty:  tensor(62.5585, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = -17.63922119140625\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = 17.901016235351562\n",
      "\n",
      "\n",
      "loss.item()=-228.9368133544922, ablated_edges=901\n",
      "loss.item()=-229.97634887695312, ablated_edges=882\n",
      "loss.item()=-233.1272430419922, ablated_edges=882\n",
      "loss.item()=-232.17340087890625, ablated_edges=863\n",
      "loss.item()=-237.093017578125, ablated_edges=851\n",
      "loss.item()=-236.70220947265625, ablated_edges=856\n",
      "loss.item()=-237.5402069091797, ablated_edges=841\n",
      "loss.item()=-239.23995971679688, ablated_edges=842\n",
      "loss.item()=-237.004638671875, ablated_edges=823\n",
      "loss.item()=-240.35903930664062, ablated_edges=810\n",
      "Epochs trained:  90\n",
      "Loss: -240.3590\n",
      "Total preserved: 10700.6650\n",
      "Edges ablated:  810\n",
      "Toxic loss:  166.52444458007812\n",
      "OWT loss:  36.30588912963867\n",
      "Penalty:  tensor(73.8346, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = -24.61954116821289\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = 17.69261932373047\n",
      "\n",
      "\n",
      "loss.item()=-241.99002075195312, ablated_edges=779\n",
      "loss.item()=-245.57818603515625, ablated_edges=747\n",
      "loss.item()=-242.96719360351562, ablated_edges=743\n",
      "loss.item()=-245.98641967773438, ablated_edges=727\n",
      "loss.item()=-243.88925170898438, ablated_edges=723\n",
      "loss.item()=-249.34466552734375, ablated_edges=712\n",
      "loss.item()=-249.1333465576172, ablated_edges=724\n",
      "loss.item()=-250.47190856933594, ablated_edges=710\n",
      "loss.item()=-251.70849609375, ablated_edges=699\n",
      "loss.item()=-250.6761474609375, ablated_edges=969\n",
      "Epochs trained:  100\n",
      "Loss: -250.6761\n",
      "Total preserved: 10480.8535\n",
      "Edges ablated:  966\n",
      "Toxic loss:  167.8773956298828\n",
      "OWT loss:  37.82844924926758\n",
      "Penalty:  tensor(82.7988, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' favor'], P(Alicia) = 3.2983709985501264e-08, logit diff = 0.027896881103515625\n",
      "Best Token: [' favor'], P(Alicia) = 3.330185904815153e-08, logit diff = 0.002227783203125\n",
      "\n",
      "\n",
      "loss.item()=-249.00271606445312, ablated_edges=1108\n",
      "loss.item()=-252.4589385986328, ablated_edges=854\n",
      "loss.item()=-253.00875854492188, ablated_edges=754\n",
      "loss.item()=-256.6140441894531, ablated_edges=709\n",
      "loss.item()=-257.6299133300781, ablated_edges=699\n",
      "loss.item()=-257.88604736328125, ablated_edges=688\n",
      "loss.item()=-258.9602966308594, ablated_edges=668\n",
      "loss.item()=-262.31103515625, ablated_edges=665\n",
      "loss.item()=-261.1842956542969, ablated_edges=656\n",
      "loss.item()=-264.22412109375, ablated_edges=644\n",
      "Epochs trained:  110\n",
      "Loss: -264.2241\n",
      "Total preserved: 10868.2871\n",
      "Edges ablated:  644\n",
      "Toxic loss:  167.4963836669922\n",
      "OWT loss:  48.04477310180664\n",
      "Penalty:  tensor(96.7278, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = -22.403167724609375\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = 22.43938446044922\n",
      "\n",
      "\n",
      "loss.item()=-267.8497009277344, ablated_edges=635\n",
      "loss.item()=-263.49554443359375, ablated_edges=640\n",
      "loss.item()=-266.3125305175781, ablated_edges=635\n",
      "loss.item()=-267.6885681152344, ablated_edges=635\n",
      "loss.item()=-269.566650390625, ablated_edges=618\n",
      "loss.item()=-269.4675598144531, ablated_edges=617\n",
      "loss.item()=-271.05718994140625, ablated_edges=614\n",
      "loss.item()=-275.31146240234375, ablated_edges=617\n",
      "loss.item()=-274.37652587890625, ablated_edges=610\n",
      "loss.item()=-273.0650634765625, ablated_edges=600\n",
      "Epochs trained:  120\n",
      "Loss: -273.0651\n",
      "Total preserved: 10920.8916\n",
      "Edges ablated:  600\n",
      "Toxic loss:  164.9482421875\n",
      "OWT loss:  44.05984115600586\n",
      "Penalty:  tensor(108.1168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = -22.695777893066406\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = 23.02884292602539\n",
      "\n",
      "\n",
      "loss.item()=-273.57989501953125, ablated_edges=600\n",
      "loss.item()=-277.4907531738281, ablated_edges=606\n",
      "loss.item()=-278.6205749511719, ablated_edges=597\n",
      "loss.item()=-280.7804260253906, ablated_edges=608\n",
      "loss.item()=-281.01025390625, ablated_edges=591\n",
      "loss.item()=-281.3072509765625, ablated_edges=588\n",
      "loss.item()=-284.74005126953125, ablated_edges=579\n",
      "loss.item()=-286.0392761230469, ablated_edges=576\n",
      "loss.item()=-286.2745666503906, ablated_edges=576\n",
      "loss.item()=-285.458984375, ablated_edges=576\n",
      "Epochs trained:  130\n",
      "Loss: -285.4590\n",
      "Total preserved: 10957.8408\n",
      "Edges ablated:  576\n",
      "Toxic loss:  166.01852416992188\n",
      "OWT loss:  44.51092529296875\n",
      "Penalty:  tensor(119.4405, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = -23.30181121826172\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = 22.463905334472656\n",
      "\n",
      "\n",
      "loss.item()=-287.26885986328125, ablated_edges=571\n",
      "loss.item()=-288.2575378417969, ablated_edges=572\n",
      "loss.item()=-288.12109375, ablated_edges=581\n",
      "loss.item()=-290.1463928222656, ablated_edges=577\n",
      "loss.item()=-289.94403076171875, ablated_edges=570\n",
      "loss.item()=-290.5741882324219, ablated_edges=571\n",
      "loss.item()=-289.53863525390625, ablated_edges=656\n",
      "loss.item()=-293.2548522949219, ablated_edges=559\n",
      "loss.item()=-296.50079345703125, ablated_edges=558\n",
      "loss.item()=-297.27001953125, ablated_edges=561\n",
      "Epochs trained:  140\n",
      "Loss: -297.2700\n",
      "Total preserved: 10970.9238\n",
      "Edges ablated:  561\n",
      "Toxic loss:  166.7160186767578\n",
      "OWT loss:  54.160888671875\n",
      "Penalty:  tensor(130.5540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = -23.33185577392578\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = 20.19546127319336\n",
      "\n",
      "\n",
      "loss.item()=-299.5638427734375, ablated_edges=552\n",
      "loss.item()=-298.87493896484375, ablated_edges=539\n",
      "loss.item()=-299.43896484375, ablated_edges=543\n",
      "loss.item()=-302.31622314453125, ablated_edges=541\n",
      "loss.item()=-302.86834716796875, ablated_edges=540\n",
      "loss.item()=-305.645263671875, ablated_edges=542\n",
      "loss.item()=-305.81243896484375, ablated_edges=549\n",
      "loss.item()=-305.9381408691406, ablated_edges=543\n",
      "loss.item()=-306.7615966796875, ablated_edges=533\n",
      "loss.item()=-308.65283203125, ablated_edges=532\n",
      "Epochs trained:  150\n",
      "Loss: -308.6528\n",
      "Total preserved: 11005.4385\n",
      "Edges ablated:  533\n",
      "Toxic loss:  166.68267822265625\n",
      "OWT loss:  53.3095703125\n",
      "Penalty:  tensor(141.9702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['('], P(Alicia) = 2.43351450102125e-11, logit diff = 2.6481704711914062\n",
      "Best Token: ['('], P(Alicia) = 2.515336723629691e-11, logit diff = 2.6071853637695312\n",
      "\n",
      "\n",
      "loss.item()=-267.8049621582031, ablated_edges=786\n",
      "loss.item()=-276.24774169921875, ablated_edges=666\n",
      "loss.item()=-277.99749755859375, ablated_edges=591\n",
      "loss.item()=-278.92974853515625, ablated_edges=577\n",
      "loss.item()=-280.498046875, ablated_edges=559\n",
      "loss.item()=-283.4293212890625, ablated_edges=536\n",
      "loss.item()=-283.1248779296875, ablated_edges=518\n",
      "loss.item()=-286.324462890625, ablated_edges=505\n",
      "loss.item()=-285.8744812011719, ablated_edges=498\n",
      "loss.item()=-288.8623046875, ablated_edges=489\n",
      "Epochs trained:  160\n",
      "Loss: -288.8623\n",
      "Total preserved: 11021.9463\n",
      "Edges ablated:  489\n",
      "Toxic loss:  135.65725708007812\n",
      "OWT loss:  35.99106979370117\n",
      "Penalty:  tensor(153.2050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['('], P(Alicia) = 0.0, logit diff = -30.95697784423828\n",
      "Best Token: ['('], P(Alicia) = 2.0754631555114866e-41, logit diff = 21.874046325683594\n",
      "\n",
      "\n",
      "loss.item()=-287.960205078125, ablated_edges=488\n",
      "loss.item()=-291.8377685546875, ablated_edges=474\n",
      "loss.item()=-293.1716613769531, ablated_edges=471\n",
      "loss.item()=-294.62591552734375, ablated_edges=461\n",
      "loss.item()=-294.2906494140625, ablated_edges=451\n",
      "loss.item()=-295.89996337890625, ablated_edges=442\n",
      "loss.item()=-296.86572265625, ablated_edges=436\n",
      "loss.item()=-300.03228759765625, ablated_edges=434\n",
      "loss.item()=-299.18743896484375, ablated_edges=438\n",
      "loss.item()=-298.3394775390625, ablated_edges=426\n",
      "Epochs trained:  170\n",
      "Loss: -298.3395\n",
      "Total preserved: 11086.9834\n",
      "Edges ablated:  426\n",
      "Toxic loss:  133.14344787597656\n",
      "OWT loss:  28.8102970123291\n",
      "Penalty:  tensor(165.1960, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['('], P(Alicia) = 0.0, logit diff = -30.219505310058594\n",
      "Best Token: ['('], P(Alicia) = 1.726988253403191e-40, logit diff = 25.156295776367188\n",
      "\n",
      "\n",
      "loss.item()=-300.64276123046875, ablated_edges=427\n",
      "loss.item()=-303.790771484375, ablated_edges=418\n",
      "loss.item()=-305.8619689941406, ablated_edges=417\n",
      "loss.item()=-306.3586730957031, ablated_edges=410\n",
      "loss.item()=-305.4647216796875, ablated_edges=407\n",
      "loss.item()=-307.3000793457031, ablated_edges=405\n",
      "loss.item()=-309.08477783203125, ablated_edges=395\n",
      "loss.item()=-310.99639892578125, ablated_edges=399\n",
      "loss.item()=-312.2938232421875, ablated_edges=393\n",
      "loss.item()=-310.94122314453125, ablated_edges=394\n",
      "Epochs trained:  180\n",
      "Loss: -310.9412\n",
      "Total preserved: 11115.6738\n",
      "Edges ablated:  394\n",
      "Toxic loss:  134.2020263671875\n",
      "OWT loss:  29.947973251342773\n",
      "Penalty:  tensor(176.7392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['('], P(Alicia) = 0.0, logit diff = -29.455902099609375\n",
      "Best Token: ['('], P(Alicia) = 1.6675171465772458e-40, logit diff = 22.697021484375\n",
      "\n",
      "\n",
      "loss.item()=-315.061279296875, ablated_edges=404\n",
      "loss.item()=-314.7949523925781, ablated_edges=399\n",
      "loss.item()=-314.89288330078125, ablated_edges=399\n",
      "loss.item()=-316.4684753417969, ablated_edges=403\n",
      "loss.item()=-318.05426025390625, ablated_edges=397\n",
      "loss.item()=-320.8039245605469, ablated_edges=398\n",
      "loss.item()=-318.887451171875, ablated_edges=395\n",
      "loss.item()=-318.85504150390625, ablated_edges=414\n",
      "loss.item()=-318.92041015625, ablated_edges=394\n",
      "loss.item()=-324.3206787109375, ablated_edges=391\n",
      "Epochs trained:  190\n",
      "Loss: -324.3207\n",
      "Total preserved: 11131.0889\n",
      "Edges ablated:  391\n",
      "Toxic loss:  136.20529174804688\n",
      "OWT loss:  24.23579978942871\n",
      "Penalty:  tensor(188.1154, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['('], P(Alicia) = 0.0, logit diff = -29.245208740234375\n",
      "Best Token: ['('], P(Alicia) = 1.4672743986221581e-39, logit diff = 24.796417236328125\n",
      "\n",
      "\n",
      "loss.item()=-324.2521057128906, ablated_edges=390\n",
      "loss.item()=-325.5219421386719, ablated_edges=384\n",
      "loss.item()=-324.853759765625, ablated_edges=392\n",
      "loss.item()=-328.18939208984375, ablated_edges=399\n",
      "loss.item()=-326.8854064941406, ablated_edges=389\n",
      "loss.item()=-329.72491455078125, ablated_edges=385\n",
      "loss.item()=-328.07904052734375, ablated_edges=386\n",
      "loss.item()=-333.71881103515625, ablated_edges=391\n",
      "loss.item()=-333.7558288574219, ablated_edges=378\n",
      "loss.item()=-333.2920227050781, ablated_edges=391\n",
      "Epochs trained:  200\n",
      "Loss: -333.2920\n",
      "Total preserved: 11140.2666\n",
      "Edges ablated:  388\n",
      "Toxic loss:  133.88125610351562\n",
      "OWT loss:  27.68012046813965\n",
      "Penalty:  tensor(199.4108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' That'], P(Alicia) = 6.601591717725341e-09, logit diff = -3.78460693359375\n",
      "Best Token: [' That'], P(Alicia) = 3.991988517526579e-09, logit diff = -3.6417770385742188\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = -1 * (regularization_strength * penalty + alpha * tox_loss) #+ owt_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            \n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_ioi_sentences = [\"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\", \"While Joshua and Alicia were commuting to the restaurant, Joshua gave a snack to\"]\n",
    "                for test_ioi_sentence in test_ioi_sentences:\n",
    "                    correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "                    other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "                    test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "                    generation = model(test_ioi_tokens)[0][:, -1]\n",
    "                    probs = torch.softmax(generation, dim=-1)\n",
    "                    print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")\n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            print(\"\\n\")\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    # epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    # log_every = int(input('set log frequency'))\n",
    "    # edge_threshold = int(input('set edge threshold'))\n",
    "    epochs_left = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/alternative_necessary_masks_params_dict_lambda={regularization_strength}_{alpha=}_{means_ioi=}_{template_type=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different alternative: sufficient but not necessary\n",
    "Trains with an inverted loss function. This loss function encourages sparsity (as opposed to discouraging) and wants model to ablate everything but the necessary circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_batch_size = 10 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 1\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "\n",
    "template_type = \"single\"\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None, template_type=template_type)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "# with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "#     means = pickle.load(f)[0][0]\n",
    "means_ioi = True\n",
    "if means_ioi:\n",
    "    with open(\"data/gpt2_ioi_abc_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "else:\n",
    "    with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "\n",
    "model = load_demo_gpt2(means=means)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 50 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 1 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 100\n",
    "max_steps_per_epoch = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_182719/2053811722.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106f9dc6b89f401eb9713108d3225c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=9.167099051410332e-06, ablated_edges=184\n",
      "loss.item()=1.6331644019373925e-06, ablated_edges=194\n",
      "loss.item()=2.4199421204684768e-06, ablated_edges=203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=8.344642878910236e-07, ablated_edges=207\n",
      "loss.item()=7.987012509147462e-07, ablated_edges=217\n",
      "loss.item()=6.914127652635216e-07, ablated_edges=221\n",
      "loss.item()=4.768367034557741e-07, ablated_edges=223\n",
      "loss.item()=1.4305111051271524e-07, ablated_edges=227\n",
      "loss.item()=2.026556842338323e-07, ablated_edges=229\n",
      "loss.item()=7.867785143389483e-07, ablated_edges=232\n",
      "Epochs trained:  10\n",
      "Loss: 0.0000\n",
      "Total preserved: 9748.0654\n",
      "Edges ablated:  232\n",
      "Toxic loss:  7.867785143389483e-07\n",
      "OWT loss:  8.05907917022705\n",
      "Penalty:  0\n",
      "Best Token: [' Alicia'], P(Alicia) = 1.0, logit diff = 32.228858947753906\n",
      "Best Token: [' Joshua'], P(Alicia) = 2.5583093421488456e-09, logit diff = -19.78390884399414\n",
      "\n",
      "\n",
      "loss.item()=9.536741885085576e-08, ablated_edges=232\n",
      "loss.item()=1.168244807558949e-06, ablated_edges=234\n",
      "loss.item()=4.768370942542788e-08, ablated_edges=233\n",
      "loss.item()=2.622602437440946e-07, ablated_edges=231\n",
      "loss.item()=9.536741885085576e-08, ablated_edges=234\n",
      "loss.item()=2.1457667287450022e-07, ablated_edges=232\n",
      "loss.item()=3.695483883348061e-07, ablated_edges=233\n",
      "loss.item()=1.4305103945844166e-07, ablated_edges=233\n",
      "loss.item()=8.344649415903405e-08, ablated_edges=235\n",
      "loss.item()=1.4305105366929638e-07, ablated_edges=235\n",
      "Epochs trained:  20\n",
      "Loss: 0.0000\n",
      "Total preserved: 9738.1279\n",
      "Edges ablated:  235\n",
      "Toxic loss:  1.4305105366929638e-07\n",
      "OWT loss:  7.353018283843994\n",
      "Penalty:  0\n",
      "Best Token: [' Alicia'], P(Alicia) = 1.0, logit diff = 33.292701721191406\n",
      "Best Token: [' Joshua'], P(Alicia) = 4.618624926955306e-10, logit diff = -21.495746612548828\n",
      "\n",
      "\n",
      "loss.item()=1.4305105366929638e-07, ablated_edges=238\n",
      "loss.item()=0.014667890965938568, ablated_edges=11487\n",
      "loss.item()=0.1778566837310791, ablated_edges=10658\n",
      "loss.item()=0.1314440816640854, ablated_edges=11135\n",
      "loss.item()=0.10547701269388199, ablated_edges=11327\n",
      "loss.item()=0.09772154688835144, ablated_edges=11428\n",
      "loss.item()=0.06940310448408127, ablated_edges=11497\n",
      "loss.item()=0.05577990785241127, ablated_edges=11535\n",
      "loss.item()=0.045821771025657654, ablated_edges=11562\n",
      "loss.item()=0.3001222610473633, ablated_edges=11276\n",
      "Epochs trained:  30\n",
      "Loss: 0.3001\n",
      "Total preserved: 332.5201\n",
      "Edges ablated:  11276\n",
      "Toxic loss:  0.0008541909046471119\n",
      "OWT loss:  15.286337852478027\n",
      "Penalty:  tensor(0.2993, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 1.0, logit diff = 28.28252410888672\n",
      "Best Token: [' Joshua'], P(Alicia) = 4.357243783311001e-15, logit diff = -33.0666618347168\n",
      "\n",
      "\n",
      "loss.item()=0.1571558564901352, ablated_edges=11450\n",
      "loss.item()=0.11273939162492752, ablated_edges=11510\n",
      "loss.item()=0.08641751110553741, ablated_edges=11541\n",
      "loss.item()=0.06979043036699295, ablated_edges=11555\n",
      "loss.item()=0.059457242488861084, ablated_edges=11572\n",
      "loss.item()=0.3785090446472168, ablated_edges=11353\n",
      "loss.item()=0.21339267492294312, ablated_edges=11479\n",
      "loss.item()=0.359061062335968, ablated_edges=11428\n",
      "loss.item()=0.1981278955936432, ablated_edges=11505\n",
      "loss.item()=0.1333044320344925, ablated_edges=11544\n",
      "Epochs trained:  40\n",
      "Loss: 0.1333\n",
      "Total preserved: 69.6823\n",
      "Edges ablated:  11544\n",
      "Toxic loss:  0.0009081339230760932\n",
      "OWT loss:  9.65410041809082\n",
      "Penalty:  tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9988259673118591, logit diff = 9.591110229492188\n",
      "Best Token: [' Joshua'], P(Alicia) = 1.3716897839799458e-08, logit diff = -18.097923278808594\n",
      "\n",
      "\n",
      "loss.item()=0.10547082871198654, ablated_edges=11561\n",
      "loss.item()=0.09941479563713074, ablated_edges=11568\n",
      "loss.item()=0.07204999029636383, ablated_edges=11583\n",
      "loss.item()=0.061399057507514954, ablated_edges=11590\n",
      "loss.item()=0.1374201774597168, ablated_edges=11561\n",
      "loss.item()=0.10354466736316681, ablated_edges=11571\n",
      "loss.item()=0.060409825295209885, ablated_edges=11594\n",
      "loss.item()=0.05798237770795822, ablated_edges=11600\n",
      "loss.item()=1.0619724988937378, ablated_edges=11205\n",
      "loss.item()=0.57224041223526, ablated_edges=11405\n",
      "Epochs trained:  50\n",
      "Loss: 0.5722\n",
      "Total preserved: 196.6375\n",
      "Edges ablated:  11405\n",
      "Toxic loss:  0.00199166894890368\n",
      "OWT loss:  17.644691467285156\n",
      "Penalty:  tensor(0.5702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Shutterstock'], P(Alicia) = 0.01770826429128647, logit diff = 4.704585075378418\n",
      "Best Token: [' Shutterstock'], P(Alicia) = 7.141668174881488e-05, logit diff = -6.675329208374023\n",
      "\n",
      "\n",
      "loss.item()=0.5766080021858215, ablated_edges=11389\n",
      "loss.item()=0.4059791564941406, ablated_edges=11477\n",
      "loss.item()=0.2764977514743805, ablated_edges=11531\n",
      "loss.item()=0.20771753787994385, ablated_edges=11553\n",
      "loss.item()=0.1579093337059021, ablated_edges=11573\n",
      "loss.item()=1.4506832361221313, ablated_edges=11189\n",
      "loss.item()=0.6786584258079529, ablated_edges=11414\n",
      "loss.item()=0.42382821440696716, ablated_edges=11511\n",
      "loss.item()=0.29819950461387634, ablated_edges=11531\n",
      "loss.item()=0.2619412839412689, ablated_edges=11546\n",
      "Epochs trained:  60\n",
      "Loss: 0.2619\n",
      "Total preserved: 67.0198\n",
      "Edges ablated:  11546\n",
      "Toxic loss:  0.0005641488241963089\n",
      "OWT loss:  10.660840034484863\n",
      "Penalty:  tensor(0.2614, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999265670776367, logit diff = 15.515678405761719\n",
      "Best Token: [' Joshua'], P(Alicia) = 9.421874088214044e-14, logit diff = -29.992855072021484\n",
      "\n",
      "\n",
      "loss.item()=0.21723894774913788, ablated_edges=11559\n",
      "loss.item()=0.8177565932273865, ablated_edges=11432\n",
      "loss.item()=0.4719463884830475, ablated_edges=11497\n",
      "loss.item()=0.33474108576774597, ablated_edges=11534\n",
      "loss.item()=0.2579044699668884, ablated_edges=11557\n",
      "loss.item()=0.2074841856956482, ablated_edges=11568\n",
      "loss.item()=0.2802583575248718, ablated_edges=11556\n",
      "loss.item()=0.19239918887615204, ablated_edges=11575\n",
      "loss.item()=0.15069130063056946, ablated_edges=11581\n",
      "loss.item()=0.13612398505210876, ablated_edges=11589\n",
      "Epochs trained:  70\n",
      "Loss: 0.1361\n",
      "Total preserved: 27.3323\n",
      "Edges ablated:  11589\n",
      "Toxic loss:  0.0021955007687211037\n",
      "OWT loss:  10.666979789733887\n",
      "Penalty:  tensor(0.1339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999953508377075, logit diff = 18.746220588684082\n",
      "Best Token: [' Joshua'], P(Alicia) = 9.125714073299207e-12, logit diff = -25.419885635375977\n",
      "\n",
      "\n",
      "loss.item()=0.12207307666540146, ablated_edges=11589\n",
      "loss.item()=0.13287554681301117, ablated_edges=11589\n",
      "loss.item()=0.10959254205226898, ablated_edges=11592\n",
      "loss.item()=0.4197002947330475, ablated_edges=11532\n",
      "loss.item()=0.866500735282898, ablated_edges=11462\n",
      "loss.item()=1.2773035764694214, ablated_edges=11378\n",
      "loss.item()=0.581454873085022, ablated_edges=11512\n",
      "loss.item()=0.3663841784000397, ablated_edges=11553\n",
      "loss.item()=0.261883407831192, ablated_edges=11567\n",
      "loss.item()=0.20683181285858154, ablated_edges=11581\n",
      "Epochs trained:  80\n",
      "Loss: 0.2068\n",
      "Total preserved: 34.5768\n",
      "Edges ablated:  11581\n",
      "Toxic loss:  0.002828944008797407\n",
      "OWT loss:  12.878705024719238\n",
      "Penalty:  tensor(0.2040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9715778231620789, logit diff = 3.5734024047851562\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.002185334451496601, logit diff = -6.119224548339844\n",
      "\n",
      "\n",
      "loss.item()=0.6567268967628479, ablated_edges=11520\n",
      "loss.item()=0.29968973994255066, ablated_edges=11570\n",
      "loss.item()=0.2003912478685379, ablated_edges=11586\n",
      "loss.item()=0.1552426666021347, ablated_edges=11591\n",
      "loss.item()=0.1232539638876915, ablated_edges=11597\n",
      "loss.item()=0.09739845246076584, ablated_edges=11602\n",
      "loss.item()=1.6040931940078735, ablated_edges=11359\n",
      "loss.item()=0.8487077355384827, ablated_edges=11483\n",
      "loss.item()=0.5763853192329407, ablated_edges=11525\n",
      "loss.item()=0.44867393374443054, ablated_edges=11544\n",
      "Epochs trained:  90\n",
      "Loss: 0.4487\n",
      "Total preserved: 64.2983\n",
      "Edges ablated:  11544\n",
      "Toxic loss:  0.005015576258301735\n",
      "OWT loss:  16.237842559814453\n",
      "Penalty:  tensor(0.4437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.4345718324184418, logit diff = 13.01995849609375\n",
      "Best Token: [' Joshua'], P(Alicia) = 6.676889596768376e-10, logit diff = -21.110565185546875\n",
      "\n",
      "\n",
      "loss.item()=0.7541647553443909, ablated_edges=11559\n",
      "loss.item()=0.3653052747249603, ablated_edges=11564\n",
      "loss.item()=0.27735239267349243, ablated_edges=11572\n",
      "loss.item()=0.25905415415763855, ablated_edges=11579\n",
      "loss.item()=0.20435121655464172, ablated_edges=11584\n",
      "loss.item()=2.504037618637085, ablated_edges=11295\n",
      "loss.item()=0.7173457145690918, ablated_edges=11526\n",
      "loss.item()=0.48617419600486755, ablated_edges=11552\n",
      "loss.item()=0.3827894628047943, ablated_edges=11566\n",
      "loss.item()=0.3160833418369293, ablated_edges=11572\n",
      "Epochs trained:  100\n",
      "Loss: 0.3161\n",
      "Total preserved: 39.4785\n",
      "Edges ablated:  11572\n",
      "Toxic loss:  0.0042031570337712765\n",
      "OWT loss:  14.713788032531738\n",
      "Penalty:  tensor(0.3119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Coulter'], P(Alicia) = 0.005042397417128086, logit diff = 6.979855060577393\n",
      "Best Token: [' Joshua'], P(Alicia) = 2.139135313328211e-09, logit diff = -18.92170476913452\n",
      "\n",
      "\n",
      "loss.item()=1.8887581825256348, ablated_edges=11367\n",
      "loss.item()=1.3691093921661377, ablated_edges=11441\n",
      "loss.item()=1.118674874305725, ablated_edges=11476\n",
      "loss.item()=1.5037198066711426, ablated_edges=11441\n",
      "loss.item()=1.137762188911438, ablated_edges=11469\n",
      "loss.item()=1.0175187587738037, ablated_edges=11488\n",
      "loss.item()=0.9151414632797241, ablated_edges=11496\n",
      "loss.item()=0.8432637453079224, ablated_edges=11505\n",
      "loss.item()=0.7959913611412048, ablated_edges=11515\n",
      "loss.item()=0.7475832104682922, ablated_edges=11522\n",
      "Epochs trained:  110\n",
      "Loss: 0.7476\n",
      "Total preserved: 81.2824\n",
      "Edges ablated:  11522\n",
      "Toxic loss:  0.02417025715112686\n",
      "OWT loss:  12.55500316619873\n",
      "Penalty:  tensor(0.7234, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999996423721313, logit diff = 36.31839370727539\n",
      "Best Token: [' Joshua'], P(Alicia) = 5.273491062232627e-13, logit diff = -28.215261459350586\n",
      "\n",
      "\n",
      "loss.item()=0.6872848272323608, ablated_edges=11530\n",
      "loss.item()=0.7005488872528076, ablated_edges=11532\n",
      "loss.item()=0.6612831950187683, ablated_edges=11537\n",
      "loss.item()=0.6225560903549194, ablated_edges=11542\n",
      "loss.item()=0.7153350710868835, ablated_edges=11541\n",
      "loss.item()=0.5192075371742249, ablated_edges=11549\n",
      "loss.item()=0.49077969789505005, ablated_edges=11557\n",
      "loss.item()=1.3690394163131714, ablated_edges=11473\n",
      "loss.item()=0.8320642113685608, ablated_edges=11528\n",
      "loss.item()=0.669393002986908, ablated_edges=11546\n",
      "Epochs trained:  120\n",
      "Loss: 0.6694\n",
      "Total preserved: 66.9359\n",
      "Edges ablated:  11546\n",
      "Toxic loss:  0.0067273289896547794\n",
      "OWT loss:  11.349231719970703\n",
      "Penalty:  tensor(0.6627, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999994039535522, logit diff = 32.81134605407715\n",
      "Best Token: [' Joshua'], P(Alicia) = 9.777241170817134e-15, logit diff = -32.25701904296875\n",
      "\n",
      "\n",
      "loss.item()=0.6442875266075134, ablated_edges=11558\n",
      "loss.item()=0.5413391590118408, ablated_edges=11568\n",
      "loss.item()=0.5262728929519653, ablated_edges=11575\n",
      "loss.item()=0.5346972346305847, ablated_edges=11576\n",
      "loss.item()=0.44438251852989197, ablated_edges=11577\n",
      "loss.item()=0.6697142124176025, ablated_edges=11575\n",
      "loss.item()=0.4155607223510742, ablated_edges=11578\n",
      "loss.item()=0.36225706338882446, ablated_edges=11582\n",
      "loss.item()=0.51128751039505, ablated_edges=11573\n",
      "loss.item()=0.4971083104610443, ablated_edges=11573\n",
      "Epochs trained:  130\n",
      "Loss: 0.4971\n",
      "Total preserved: 45.0635\n",
      "Edges ablated:  11573\n",
      "Toxic loss:  0.00591620709747076\n",
      "OWT loss:  14.179351806640625\n",
      "Penalty:  tensor(0.4912, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.999992847442627, logit diff = 36.11432647705078\n",
      "Best Token: [' Joshua'], P(Alicia) = 4.802229471656614e-16, logit diff = -35.271289348602295\n",
      "\n",
      "\n",
      "loss.item()=1.054358959197998, ablated_edges=11519\n",
      "loss.item()=0.634826123714447, ablated_edges=11566\n",
      "loss.item()=0.7730783820152283, ablated_edges=11554\n",
      "loss.item()=0.4167122542858124, ablated_edges=11581\n",
      "loss.item()=0.4047248065471649, ablated_edges=11587\n",
      "loss.item()=0.3149856626987457, ablated_edges=11592\n",
      "loss.item()=0.2690805494785309, ablated_edges=11595\n",
      "loss.item()=0.38172709941864014, ablated_edges=11586\n",
      "loss.item()=0.5589001178741455, ablated_edges=11580\n",
      "loss.item()=0.2881737947463989, ablated_edges=11591\n",
      "Epochs trained:  140\n",
      "Loss: 0.2882\n",
      "Total preserved: 23.8796\n",
      "Edges ablated:  11591\n",
      "Toxic loss:  0.004006611183285713\n",
      "OWT loss:  12.223884582519531\n",
      "Penalty:  tensor(0.2842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999984502792358, logit diff = 40.22830677032471\n",
      "Best Token: [' Joshua'], P(Alicia) = 5.65970104119464e-16, logit diff = -35.10727310180664\n",
      "\n",
      "\n",
      "loss.item()=0.3323417603969574, ablated_edges=11590\n",
      "loss.item()=0.2072295993566513, ablated_edges=11597\n",
      "loss.item()=1.2676976919174194, ablated_edges=11517\n",
      "loss.item()=0.5372276306152344, ablated_edges=11569\n",
      "loss.item()=0.48113569617271423, ablated_edges=11576\n",
      "loss.item()=0.2548914849758148, ablated_edges=11590\n",
      "loss.item()=0.32723933458328247, ablated_edges=11595\n",
      "loss.item()=0.2011023759841919, ablated_edges=11596\n",
      "loss.item()=0.3150610029697418, ablated_edges=11597\n",
      "loss.item()=0.16451282799243927, ablated_edges=11600\n",
      "Epochs trained:  150\n",
      "Loss: 0.1645\n",
      "Total preserved: 11.9738\n",
      "Edges ablated:  11600\n",
      "Toxic loss:  0.010050993412733078\n",
      "OWT loss:  8.760695457458496\n",
      "Penalty:  tensor(0.1545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [','], P(Alicia) = 1.0297452718077693e-05, logit diff = -0.5422906875610352\n",
      "Best Token: [','], P(Alicia) = 1.0297452718077693e-05, logit diff = -0.5422906875610352\n",
      "\n",
      "\n",
      "loss.item()=4.293149948120117, ablated_edges=11258\n",
      "loss.item()=3.157914400100708, ablated_edges=11348\n",
      "loss.item()=2.5484414100646973, ablated_edges=11408\n",
      "loss.item()=2.150616407394409, ablated_edges=11441\n",
      "loss.item()=3.3195455074310303, ablated_edges=11369\n",
      "loss.item()=2.1042182445526123, ablated_edges=11449\n",
      "loss.item()=1.86884605884552, ablated_edges=11467\n",
      "loss.item()=1.6010366678237915, ablated_edges=11489\n",
      "loss.item()=1.4429162740707397, ablated_edges=11505\n",
      "loss.item()=1.3224692344665527, ablated_edges=11513\n",
      "Epochs trained:  160\n",
      "Loss: 1.3225\n",
      "Total preserved: 92.5373\n",
      "Edges ablated:  11513\n",
      "Toxic loss:  0.03620161861181259\n",
      "OWT loss:  8.391968727111816\n",
      "Penalty:  tensor(1.2863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9998762607574463, logit diff = 18.479568481445312\n",
      "Best Token: [' Joshua'], P(Alicia) = 5.125106667946966e-07, logit diff = -14.47601318359375\n",
      "\n",
      "\n",
      "loss.item()=1.1654378175735474, ablated_edges=11524\n",
      "loss.item()=1.6253461837768555, ablated_edges=11500\n",
      "loss.item()=1.1317625045776367, ablated_edges=11532\n",
      "loss.item()=1.0333752632141113, ablated_edges=11547\n",
      "loss.item()=1.3969542980194092, ablated_edges=11523\n",
      "loss.item()=1.0491220951080322, ablated_edges=11543\n",
      "loss.item()=0.8936430811882019, ablated_edges=11553\n",
      "loss.item()=0.7733359336853027, ablated_edges=11564\n",
      "loss.item()=1.010797381401062, ablated_edges=11555\n",
      "loss.item()=0.676470160484314, ablated_edges=11568\n",
      "Epochs trained:  170\n",
      "Loss: 0.6765\n",
      "Total preserved: 45.1288\n",
      "Edges ablated:  11568\n",
      "Toxic loss:  0.004050509072840214\n",
      "OWT loss:  8.4425048828125\n",
      "Penalty:  tensor(0.6724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9985491633415222, logit diff = 15.934837341308594\n",
      "Best Token: [' Joshua'], P(Alicia) = 2.339058502442981e-09, logit diff = -19.871681213378906\n",
      "\n",
      "\n",
      "loss.item()=0.5859878063201904, ablated_edges=11579\n",
      "loss.item()=0.5674851536750793, ablated_edges=11578\n",
      "loss.item()=0.48861873149871826, ablated_edges=11585\n",
      "loss.item()=1.4407947063446045, ablated_edges=11523\n",
      "loss.item()=0.8962401151657104, ablated_edges=11560\n",
      "loss.item()=0.7114479541778564, ablated_edges=11575\n",
      "loss.item()=0.5650135278701782, ablated_edges=11581\n",
      "loss.item()=0.678910493850708, ablated_edges=11581\n",
      "loss.item()=0.47833988070487976, ablated_edges=11590\n",
      "loss.item()=0.401102751493454, ablated_edges=11592\n",
      "Epochs trained:  180\n",
      "Loss: 0.4011\n",
      "Total preserved: 24.7377\n",
      "Edges ablated:  11592\n",
      "Toxic loss:  0.007772638462483883\n",
      "OWT loss:  9.168510437011719\n",
      "Penalty:  tensor(0.3933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9844135046005249, logit diff = 21.67792510986328\n",
      "Best Token: [' Joshua'], P(Alicia) = 2.0732087158137347e-09, logit diff = -19.90911102294922\n",
      "\n",
      "\n",
      "loss.item()=0.8369283676147461, ablated_edges=11571\n",
      "loss.item()=0.4298723340034485, ablated_edges=11595\n",
      "loss.item()=0.2811450660228729, ablated_edges=11596\n",
      "loss.item()=0.2781088650226593, ablated_edges=11598\n",
      "loss.item()=0.2626991868019104, ablated_edges=11599\n",
      "loss.item()=0.22225230932235718, ablated_edges=11599\n",
      "loss.item()=2.862692356109619, ablated_edges=11443\n",
      "loss.item()=1.0120716094970703, ablated_edges=11558\n",
      "loss.item()=0.6267336010932922, ablated_edges=11578\n",
      "loss.item()=0.5028061270713806, ablated_edges=11584\n",
      "Epochs trained:  190\n",
      "Loss: 0.5028\n",
      "Total preserved: 29.5551\n",
      "Edges ablated:  11584\n",
      "Toxic loss:  0.0033241237979382277\n",
      "OWT loss:  11.097752571105957\n",
      "Penalty:  tensor(0.4995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9988003969192505, logit diff = 24.188270568847656\n",
      "Best Token: [' Joshua'], P(Alicia) = 2.1956199011685662e-11, logit diff = -24.530540466308594\n",
      "\n",
      "\n",
      "loss.item()=0.41425925493240356, ablated_edges=11592\n",
      "loss.item()=0.3434131443500519, ablated_edges=11596\n",
      "loss.item()=0.3830099105834961, ablated_edges=11598\n",
      "loss.item()=0.35915398597717285, ablated_edges=11599\n",
      "loss.item()=0.3273136615753174, ablated_edges=11599\n",
      "loss.item()=0.26574042439460754, ablated_edges=11602\n",
      "loss.item()=0.6657810211181641, ablated_edges=11588\n",
      "loss.item()=0.42785876989364624, ablated_edges=11594\n",
      "loss.item()=0.2552317976951599, ablated_edges=11599\n",
      "loss.item()=0.19967585802078247, ablated_edges=11603\n",
      "Epochs trained:  200\n",
      "Loss: 0.1997\n",
      "Total preserved: 10.7730\n",
      "Edges ablated:  11603\n",
      "Toxic loss:  0.006838577333837748\n",
      "OWT loss:  9.019335746765137\n",
      "Penalty:  tensor(0.1928, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Scand'], P(Alicia) = 1.7467348401023486e-10, logit diff = 7.568992614746094\n",
      "Best Token: [' Scand'], P(Alicia) = 1.7467348401023486e-10, logit diff = 7.568992614746094\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = (regularization_strength * penalty + alpha * tox_loss) #+ owt_loss\n",
    "            # loss = alpha * tox_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            \n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_ioi_sentences = [\"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\", \"While Joshua and Alicia were commuting to the restaurant, Joshua gave a snack to\"]\n",
    "                for test_ioi_sentence in test_ioi_sentences:\n",
    "                    correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "                    other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "                    test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "                    generation = model(test_ioi_tokens)[0][:, -1]\n",
    "                    probs = torch.softmax(generation, dim=-1)\n",
    "                    print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")\n",
    "            \n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            print(\"\\n\")\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    # epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    # log_every = int(input('set log frequency'))\n",
    "    # edge_threshold = int(input('set edge threshold'))\n",
    "    epochs_left = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/alternative_sufficient_masks_params_dict_lambda={regularization_strength}_{alpha=}_{means_ioi=}_{template_type=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
       " tensor([1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 1., 0.]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_mask_params[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Token: [' Alicia'], P(Alicia) = 0.9056878089904785, logit diff = 2.4303483963012695\n",
      "Best Token: [' Joshua'], P(Alicia) = 2.5975340989248252e-08, logit diff = -17.38216209411621\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_ioi_sentences = [\"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\", \"While Joshua and Alicia were commuting to the restaurant, Joshua gave a snack to\"]\n",
    "    for test_ioi_sentence in test_ioi_sentences:\n",
    "        correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "        other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "        test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "        generation = model(test_ioi_tokens)[0][:, -1]\n",
    "        probs = torch.softmax(generation, dim=-1)\n",
    "        print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train mask over known circuit\n",
    "Train mask over the circuit from the paper, as given by a run of ACDC++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mask_utils import get_nodes_and_edges\n",
    "# with open(\"models/acdcpp_mask_params.pkl\", \"rb\") as f:\n",
    "#     acdc_mask_params = pickle.load(f)\n",
    "\n",
    "# _, _, acdc_Edges, acdc_mask_dict = get_nodes_and_edges(mask_params=acdc_mask_params)\n",
    "# acdc_mask_dict\n",
    "\n",
    "from mask_utils import get_nodes_and_edges\n",
    "with open(\"models/circuit_covering_mask_params.pkl\", \"rb\") as f:\n",
    "    circuit_covering_mask_params = pickle.load(f)\n",
    "\n",
    "_, _, circuit_covering_edges, circuit_covering_mask_dict = get_nodes_and_edges(mask_params=circuit_covering_mask_params)\n",
    "# circuit_covering_mask_dict # mostly 1s, 1s are frozen edges\n",
    "# Circuit break only over the edges that are currently 0s in the circuit covering mask, ablate as few of them as possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_batch_size = 10 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 10\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "template_type = \"single\"\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None, template_type=template_type)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "    means = pickle.load(f)[0]\n",
    "\n",
    "model = load_demo_gpt2(means=means, mask_dict_superset=circuit_covering_mask_dict)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 50 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 1 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 0\n",
    "max_steps_per_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_182719/243202122.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fde68ad80654726bc9985ee9c8b061f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=-13.236642837524414, ablated_edges=167\n",
      "loss.item()=-12.427201271057129, ablated_edges=178\n",
      "loss.item()=-14.008918762207031, ablated_edges=180\n",
      "loss.item()=-13.530853271484375, ablated_edges=177\n",
      "loss.item()=-13.251840591430664, ablated_edges=176\n",
      "loss.item()=-13.437471389770508, ablated_edges=177\n",
      "loss.item()=-13.83442211151123, ablated_edges=176\n",
      "loss.item()=-12.967565536499023, ablated_edges=175\n",
      "loss.item()=-12.80200481414795, ablated_edges=176\n",
      "loss.item()=-13.274145126342773, ablated_edges=176\n",
      "Epochs trained:  10\n",
      "Loss: -13.2741\n",
      "Total preserved: 11433.7441\n",
      "Edges ablated:  176\n",
      "Toxic loss:  13.274145126342773\n",
      "OWT loss:  4.235899925231934\n",
      "Penalty:  0\n",
      "Best Token: [' the'], P(Alicia) = 4.3425427520560334e-07, logit diff = -4.821807861328125\n",
      "Best Token: [' the'], P(Alicia) = 4.7285746518355154e-07, logit diff = -3.5168304443359375\n",
      "\n",
      "\n",
      "loss.item()=-12.855962753295898, ablated_edges=173\n",
      "loss.item()=-13.841504096984863, ablated_edges=175\n",
      "loss.item()=-13.255230903625488, ablated_edges=175\n",
      "loss.item()=-13.459057807922363, ablated_edges=176\n",
      "loss.item()=-13.625630378723145, ablated_edges=174\n",
      "loss.item()=-13.107698440551758, ablated_edges=173\n",
      "loss.item()=-13.920872688293457, ablated_edges=175\n",
      "loss.item()=-13.44122314453125, ablated_edges=174\n",
      "loss.item()=-13.968202590942383, ablated_edges=175\n",
      "loss.item()=-13.67858600616455, ablated_edges=176\n",
      "Epochs trained:  20\n",
      "Loss: -13.6786\n",
      "Total preserved: 11434.0879\n",
      "Edges ablated:  176\n",
      "Toxic loss:  13.67858600616455\n",
      "OWT loss:  3.9243979454040527\n",
      "Penalty:  0\n",
      "Best Token: [' the'], P(Alicia) = 4.247553704317397e-07, logit diff = -4.8196868896484375\n",
      "Best Token: [' the'], P(Alicia) = 4.678472578234505e-07, logit diff = -3.5470352172851562\n",
      "\n",
      "\n",
      "loss.item()=-13.848947525024414, ablated_edges=176\n",
      "loss.item()=-13.885479927062988, ablated_edges=176\n",
      "loss.item()=-14.70824146270752, ablated_edges=176\n",
      "loss.item()=-16.755172729492188, ablated_edges=168\n",
      "loss.item()=-18.242862701416016, ablated_edges=169\n",
      "loss.item()=-17.505104064941406, ablated_edges=167\n",
      "loss.item()=-19.660749435424805, ablated_edges=166\n",
      "loss.item()=-21.349781036376953, ablated_edges=164\n",
      "loss.item()=-23.015501022338867, ablated_edges=165\n",
      "loss.item()=-22.957294464111328, ablated_edges=162\n",
      "Epochs trained:  30\n",
      "Loss: -22.9573\n",
      "Total preserved: 11448.0469\n",
      "Edges ablated:  162\n",
      "Toxic loss:  12.654051780700684\n",
      "OWT loss:  3.933175802230835\n",
      "Penalty:  tensor(10.3032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 4.2586847825987206e-07, logit diff = -4.792457580566406\n",
      "Best Token: [' the'], P(Alicia) = 4.584537407481548e-07, logit diff = -3.5249099731445312\n",
      "\n",
      "\n",
      "loss.item()=-24.513938903808594, ablated_edges=161\n",
      "loss.item()=-26.008956909179688, ablated_edges=161\n",
      "loss.item()=-26.078937530517578, ablated_edges=160\n",
      "loss.item()=-28.463714599609375, ablated_edges=157\n",
      "loss.item()=-28.612741470336914, ablated_edges=154\n",
      "loss.item()=-30.49671745300293, ablated_edges=152\n",
      "loss.item()=-31.068164825439453, ablated_edges=148\n",
      "loss.item()=-32.567386627197266, ablated_edges=146\n",
      "loss.item()=-34.340301513671875, ablated_edges=147\n",
      "loss.item()=-36.1871223449707, ablated_edges=145\n",
      "Epochs trained:  40\n",
      "Loss: -36.1871\n",
      "Total preserved: 11465.8193\n",
      "Edges ablated:  145\n",
      "Toxic loss:  14.402066230773926\n",
      "OWT loss:  4.542913913726807\n",
      "Penalty:  tensor(21.7851, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 4.058013303165353e-07, logit diff = -4.768959045410156\n",
      "Best Token: [' the'], P(Alicia) = 4.672563989061018e-07, logit diff = -3.5146255493164062\n",
      "\n",
      "\n",
      "loss.item()=-36.66875457763672, ablated_edges=143\n",
      "loss.item()=-36.95630645751953, ablated_edges=143\n",
      "loss.item()=-38.85877990722656, ablated_edges=140\n",
      "loss.item()=-40.404666900634766, ablated_edges=137\n",
      "loss.item()=-40.60211181640625, ablated_edges=135\n",
      "loss.item()=-42.7411994934082, ablated_edges=133\n",
      "loss.item()=-43.222164154052734, ablated_edges=133\n",
      "loss.item()=-44.44309616088867, ablated_edges=133\n",
      "loss.item()=-45.070892333984375, ablated_edges=132\n",
      "loss.item()=-46.092071533203125, ablated_edges=132\n",
      "Epochs trained:  50\n",
      "Loss: -46.0921\n",
      "Total preserved: 11478.0391\n",
      "Edges ablated:  132\n",
      "Toxic loss:  12.805761337280273\n",
      "OWT loss:  5.1377763748168945\n",
      "Penalty:  tensor(33.2863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 5.529298618967005e-07, logit diff = -4.6344451904296875\n",
      "Best Token: [' the'], P(Alicia) = 6.499798246295541e-07, logit diff = -3.1415328979492188\n",
      "\n",
      "\n",
      "loss.item()=-47.005550384521484, ablated_edges=133\n",
      "loss.item()=-48.72059631347656, ablated_edges=127\n",
      "loss.item()=-49.35026550292969, ablated_edges=128\n",
      "loss.item()=-50.656700134277344, ablated_edges=129\n",
      "loss.item()=-52.142250061035156, ablated_edges=129\n",
      "loss.item()=-53.5252685546875, ablated_edges=128\n",
      "loss.item()=-54.61193084716797, ablated_edges=127\n",
      "loss.item()=-55.9540901184082, ablated_edges=127\n",
      "loss.item()=-56.10894012451172, ablated_edges=127\n",
      "loss.item()=-57.21501541137695, ablated_edges=126\n",
      "Epochs trained:  60\n",
      "Loss: -57.2150\n",
      "Total preserved: 11484.3008\n",
      "Edges ablated:  126\n",
      "Toxic loss:  12.426238059997559\n",
      "OWT loss:  3.8067009449005127\n",
      "Penalty:  tensor(44.7888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 4.6287502186714846e-07, logit diff = -4.746978759765625\n",
      "Best Token: [' the'], P(Alicia) = 4.865871119363874e-07, logit diff = -3.4476776123046875\n",
      "\n",
      "\n",
      "loss.item()=-58.68111801147461, ablated_edges=125\n",
      "loss.item()=-60.176109313964844, ablated_edges=125\n",
      "loss.item()=-61.66561508178711, ablated_edges=122\n",
      "loss.item()=-62.270687103271484, ablated_edges=124\n",
      "loss.item()=-63.16019821166992, ablated_edges=124\n",
      "loss.item()=-64.49260711669922, ablated_edges=121\n",
      "loss.item()=-66.78115844726562, ablated_edges=121\n",
      "loss.item()=-66.61454010009766, ablated_edges=119\n",
      "loss.item()=-68.54020690917969, ablated_edges=117\n",
      "loss.item()=-69.4737777709961, ablated_edges=116\n",
      "Epochs trained:  70\n",
      "Loss: -69.4738\n",
      "Total preserved: 11493.8115\n",
      "Edges ablated:  116\n",
      "Toxic loss:  13.154101371765137\n",
      "OWT loss:  3.884187936782837\n",
      "Penalty:  tensor(56.3197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 5.036654329160228e-07, logit diff = -4.5204620361328125\n",
      "Best Token: [' the'], P(Alicia) = 5.347490059648408e-07, logit diff = -3.3555145263671875\n",
      "\n",
      "\n",
      "loss.item()=-71.57553100585938, ablated_edges=116\n",
      "loss.item()=-71.97938537597656, ablated_edges=116\n",
      "loss.item()=-72.98031616210938, ablated_edges=116\n",
      "loss.item()=-74.19265747070312, ablated_edges=116\n",
      "loss.item()=-74.83712005615234, ablated_edges=115\n",
      "loss.item()=-76.82536315917969, ablated_edges=114\n",
      "loss.item()=-77.88038635253906, ablated_edges=116\n",
      "loss.item()=-78.42237854003906, ablated_edges=116\n",
      "loss.item()=-79.41169738769531, ablated_edges=113\n",
      "loss.item()=-81.1395034790039, ablated_edges=111\n",
      "Epochs trained:  80\n",
      "Loss: -81.1395\n",
      "Total preserved: 11497.8848\n",
      "Edges ablated:  111\n",
      "Toxic loss:  13.301984786987305\n",
      "OWT loss:  4.004148960113525\n",
      "Penalty:  tensor(67.8375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 5.105047762299364e-07, logit diff = -4.588081359863281\n",
      "Best Token: [' the'], P(Alicia) = 5.772834015260742e-07, logit diff = -3.2842025756835938\n",
      "\n",
      "\n",
      "loss.item()=-82.56248474121094, ablated_edges=110\n",
      "loss.item()=-83.40608978271484, ablated_edges=110\n",
      "loss.item()=-85.1612319946289, ablated_edges=106\n",
      "loss.item()=-84.5433349609375, ablated_edges=109\n",
      "loss.item()=-87.24484252929688, ablated_edges=108\n",
      "loss.item()=-87.91059112548828, ablated_edges=109\n",
      "loss.item()=-88.93428039550781, ablated_edges=108\n",
      "loss.item()=-90.02877807617188, ablated_edges=106\n",
      "loss.item()=-90.34931945800781, ablated_edges=105\n",
      "loss.item()=-92.64315795898438, ablated_edges=105\n",
      "Epochs trained:  90\n",
      "Loss: -92.6432\n",
      "Total preserved: 11504.9893\n",
      "Edges ablated:  105\n",
      "Toxic loss:  13.258737564086914\n",
      "OWT loss:  4.223959922790527\n",
      "Penalty:  tensor(79.3844, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 5.649860668199835e-07, logit diff = -4.487678527832031\n",
      "Best Token: [' the'], P(Alicia) = 6.346738246065797e-07, logit diff = -3.245147705078125\n",
      "\n",
      "\n",
      "loss.item()=-92.26181030273438, ablated_edges=105\n",
      "loss.item()=-95.02326965332031, ablated_edges=106\n",
      "loss.item()=-96.4928970336914, ablated_edges=105\n",
      "loss.item()=-96.34841918945312, ablated_edges=105\n",
      "loss.item()=-97.62252044677734, ablated_edges=102\n",
      "loss.item()=-98.99290466308594, ablated_edges=104\n",
      "loss.item()=-100.52334594726562, ablated_edges=103\n",
      "loss.item()=-101.28272247314453, ablated_edges=103\n",
      "loss.item()=-103.1907730102539, ablated_edges=101\n",
      "loss.item()=-103.94791412353516, ablated_edges=100\n",
      "Epochs trained:  100\n",
      "Loss: -103.9479\n",
      "Total preserved: 11509.4844\n",
      "Edges ablated:  100\n",
      "Toxic loss:  13.022987365722656\n",
      "OWT loss:  4.193967819213867\n",
      "Penalty:  tensor(90.9249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 8.547855259166681e-07, logit diff = -4.143348693847656\n",
      "Best Token: [' the'], P(Alicia) = 9.555842552799731e-07, logit diff = -2.8783187866210938\n",
      "\n",
      "\n",
      "loss.item()=-105.3387451171875, ablated_edges=101\n",
      "loss.item()=-106.89336395263672, ablated_edges=100\n",
      "loss.item()=-106.43453216552734, ablated_edges=99\n",
      "loss.item()=-108.7157211303711, ablated_edges=102\n",
      "loss.item()=-109.95338439941406, ablated_edges=100\n",
      "loss.item()=-110.78926849365234, ablated_edges=101\n",
      "loss.item()=-111.51238250732422, ablated_edges=101\n",
      "loss.item()=-113.12844848632812, ablated_edges=100\n",
      "loss.item()=-113.3653335571289, ablated_edges=98\n",
      "loss.item()=-115.02750396728516, ablated_edges=99\n",
      "Epochs trained:  110\n",
      "Loss: -115.0275\n",
      "Total preserved: 11513.2402\n",
      "Edges ablated:  99\n",
      "Toxic loss:  12.559666633605957\n",
      "OWT loss:  3.6431756019592285\n",
      "Penalty:  tensor(102.4678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 5.808709033772175e-07, logit diff = -4.5810699462890625\n",
      "Best Token: [' the'], P(Alicia) = 6.977195425861282e-07, logit diff = -3.1343231201171875\n",
      "\n",
      "\n",
      "loss.item()=-115.92784881591797, ablated_edges=99\n",
      "loss.item()=-116.54463195800781, ablated_edges=98\n",
      "loss.item()=-119.41181182861328, ablated_edges=98\n",
      "loss.item()=-121.12705993652344, ablated_edges=96\n",
      "loss.item()=-121.62355041503906, ablated_edges=97\n",
      "loss.item()=-121.74147033691406, ablated_edges=96\n",
      "loss.item()=-122.74212646484375, ablated_edges=97\n",
      "loss.item()=-124.53524017333984, ablated_edges=94\n",
      "loss.item()=-126.29042053222656, ablated_edges=95\n",
      "loss.item()=-126.7813720703125, ablated_edges=94\n",
      "Epochs trained:  120\n",
      "Loss: -126.7814\n",
      "Total preserved: 11516.5391\n",
      "Edges ablated:  94\n",
      "Toxic loss:  12.76762580871582\n",
      "OWT loss:  4.579450607299805\n",
      "Penalty:  tensor(114.0137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 6.480682372966839e-07, logit diff = -4.388969421386719\n",
      "Best Token: [' the'], P(Alicia) = 6.928844982212468e-07, logit diff = -3.11639404296875\n",
      "\n",
      "\n",
      "loss.item()=-127.94602966308594, ablated_edges=93\n",
      "loss.item()=-128.79969787597656, ablated_edges=95\n",
      "loss.item()=-130.09271240234375, ablated_edges=93\n",
      "loss.item()=-130.9403839111328, ablated_edges=95\n",
      "loss.item()=-133.34471130371094, ablated_edges=95\n",
      "loss.item()=-133.7942657470703, ablated_edges=92\n",
      "loss.item()=-134.7262725830078, ablated_edges=91\n",
      "loss.item()=-136.41575622558594, ablated_edges=93\n",
      "loss.item()=-136.9857940673828, ablated_edges=92\n",
      "loss.item()=-138.68731689453125, ablated_edges=89\n",
      "Epochs trained:  130\n",
      "Loss: -138.6873\n",
      "Total preserved: 11521.3750\n",
      "Edges ablated:  89\n",
      "Toxic loss:  13.104331970214844\n",
      "OWT loss:  3.7181525230407715\n",
      "Penalty:  tensor(125.5830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 6.702366590616293e-07, logit diff = -4.359352111816406\n",
      "Best Token: [' the'], P(Alicia) = 7.543304150203767e-07, logit diff = -2.9630508422851562\n",
      "\n",
      "\n",
      "loss.item()=-140.29449462890625, ablated_edges=91\n",
      "loss.item()=-141.10479736328125, ablated_edges=92\n",
      "loss.item()=-141.9321746826172, ablated_edges=91\n",
      "loss.item()=-142.29318237304688, ablated_edges=91\n",
      "loss.item()=-144.09124755859375, ablated_edges=91\n",
      "loss.item()=-146.16098022460938, ablated_edges=91\n",
      "loss.item()=-146.34945678710938, ablated_edges=91\n",
      "loss.item()=-147.26583862304688, ablated_edges=89\n",
      "loss.item()=-149.03138732910156, ablated_edges=92\n",
      "loss.item()=-149.683349609375, ablated_edges=91\n",
      "Epochs trained:  140\n",
      "Loss: -149.6833\n",
      "Total preserved: 11520.6914\n",
      "Edges ablated:  91\n",
      "Toxic loss:  12.587115287780762\n",
      "OWT loss:  3.6495916843414307\n",
      "Penalty:  tensor(137.0962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 6.589097552023304e-07, logit diff = -4.407264709472656\n",
      "Best Token: [' the'], P(Alicia) = 8.048506856539461e-07, logit diff = -2.91510009765625\n",
      "\n",
      "\n",
      "loss.item()=-150.97669982910156, ablated_edges=91\n",
      "loss.item()=-152.42947387695312, ablated_edges=89\n",
      "loss.item()=-153.6796417236328, ablated_edges=90\n",
      "loss.item()=-153.34532165527344, ablated_edges=86\n",
      "loss.item()=-154.64151000976562, ablated_edges=88\n",
      "loss.item()=-156.8426971435547, ablated_edges=90\n",
      "loss.item()=-157.39642333984375, ablated_edges=90\n",
      "loss.item()=-160.1011962890625, ablated_edges=87\n",
      "loss.item()=-159.9700927734375, ablated_edges=87\n",
      "loss.item()=-161.6258544921875, ablated_edges=85\n",
      "Epochs trained:  150\n",
      "Loss: -161.6259\n",
      "Total preserved: 11525.3926\n",
      "Edges ablated:  85\n",
      "Toxic loss:  12.948275566101074\n",
      "OWT loss:  4.221805572509766\n",
      "Penalty:  tensor(148.6776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 8.955948374023137e-07, logit diff = -4.271461486816406\n",
      "Best Token: [' the'], P(Alicia) = 1.1008818319169222e-06, logit diff = -2.65594482421875\n",
      "\n",
      "\n",
      "loss.item()=-162.74783325195312, ablated_edges=86\n",
      "loss.item()=-163.5485076904297, ablated_edges=84\n",
      "loss.item()=-165.10946655273438, ablated_edges=83\n",
      "loss.item()=-166.32643127441406, ablated_edges=83\n",
      "loss.item()=-167.84376525878906, ablated_edges=80\n",
      "loss.item()=-168.7957305908203, ablated_edges=83\n",
      "loss.item()=-169.68283081054688, ablated_edges=80\n",
      "loss.item()=-171.43804931640625, ablated_edges=80\n",
      "loss.item()=-171.56591796875, ablated_edges=81\n",
      "loss.item()=-173.12464904785156, ablated_edges=78\n",
      "Epochs trained:  160\n",
      "Loss: -173.1246\n",
      "Total preserved: 11530.3906\n",
      "Edges ablated:  78\n",
      "Toxic loss:  12.8522310256958\n",
      "OWT loss:  4.1804890632629395\n",
      "Penalty:  tensor(160.2724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 6.659329301328398e-07, logit diff = -4.2660369873046875\n",
      "Best Token: [' the'], P(Alicia) = 7.6916842317587e-07, logit diff = -2.9383544921875\n",
      "\n",
      "\n",
      "loss.item()=-173.64007568359375, ablated_edges=79\n",
      "loss.item()=-175.5027313232422, ablated_edges=80\n",
      "loss.item()=-175.644287109375, ablated_edges=79\n",
      "loss.item()=-177.35296630859375, ablated_edges=79\n",
      "loss.item()=-178.7149658203125, ablated_edges=77\n",
      "loss.item()=-180.45343017578125, ablated_edges=78\n",
      "loss.item()=-180.3551025390625, ablated_edges=77\n",
      "loss.item()=-182.15196228027344, ablated_edges=77\n",
      "loss.item()=-183.15333557128906, ablated_edges=77\n",
      "loss.item()=-184.37474060058594, ablated_edges=79\n",
      "Epochs trained:  170\n",
      "Loss: -184.3747\n",
      "Total preserved: 11532.6348\n",
      "Edges ablated:  79\n",
      "Toxic loss:  12.53846263885498\n",
      "OWT loss:  4.135136127471924\n",
      "Penalty:  tensor(171.8363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 7.365035799011821e-07, logit diff = -4.195274353027344\n",
      "Best Token: [' the'], P(Alicia) = 7.865641009630053e-07, logit diff = -2.9011917114257812\n",
      "\n",
      "\n",
      "loss.item()=-185.72958374023438, ablated_edges=77\n",
      "loss.item()=-186.49400329589844, ablated_edges=77\n",
      "loss.item()=-187.80078125, ablated_edges=77\n",
      "loss.item()=-189.00685119628906, ablated_edges=78\n",
      "loss.item()=-190.92724609375, ablated_edges=76\n",
      "loss.item()=-191.5331573486328, ablated_edges=76\n",
      "loss.item()=-192.57113647460938, ablated_edges=76\n",
      "loss.item()=-193.543701171875, ablated_edges=74\n",
      "loss.item()=-195.31983947753906, ablated_edges=74\n",
      "loss.item()=-195.28924560546875, ablated_edges=73\n",
      "Epochs trained:  180\n",
      "Loss: -195.2892\n",
      "Total preserved: 11536.9688\n",
      "Edges ablated:  73\n",
      "Toxic loss:  11.851434707641602\n",
      "OWT loss:  4.2791290283203125\n",
      "Penalty:  tensor(183.4378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 7.636747341166483e-07, logit diff = -4.065376281738281\n",
      "Best Token: [' the'], P(Alicia) = 7.525021032961376e-07, logit diff = -3.0545806884765625\n",
      "\n",
      "\n",
      "loss.item()=-196.90086364746094, ablated_edges=72\n",
      "loss.item()=-198.61868286132812, ablated_edges=72\n",
      "loss.item()=-200.31546020507812, ablated_edges=72\n",
      "loss.item()=-200.2914276123047, ablated_edges=71\n",
      "loss.item()=-202.61178588867188, ablated_edges=73\n",
      "loss.item()=-203.4361572265625, ablated_edges=71\n",
      "loss.item()=-204.4598388671875, ablated_edges=71\n",
      "loss.item()=-205.51669311523438, ablated_edges=72\n",
      "loss.item()=-206.59266662597656, ablated_edges=70\n",
      "loss.item()=-207.98147583007812, ablated_edges=70\n",
      "Epochs trained:  190\n",
      "Loss: -207.9815\n",
      "Total preserved: 11540.6777\n",
      "Edges ablated:  70\n",
      "Toxic loss:  12.944040298461914\n",
      "OWT loss:  3.966722249984741\n",
      "Penalty:  tensor(195.0374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 7.860719506425085e-07, logit diff = -4.152061462402344\n",
      "Best Token: [' the'], P(Alicia) = 8.110899329949461e-07, logit diff = -2.9906692504882812\n",
      "\n",
      "\n",
      "loss.item()=-207.76214599609375, ablated_edges=71\n",
      "loss.item()=-209.87078857421875, ablated_edges=70\n",
      "loss.item()=-209.6343231201172, ablated_edges=68\n",
      "loss.item()=-212.59815979003906, ablated_edges=70\n",
      "loss.item()=-213.78065490722656, ablated_edges=70\n",
      "loss.item()=-214.0928497314453, ablated_edges=69\n",
      "loss.item()=-215.7135467529297, ablated_edges=69\n",
      "loss.item()=-216.8952178955078, ablated_edges=69\n",
      "loss.item()=-217.69415283203125, ablated_edges=68\n",
      "loss.item()=-219.7125244140625, ablated_edges=67\n",
      "Epochs trained:  200\n",
      "Loss: -219.7125\n",
      "Total preserved: 11543.6748\n",
      "Edges ablated:  67\n",
      "Toxic loss:  13.080759048461914\n",
      "OWT loss:  4.303644180297852\n",
      "Penalty:  tensor(206.6318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 8.163637517100142e-07, logit diff = -4.327522277832031\n",
      "Best Token: [' the'], P(Alicia) = 9.230402042703645e-07, logit diff = -2.9377822875976562\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = -1 * (regularization_strength * penalty + alpha * tox_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            with torch.no_grad():\n",
    "                test_ioi_sentences = [\"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\", \"While Joshua and Alicia were commuting to the restaurant, Joshua gave a snack to\"]\n",
    "                for test_ioi_sentence in test_ioi_sentences:\n",
    "                    correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "                    other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "                    test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "                    generation = model(test_ioi_tokens)[0][:, -1]\n",
    "                    probs = torch.softmax(generation, dim=-1)\n",
    "                    print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    # epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    epochs_left = -1\n",
    "    # log_every = int(input('set log frequency'))\n",
    "    # edge_threshold = int(input('set edge threshold'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/circuit_covering_alternative_necessary_params_dict_lambda={regularization_strength}_{alpha=}_means_ioi=False_{template_type=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model before and after circuit breaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/ioi_sentences_test.pkl\", \"rb\") as f:\n",
    "    ioi_sentences_test = pickle.load(f)\n",
    "    # ioi_sentences_test = [t[2] for t in ioi_sentences_test]\n",
    "\n",
    "with open(\"data/eval_uniform.pkl\", \"rb\") as f:\n",
    "    uniform_samples = pickle.load(f)\n",
    "    uniform_sentences = [t[2] for t in uniform_samples]\n",
    "\n",
    "original_model = load_demo_gpt2(means=False)\n",
    "\n",
    "# with open(\"models/masked_gpt2_mean_ablation_v6.pkl\", \"rb\") as f:\n",
    "#     model.state_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on an ioi_sentence\n",
    "ioi_sentence = ioi_sentences_test[0]\n",
    "print(ioi_sentence)\n",
    "# ioi_tokens = tokenizer(ioi_sentence, return_tensors='pt').input_ids.to('cuda')\n",
    "\n",
    "original_model.eval()\n",
    "original_model.to('cuda')\n",
    "def get_last_token(model, prompt, topk=5):\n",
    "    # generate last token\n",
    "    tokens = tokenizer(prompt, return_tensors='pt').input_ids[:, :-1]\n",
    "\n",
    "    # generate one token, decode original_model(ioi_tokens[:, :-1])\n",
    "    model_outputs = model(tokens)[0]\n",
    "    model_outputs = model_outputs.squeeze(0)[-1]\n",
    "    probs = torch.nn.functional.softmax(model_outputs, dim=-1)\n",
    "\n",
    "    topk_outputs = torch.topk(model_outputs, topk)\n",
    "    topk_tokens = topk_outputs.indices\n",
    "    topk_probs = probs[topk_outputs.indices]\n",
    "    \n",
    "    # decode tokens\n",
    "    for i in range(topk):\n",
    "        print(f\"{tokenizer.decode(topk_tokens[i].unsqueeze(0))}, probability of {topk_probs[i]}\")\n",
    "    topk_tokens_decoded = tokenizer.batch_decode(topk_tokens)\n",
    "    return topk_tokens_decoded, topk_probs\n",
    "\n",
    "print(\"Before ablation\")\n",
    "_ = get_last_token(original_model, ioi_sentence)\n",
    "print()\n",
    "print()\n",
    "print(\"After ablation\")\n",
    "_ = get_last_token(model, ioi_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try on uniform samples\n",
    "for idx in range(3):\n",
    "    print(uniform_sentences[idx])\n",
    "    print(\"Before ablation\")\n",
    "    _ = get_last_token(original_model, uniform_sentences[idx])\n",
    "    print()\n",
    "    print(\"After ablation\")\n",
    "    _ = get_last_token(model, uniform_sentences[idx])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize mask\n",
    "Create the computational graphs in edge attribution patching paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate which nodes will be in the graph\n",
    "connected_nodes = set()\n",
    "# add embed node at position\n",
    "# connected_nodes.add((-1, \"embed\"))\n",
    "n_heads = 12\n",
    "n_layers = 12\n",
    "\n",
    "# associate each node with a position\n",
    "all_possible_nodes = [(-1, \"embed\")]\n",
    "mask_dict = {}\n",
    "# empty tensor\n",
    "mask_dict[\"embed\"] = torch.zeros(size=(0,))\n",
    "for idx in range(len(mask_params)):\n",
    "    if \"attention\" in param_names[idx]:\n",
    "        layer = int(param_names[idx].split(\".\")[1])\n",
    "        for i in range(n_heads):\n",
    "            all_possible_nodes.append((layer, f\"a{layer}.{i}\"))\n",
    "            mask_dict[f\"a{layer}.{i}\"] = mask_params[idx][:,i].detach().cpu()\n",
    "    elif \"mlp\" in param_names[idx]:\n",
    "        layer = int(param_names[idx].split(\".\")[1])\n",
    "        all_possible_nodes.append((layer, f\"m{layer}\"))\n",
    "        mask_dict[f\"m{layer}\"] = mask_params[idx].detach().cpu()\n",
    "all_possible_nodes.append((n_heads, \"output\"))\n",
    "mask_dict[\"output\"] = mask_params[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate where edges are based on the mask\n",
    "# Edge between node i and node j if mask_dict[i][all_possible_nodes.index(j)] == 0\n",
    "sufficient = True\n",
    "\n",
    "edges = []\n",
    "for i in range(len(all_possible_nodes)):\n",
    "    for j in range(len(all_possible_nodes)):\n",
    "        j_index = all_possible_nodes.index(all_possible_nodes[j])\n",
    "        if j_index < len(mask_dict[all_possible_nodes[i][1]]) and mask_dict[all_possible_nodes[i][1]][all_possible_nodes.index(all_possible_nodes[j])] == (1 if sufficient else 0):\n",
    "            edges.append((all_possible_nodes[i], all_possible_nodes[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aligned_graph(all_possible_nodes, edges):\n",
    "    G = pgv.AGraph(strict=False, directed=True)\n",
    "\n",
    "    # Find the maximum layer number for adjusting the graph\n",
    "    max_layer = max(layer for layer, _ in all_possible_nodes if isinstance(layer, int))\n",
    "    nodes_with_edges = set([node for edge in edges for node in edge])\n",
    "    print(nodes_with_edges)\n",
    "    # Add nodes and edges to the graph\n",
    "    # for node in all_possible_nodes:\n",
    "    #     if node in [edge[0] for edge in edges] or node in [edge[1] for edge in edges]:\n",
    "    #         G.add_node(node[1], layer=str(max_layer - node[0]))\n",
    "\n",
    "    for edge in edges:\n",
    "        G.add_edge(edge[1][1], edge[0][1])\n",
    "\n",
    "    # Create subgraphs to ensure nodes of the same layer have the same rank\n",
    "    for layer in range(max_layer, -2, -1):\n",
    "        with G.subgraph(name=f'cluster_{layer}') as s:\n",
    "            s.graph_attr['rank'] = 'same'\n",
    "            for node in nodes_with_edges:\n",
    "                if node[0] == layer:\n",
    "                    s.add_node(node[1])\n",
    "\n",
    "    # Apply layout and render the graph\n",
    "    G.layout(prog='dot')\n",
    "    G.draw('aligned_graph.png')\n",
    "    return Image('aligned_graph.png')\n",
    "\n",
    "# Call the function with your nodes and edges\n",
    "flipped_graph_image = create_aligned_graph(all_possible_nodes, edges)\n",
    "\n",
    "# To display the graph in Jupyter Notebook\n",
    "flipped_graph_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "unlrn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
