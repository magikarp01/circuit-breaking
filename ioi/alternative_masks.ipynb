{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train different kinds of masks over IOI edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir(\"/data/phillip_guo/circuit-breaking/ioi/\")\n",
    "from models import load_gpt2_weights, load_demo_gpt2, tokenizer\n",
    "from data import retrieve_toxic_data, retrieve_owt_data, retrieve_toxic_data_low_loss, retrieve_toxic_filtered_data, FILTER_DEMO_LEN, CONTEXT_LENGTH\n",
    "from inference import infer_batch_with_owt, infer_batch, prepare_fixed_demo, criterion\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import pickle\n",
    "import datasets\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from itertools import cycle\n",
    "# from eval import evaluate_model\n",
    "from data import batch_text_to_tokens\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train params of mask\n",
    "Train without the original D_train loss term (only mask loss and IOI data loss)\n",
    "Finds necessary (but not sufficient) edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_batch_size = 10 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 1\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "\n",
    "template_type = \"single\"\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None, template_type=template_type)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "# with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "#     means = pickle.load(f)[0][0]\n",
    "means_ioi = True\n",
    "if means_ioi:\n",
    "    with open(\"data/gpt2_ioi_abc_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "else:\n",
    "    with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "\n",
    "model = load_demo_gpt2(means=means)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 50 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 1 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 100\n",
    "max_steps_per_epoch = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_mask torch.Size([157])\n",
      "embed.W_E torch.Size([50257, 768])\n",
      "pos_embed.W_pos torch.Size([1024, 768])\n",
      "ln_final.w torch.Size([768])\n",
      "ln_final.b torch.Size([768])\n",
      "unembed.W_U torch.Size([768, 50257])\n",
      "unembed.b_U torch.Size([50257])\n",
      "blocks.0.edge_mask_attentions torch.Size([1, 12])\n",
      "blocks.0.edge_mask_mlp torch.Size([13])\n",
      "blocks.0.ln1.w torch.Size([768])\n",
      "blocks.0.ln1.b torch.Size([768])\n",
      "blocks.0.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.0.attn.b_Q torch.Size([12, 64])\n",
      "blocks.0.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.0.attn.b_K torch.Size([12, 64])\n",
      "blocks.0.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.0.attn.b_V torch.Size([12, 64])\n",
      "blocks.0.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.0.attn.b_O torch.Size([768])\n",
      "blocks.0.ln2.w torch.Size([768])\n",
      "blocks.0.ln2.b torch.Size([768])\n",
      "blocks.0.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.0.mlp.b_in torch.Size([3072])\n",
      "blocks.0.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.0.mlp.b_out torch.Size([768])\n",
      "blocks.1.edge_mask_attentions torch.Size([14, 12])\n",
      "blocks.1.edge_mask_mlp torch.Size([26])\n",
      "blocks.1.ln1.w torch.Size([768])\n",
      "blocks.1.ln1.b torch.Size([768])\n",
      "blocks.1.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.1.attn.b_Q torch.Size([12, 64])\n",
      "blocks.1.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.1.attn.b_K torch.Size([12, 64])\n",
      "blocks.1.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.1.attn.b_V torch.Size([12, 64])\n",
      "blocks.1.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.1.attn.b_O torch.Size([768])\n",
      "blocks.1.ln2.w torch.Size([768])\n",
      "blocks.1.ln2.b torch.Size([768])\n",
      "blocks.1.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.1.mlp.b_in torch.Size([3072])\n",
      "blocks.1.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.1.mlp.b_out torch.Size([768])\n",
      "blocks.2.edge_mask_attentions torch.Size([27, 12])\n",
      "blocks.2.edge_mask_mlp torch.Size([39])\n",
      "blocks.2.ln1.w torch.Size([768])\n",
      "blocks.2.ln1.b torch.Size([768])\n",
      "blocks.2.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.2.attn.b_Q torch.Size([12, 64])\n",
      "blocks.2.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.2.attn.b_K torch.Size([12, 64])\n",
      "blocks.2.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.2.attn.b_V torch.Size([12, 64])\n",
      "blocks.2.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.2.attn.b_O torch.Size([768])\n",
      "blocks.2.ln2.w torch.Size([768])\n",
      "blocks.2.ln2.b torch.Size([768])\n",
      "blocks.2.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.2.mlp.b_in torch.Size([3072])\n",
      "blocks.2.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.2.mlp.b_out torch.Size([768])\n",
      "blocks.3.edge_mask_attentions torch.Size([40, 12])\n",
      "blocks.3.edge_mask_mlp torch.Size([52])\n",
      "blocks.3.ln1.w torch.Size([768])\n",
      "blocks.3.ln1.b torch.Size([768])\n",
      "blocks.3.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.3.attn.b_Q torch.Size([12, 64])\n",
      "blocks.3.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.3.attn.b_K torch.Size([12, 64])\n",
      "blocks.3.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.3.attn.b_V torch.Size([12, 64])\n",
      "blocks.3.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.3.attn.b_O torch.Size([768])\n",
      "blocks.3.ln2.w torch.Size([768])\n",
      "blocks.3.ln2.b torch.Size([768])\n",
      "blocks.3.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.3.mlp.b_in torch.Size([3072])\n",
      "blocks.3.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.3.mlp.b_out torch.Size([768])\n",
      "blocks.4.edge_mask_attentions torch.Size([53, 12])\n",
      "blocks.4.edge_mask_mlp torch.Size([65])\n",
      "blocks.4.ln1.w torch.Size([768])\n",
      "blocks.4.ln1.b torch.Size([768])\n",
      "blocks.4.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.4.attn.b_Q torch.Size([12, 64])\n",
      "blocks.4.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.4.attn.b_K torch.Size([12, 64])\n",
      "blocks.4.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.4.attn.b_V torch.Size([12, 64])\n",
      "blocks.4.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.4.attn.b_O torch.Size([768])\n",
      "blocks.4.ln2.w torch.Size([768])\n",
      "blocks.4.ln2.b torch.Size([768])\n",
      "blocks.4.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.4.mlp.b_in torch.Size([3072])\n",
      "blocks.4.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.4.mlp.b_out torch.Size([768])\n",
      "blocks.5.edge_mask_attentions torch.Size([66, 12])\n",
      "blocks.5.edge_mask_mlp torch.Size([78])\n",
      "blocks.5.ln1.w torch.Size([768])\n",
      "blocks.5.ln1.b torch.Size([768])\n",
      "blocks.5.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.5.attn.b_Q torch.Size([12, 64])\n",
      "blocks.5.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.5.attn.b_K torch.Size([12, 64])\n",
      "blocks.5.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.5.attn.b_V torch.Size([12, 64])\n",
      "blocks.5.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.5.attn.b_O torch.Size([768])\n",
      "blocks.5.ln2.w torch.Size([768])\n",
      "blocks.5.ln2.b torch.Size([768])\n",
      "blocks.5.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.5.mlp.b_in torch.Size([3072])\n",
      "blocks.5.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.5.mlp.b_out torch.Size([768])\n",
      "blocks.6.edge_mask_attentions torch.Size([79, 12])\n",
      "blocks.6.edge_mask_mlp torch.Size([91])\n",
      "blocks.6.ln1.w torch.Size([768])\n",
      "blocks.6.ln1.b torch.Size([768])\n",
      "blocks.6.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.6.attn.b_Q torch.Size([12, 64])\n",
      "blocks.6.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.6.attn.b_K torch.Size([12, 64])\n",
      "blocks.6.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.6.attn.b_V torch.Size([12, 64])\n",
      "blocks.6.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.6.attn.b_O torch.Size([768])\n",
      "blocks.6.ln2.w torch.Size([768])\n",
      "blocks.6.ln2.b torch.Size([768])\n",
      "blocks.6.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.6.mlp.b_in torch.Size([3072])\n",
      "blocks.6.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.6.mlp.b_out torch.Size([768])\n",
      "blocks.7.edge_mask_attentions torch.Size([92, 12])\n",
      "blocks.7.edge_mask_mlp torch.Size([104])\n",
      "blocks.7.ln1.w torch.Size([768])\n",
      "blocks.7.ln1.b torch.Size([768])\n",
      "blocks.7.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.7.attn.b_Q torch.Size([12, 64])\n",
      "blocks.7.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.7.attn.b_K torch.Size([12, 64])\n",
      "blocks.7.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.7.attn.b_V torch.Size([12, 64])\n",
      "blocks.7.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.7.attn.b_O torch.Size([768])\n",
      "blocks.7.ln2.w torch.Size([768])\n",
      "blocks.7.ln2.b torch.Size([768])\n",
      "blocks.7.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.7.mlp.b_in torch.Size([3072])\n",
      "blocks.7.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.7.mlp.b_out torch.Size([768])\n",
      "blocks.8.edge_mask_attentions torch.Size([105, 12])\n",
      "blocks.8.edge_mask_mlp torch.Size([117])\n",
      "blocks.8.ln1.w torch.Size([768])\n",
      "blocks.8.ln1.b torch.Size([768])\n",
      "blocks.8.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.8.attn.b_Q torch.Size([12, 64])\n",
      "blocks.8.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.8.attn.b_K torch.Size([12, 64])\n",
      "blocks.8.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.8.attn.b_V torch.Size([12, 64])\n",
      "blocks.8.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.8.attn.b_O torch.Size([768])\n",
      "blocks.8.ln2.w torch.Size([768])\n",
      "blocks.8.ln2.b torch.Size([768])\n",
      "blocks.8.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.8.mlp.b_in torch.Size([3072])\n",
      "blocks.8.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.8.mlp.b_out torch.Size([768])\n",
      "blocks.9.edge_mask_attentions torch.Size([118, 12])\n",
      "blocks.9.edge_mask_mlp torch.Size([130])\n",
      "blocks.9.ln1.w torch.Size([768])\n",
      "blocks.9.ln1.b torch.Size([768])\n",
      "blocks.9.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.9.attn.b_Q torch.Size([12, 64])\n",
      "blocks.9.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.9.attn.b_K torch.Size([12, 64])\n",
      "blocks.9.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.9.attn.b_V torch.Size([12, 64])\n",
      "blocks.9.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.9.attn.b_O torch.Size([768])\n",
      "blocks.9.ln2.w torch.Size([768])\n",
      "blocks.9.ln2.b torch.Size([768])\n",
      "blocks.9.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.9.mlp.b_in torch.Size([3072])\n",
      "blocks.9.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.9.mlp.b_out torch.Size([768])\n",
      "blocks.10.edge_mask_attentions torch.Size([131, 12])\n",
      "blocks.10.edge_mask_mlp torch.Size([143])\n",
      "blocks.10.ln1.w torch.Size([768])\n",
      "blocks.10.ln1.b torch.Size([768])\n",
      "blocks.10.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.10.attn.b_Q torch.Size([12, 64])\n",
      "blocks.10.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.10.attn.b_K torch.Size([12, 64])\n",
      "blocks.10.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.10.attn.b_V torch.Size([12, 64])\n",
      "blocks.10.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.10.attn.b_O torch.Size([768])\n",
      "blocks.10.ln2.w torch.Size([768])\n",
      "blocks.10.ln2.b torch.Size([768])\n",
      "blocks.10.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.10.mlp.b_in torch.Size([3072])\n",
      "blocks.10.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.10.mlp.b_out torch.Size([768])\n",
      "blocks.11.edge_mask_attentions torch.Size([144, 12])\n",
      "blocks.11.edge_mask_mlp torch.Size([156])\n",
      "blocks.11.ln1.w torch.Size([768])\n",
      "blocks.11.ln1.b torch.Size([768])\n",
      "blocks.11.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.11.attn.b_Q torch.Size([12, 64])\n",
      "blocks.11.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.11.attn.b_K torch.Size([12, 64])\n",
      "blocks.11.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.11.attn.b_V torch.Size([12, 64])\n",
      "blocks.11.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.11.attn.b_O torch.Size([768])\n",
      "blocks.11.ln2.w torch.Size([768])\n",
      "blocks.11.ln2.b torch.Size([768])\n",
      "blocks.11.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.11.mlp.b_in torch.Size([3072])\n",
      "blocks.11.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.11.mlp.b_out torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Q:\\n\\nFind matching strings in table column Oracle 10g\\n\\nI am trying to search a varchar2 column in a table for matching strings using the value in another column. The column being searched allows free form text and allows words and numbers of different lengths. I want to find a string that is not part of a larger string of text and numbers.\\nExample: 1234a should match \"Invoice #1234a\" but not \"Invoice #1234a567\"\\nSteps Taken:\\nI have tried Regexp_Like(table2.Searched_Field,table1.Invoice) but get many false hits when the invoice number has a number sequence that can be found in other invoice numbers.\\n\\nA:\\n\\nSuggestions:\\n\\nMatch only at end:\\nREGEXP_LIKE(table2.Searched_Field, table1.Invoice || \\'$\\')\\n\\nMatch exactly:\\ntable2.Searched_Field = \\'Invoice #\\' || table1.Invoice\\n\\nMatch only at end with LIKE:\\ntable2.Searched_Field LIKE \\'%\\' || table1.Invoice\\n\\n'], 'meta': {'pile_set_name': ['StackExchange']}, 'tokens': [tensor([48]), tensor([25]), tensor([198]), tensor([198]), tensor([16742]), tensor([12336]), tensor([13042]), tensor([287]), tensor([3084]), tensor([5721]), tensor([18650]), tensor([838]), tensor([70]), tensor([198]), tensor([198]), tensor([40]), tensor([716]), tensor([2111]), tensor([284]), tensor([2989]), tensor([257]), tensor([410]), tensor([998]), tensor([283]), tensor([17]), tensor([5721]), tensor([287]), tensor([257]), tensor([3084]), tensor([329]), tensor([12336]), tensor([13042]), tensor([1262]), tensor([262]), tensor([1988]), tensor([287]), tensor([1194]), tensor([5721]), tensor([13]), tensor([383]), tensor([5721]), tensor([852]), tensor([16499]), tensor([3578]), tensor([1479]), tensor([1296]), tensor([2420]), tensor([290]), tensor([3578]), tensor([2456])]}\n",
      "{'text': ['/*\\n\\tMantis PCI bridge driver\\n\\n\\tCopyright (C) Manu Abraham (abraham.manu@gmail.com)\\n\\n\\tThis program is free software; you can redistribute it and/or modify\\n\\tit under the terms of the GNU General Public License as published by\\n\\tthe Free Software Foundation; either version 2 of the License, or\\n\\t(at your option) any later version.\\n\\n\\tThis program is distributed in the hope that it will be useful,\\n\\tbut WITHOUT ANY WARRANTY; without even the implied warranty of\\n\\tMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n\\tGNU General Public License for more details.\\n\\n\\tYou should have received a copy of the GNU General Public License\\n\\talong with this program; if not, write to the Free Software\\n\\tFoundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.\\n*/\\n\\n#include <asm/io.h>\\n#include <linux/ioport.h>\\n#include <linux/pci.h>\\n#include <linux/i2c.h>\\n\\n#include \"dmxdev.h\"\\n#include \"dvbdev.h\"\\n#include \"dvb_demux.h\"\\n#include \"dvb_frontend.h\"\\n#include \"dvb_net.h\"\\n\\n#include \"mantis_common.h\"\\n#include \"mantis_reg.h\"\\n#include \"mantis_i2c.h\"\\n\\n#define TRIALS\\t\\t\\t10000\\n\\nstatic int mantis_i2c_read(struct mantis_pci *mantis, const struct i2c_msg *msg)\\n{\\n\\tu32 rxd, i, stat, trials;\\n\\n\\tdprintk(MANTIS_INFO, 0, \"        %s:  Address=[0x%02x] <R>[ \",\\n\\t\\t__func__, msg->addr);\\n\\n\\tfor (i = 0; i < msg->len; i++) {\\n\\t\\trxd = (msg->addr << 25) | (1 << 24)\\n\\t\\t\\t\\t\\t| MANTIS_I2C_RATE_3\\n\\t\\t\\t\\t\\t| MANTIS_I2C_STOP\\n\\t\\t\\t\\t\\t| MANTIS_I2C_PGMODE;\\n\\n\\t\\tif (i == (msg->len - 1))\\n\\t\\t\\trxd &= ~MANTIS_I2C_STOP;\\n\\n\\t\\tmmwrite(MANTIS_INT_I2CDONE, MANTIS_INT_STAT);\\n\\t\\tmmwrite(rxd, MANTIS_I2CDATA_CTL);\\n\\n\\t\\t/* wait for xfer completion */\\n\\t\\tfor (trials = 0; trials < TRIALS; trials++) {\\n\\t\\t\\tstat = mmread(MANTIS_INT_STAT);\\n\\t\\t\\tif (stat & MANTIS_INT_I2CDONE)\\n\\t\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\tdprintk(MANTIS_TMG, 0, \"I2CDONE: trials=%d\\\\n\", trials);\\n\\n\\t\\t/* wait for xfer completion */\\n\\t\\tfor (trials = 0; trials < TRIALS; trials++) {\\n\\t\\t\\tstat = mmread(MANTIS_INT_STAT);\\n\\t\\t\\tif (stat & MANTIS_INT_I2CRACK)\\n\\t\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\tdprintk(MANTIS_TMG, 0, \"I2CRACK: trials=%d\\\\n\", trials);\\n\\n\\t\\trxd = mmread(MANTIS_I2CDATA_CTL);\\n\\t\\tmsg->buf[i] = (u8)((rxd >> 8) & 0xFF);\\n\\t\\tdprintk(MANTIS_INFO, 0, \"%02x \", msg->buf[i]);\\n\\t}\\n\\tdprintk(MANTIS_INFO, 0, \"]\\\\n\");\\n\\n\\treturn 0;\\n}\\n\\nstatic int mantis_i2c_write(struct mantis_pci *mantis, const struct i2c_msg *msg)\\n{\\n\\tint i;\\n\\tu32 txd = 0, stat, trials;\\n\\n\\tdprintk(MANTIS_INFO, 0, \"        %s: Address=[0x%02x] <W>[ \",\\n\\t\\t__func__, msg->addr);\\n\\n\\tfor (i = 0; i < msg->len; i++) {\\n\\t\\tdprintk(MANTIS_INFO, 0, \"%02x \", msg->buf[i]);\\n\\t\\ttxd = (msg->addr << 25) | (msg->buf[i] << 8)\\n\\t\\t\\t\\t\\t| MANTIS_I2C_RATE_3\\n\\t\\t\\t\\t\\t| MANTIS_I2C_STOP\\n\\t\\t\\t\\t\\t| MANTIS_I2C_PGMODE;\\n\\n\\t\\tif (i == (msg->len - 1))\\n\\t\\t\\ttxd &= ~MANTIS_I2C_STOP;\\n\\n\\t\\tmmwrite(MANTIS_INT_I2CDONE, MANTIS_INT_STAT);\\n\\t\\tmmwrite(txd, MANTIS_I2CDATA_CTL);\\n\\n\\t\\t/* wait for xfer completion */\\n\\t\\tfor (trials = 0; trials < TRIALS; trials++) {\\n\\t\\t\\tstat = mmread(MANTIS_INT_STAT);\\n\\t\\t\\tif (stat & MANTIS_INT_I2CDONE)\\n\\t\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\tdprintk(MANTIS_TMG, 0, \"I2CDONE: trials=%d\\\\n\", trials);\\n\\n\\t\\t/* wait for xfer completion */\\n\\t\\tfor (trials = 0; trials < TRIALS; trials++) {\\n\\t\\t\\tstat = mmread(MANTIS_INT_STAT);\\n\\t\\t\\tif (stat & MANTIS_INT_I2CRACK)\\n\\t\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\tdprintk(MANTIS_TMG, 0, \"I2CRACK: trials=%d\\\\n\", trials);\\n\\t}\\n\\tdprintk(MANTIS_INFO, 0, \"]\\\\n\");\\n\\n\\treturn 0;\\n}\\n\\nstatic int mantis_i2c_xfer(struct i2c_adapter *adapter, struct i2c_msg *msgs, int num)\\n{\\n\\tint ret = 0, i = 0, trials;\\n\\tu32 stat, data, txd;\\n\\tstruct mantis_pci *mantis;\\n\\tstruct mantis_hwconfig *config;\\n\\n\\tmantis = i2c_get_adapdata(adapter);\\n\\tBUG_ON(!mantis);\\n\\tconfig = mantis->hwconfig;\\n\\tBUG_ON(!config);\\n\\n\\tdprintk(MANTIS_DEBUG, 1, \"Messages:%d\", num);\\n\\tmutex_lock(&mantis->i2c_lock);\\n\\n\\twhile (i < num) {\\n\\t\\t/* Byte MODE */\\n\\t\\tif ((config->i2c_mode & MANTIS_BYTE_MODE) &&\\n\\t\\t    ((i + 1) < num)\\t\\t\\t&&\\n\\t\\t    (msgs[i].len < 2)\\t\\t\\t&&\\n\\t\\t    (msgs[i + 1].len < 2)\\t\\t&&\\n\\t\\t    (msgs[i + 1].flags & I2C_M_RD)) {\\n\\n\\t\\t\\tdprintk(MANTIS_DEBUG, 0, \"        Byte MODE:\\\\n\");\\n\\n\\t\\t\\t/* Read operation */\\n\\t\\t\\ttxd = msgs[i].addr << 25 | (0x1 << 24)\\n\\t\\t\\t\\t\\t\\t | (msgs[i].buf[0] << 16)\\n\\t\\t\\t\\t\\t\\t | MANTIS_I2C_RATE_3;\\n\\n\\t\\t\\tmmwrite(txd, MANTIS_I2CDATA_CTL);\\n\\t\\t\\t/* wait for xfer completion */\\n\\t\\t\\tfor (trials = 0; trials < TRIALS; trials++) {\\n\\t\\t\\t\\tstat = mmread(MANTIS_INT_STAT);\\n\\t\\t\\t\\tif (stat & MANTIS_INT_I2CDONE)\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\n\\t\\t\\t/* check for xfer completion */\\n\\t\\t\\tif (stat & MANTIS_INT_I2CDONE) {\\n\\t\\t\\t\\t/* check xfer was acknowledged */\\n\\t\\t\\t\\tif (stat & MANTIS_INT_I2CRACK) {\\n\\t\\t\\t\\t\\tdata = mmread(MANTIS_I2CDATA_CTL);\\n\\t\\t\\t\\t\\tmsgs[i + 1].buf[0] = (data >> 8) & 0xff;\\n\\t\\t\\t\\t\\tdprintk(MANTIS_DEBUG, 0, \"        Byte <%d> RXD=0x%02x  [%02x]\\\\n\", 0x0, data, msgs[i + 1].buf[0]);\\n\\t\\t\\t\\t} else {\\n\\t\\t\\t\\t\\t/* I/O error */\\n\\t\\t\\t\\t\\tdprintk(MANTIS_ERROR, 1, \"        I/O error, LINE:%d\", __LINE__);\\n\\t\\t\\t\\t\\tret = -EIO;\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\t}\\n\\t\\t\\t} else {\\n\\t\\t\\t\\t/* I/O error */\\n\\t\\t\\t\\tdprintk(MANTIS_ERROR, 1, \"        I/O error, LINE:%d\", __LINE__);\\n\\t\\t\\t\\tret = -EIO;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t\\ti += 2; /* Write/Read operation in one go */\\n\\t\\t}\\n\\n\\t\\tif (i < num) {\\n\\t\\t\\tif (msgs[i].flags & I2C_M_RD)\\n\\t\\t\\t\\tret = mantis_i2c_read(mantis, &msgs[i]);\\n\\t\\t\\telse\\n\\t\\t\\t\\tret = mantis_i2c_write(mantis, &msgs[i]);\\n\\n\\t\\t\\ti++;\\n\\t\\t\\tif (ret < 0)\\n\\t\\t\\t\\tgoto bail_out;\\n\\t\\t}\\n\\n\\t}\\n\\n\\tmutex_unlock(&mantis->i2c_lock);\\n\\n\\treturn num;\\n\\nbail_out:\\n\\tmutex_unlock(&mantis->i2c_lock);\\n\\treturn ret;\\n}\\n\\nstatic u32 mantis_i2c_func(struct i2c_adapter *adapter)\\n{\\n\\treturn I2C_FUNC_SMBUS_EMUL;\\n}\\n\\nstatic struct i2c_algorithm mantis_algo = {\\n\\t.master_xfer\\t\\t= mantis_i2c_xfer,\\n\\t.functionality\\t\\t= mantis_i2c_func,\\n};\\n\\nint __devinit mantis_i2c_init(struct mantis_pci *mantis)\\n{\\n\\tu32 intstat, intmask;\\n\\tstruct i2c_adapter *i2c_adapter = &mantis->adapter;\\n\\tstruct pci_dev *pdev\\t\\t= mantis->pdev;\\n\\n\\tinit_waitqueue_head(&mantis->i2c_wq);\\n\\tmutex_init(&mantis->i2c_lock);\\n\\tstrncpy(i2c_adapter->name, \"Mantis I2C\", sizeof(i2c_adapter->name));\\n\\ti2c_set_adapdata(i2c_adapter, mantis);\\n\\n\\ti2c_adapter->owner\\t= THIS_MODULE;\\n\\ti2c_adapter->algo\\t= &mantis_algo;\\n\\ti2c_adapter->algo_data\\t= NULL;\\n\\ti2c_adapter->timeout\\t= 500;\\n\\ti2c_adapter->retries\\t= 3;\\n\\ti2c_adapter->dev.parent\\t= &pdev->dev;\\n\\n\\tmantis->i2c_rc\\t\\t= i2c_add_adapter(i2c_adapter);\\n\\tif (mantis->i2c_rc < 0)\\n\\t\\treturn mantis->i2c_rc;\\n\\n\\tdprintk(MANTIS_DEBUG, 1, \"Initializing I2C ..\");\\n\\n\\tintstat = mmread(MANTIS_INT_STAT);\\n\\tintmask = mmread(MANTIS_INT_MASK);\\n\\tmmwrite(intstat, MANTIS_INT_STAT);\\n\\tdprintk(MANTIS_DEBUG, 1, \"Disabling I2C interrupt\");\\n\\tintmask = mmread(MANTIS_INT_MASK);\\n\\tmmwrite((intmask & ~MANTIS_INT_I2CDONE), MANTIS_INT_MASK);\\n\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL_GPL(mantis_i2c_init);\\n\\nint mantis_i2c_exit(struct mantis_pci *mantis)\\n{\\n\\tu32 intmask;\\n\\n\\tdprintk(MANTIS_DEBUG, 1, \"Disabling I2C interrupt\");\\n\\tintmask = mmread(MANTIS_INT_MASK);\\n\\tmmwrite((intmask & ~MANTIS_INT_I2CDONE), MANTIS_INT_MASK);\\n\\n\\tdprintk(MANTIS_DEBUG, 1, \"Removing I2C adapter\");\\n\\treturn i2c_del_adapter(&mantis->adapter);\\n}\\nEXPORT_SYMBOL_GPL(mantis_i2c_exit);\\n'], 'meta': {'pile_set_name': ['Github']}, 'tokens': [tensor([15211]), tensor([198]), tensor([197]), tensor([44]), tensor([20836]), tensor([17976]), tensor([7696]), tensor([4639]), tensor([628]), tensor([197]), tensor([15269]), tensor([357]), tensor([34]), tensor([8]), tensor([1869]), tensor([84]), tensor([16660]), tensor([357]), tensor([397]), tensor([13220]), tensor([13]), tensor([805]), tensor([84]), tensor([31]), tensor([14816]), tensor([13]), tensor([785]), tensor([8]), tensor([628]), tensor([197]), tensor([1212]), tensor([1430]), tensor([318]), tensor([1479]), tensor([3788]), tensor([26]), tensor([345]), tensor([460]), tensor([17678]), tensor([4163]), tensor([340]), tensor([290]), tensor([14]), tensor([273]), tensor([13096]), tensor([198]), tensor([197]), tensor([270]), tensor([739]), tensor([262])]}\n",
      "{'text': [\"Ictal autoscopic phenomena and near death experiences: a study of five patients with ictal autoscopies.\\nAutoscopic phenomena in general may-among other conditions-occur during epileptic seizures and near death experiences. We set the hypothesis that ictal autoscopic phenomena and near death experiences have a similar semiology as measured by the Near Death Experience Questionnaire. We also investigated whether patients with aura before temporal lobe seizures with or without autoscopic phenomena could be distinguished by this questionnaire. For these purposes, we examined five patients with ictal autoscopy and 12 patients with aura before temporal lobe seizures without ictal autoscopy as controls. We used a cut-off of 7 points or higher on the Near Death Experience Questionnaire for indicating the semiology of a near death experience and for distinguishing patients with ictal autoscopy from controls. This cut-off separated patients with ictal autoscopic phenomena from aura before temporal lobe seizures without autoscopy (p = 0.0002, two-sided, exact Fisher's Test; specificity: 100 % [CI95 % 77.9 and 100 %], sensitivity: 100 % [CI95 % 54.9 and 100 %]). Furthermore, all autoscopic patients (range 7-10) and none of the controls (range 0-5) had scores of 7 points or higher. Thus, the individual experiences during simple partial autoscopic seizures and near death experiences are similar, at least in some prominent aspects. These findings might be of particular interest for the pathophysiology of near death experiences, as all patients with ictal autoscopic phenomena had an epileptic dysfunction at the temporo-parietal junction or its neighboring regions. Therefore, a malfunction of this brain region might also be involved in near death experiences of other origins especially during states which could cause a near death experience and a cerebral excitability.\"], 'meta': {'pile_set_name': ['PubMed Abstracts']}, 'tokens': [tensor([40]), tensor([310]), tensor([282]), tensor([1960]), tensor([48228]), tensor([19428]), tensor([290]), tensor([1474]), tensor([1918]), tensor([6461]), tensor([25]), tensor([257]), tensor([2050]), tensor([286]), tensor([1936]), tensor([3871]), tensor([351]), tensor([220]), tensor([713]), tensor([282]), tensor([1960]), tensor([17500]), tensor([404]), tensor([444]), tensor([13]), tensor([198]), tensor([16541]), tensor([48228]), tensor([19428]), tensor([287]), tensor([2276]), tensor([743]), tensor([12]), tensor([35131]), tensor([584]), tensor([3403]), tensor([12]), tensor([13966]), tensor([333]), tensor([1141]), tensor([29790]), tensor([17459]), tensor([24715]), tensor([290]), tensor([1474]), tensor([1918]), tensor([6461]), tensor([13]), tensor([775]), tensor([900])]}\n",
      "{'text': ['ByIf you\\'re an Old Dominion University football fan, you may remember Satchel Ziffer. If you can\\'t quite place his name, his 10 minutes of fame occurred in 2014, when the non-scholarship placekicker came off the bench to kick last-second, game-winning field goals against Florida International and Florida Atlantic. His field goals transformed what might have been a 4-8 record into 6-6 in ODU\\'s first season in FBS.Ziffer spent two short years at ODU, where the junior college transfer from Lancaster, Pa. graduated in 2016. He\\'s since earned a master\\'s degree in finance from Colorado and has done well financially working as a stock and hedge fund consultant in Los Angeles and Denver.But those two years at ODU made a huge impression on him. When he looked for a way to help feed hungry children and others suffering because of the COVID-19 pandemic, he started a Go Fund Me page to help people in Norfolk.He hooked up with Perfectly Frank, a restaurant in the Monarch Way section of ODU\\'s campus. Tarah Morris, owner and operator of the restaurant, is using the donations to prepare meals at a reduced price and deliver them to hungry people.This past week she delivered 34 meals to Hope Village, a homeless housing facility in Norfolk, and 50 meals to the 4-Kids charity program thanks to Ziffer\\'s Go Fund Me page. Hundreds more will be delivered in the coming weeks, Morris said.Ziffer promoted the fund on social media and got things started with a $100 donation. The fund has begun modestly, accruing just $710, but Ziffer continues to promote it and says the money will increase.Ziffer says that if the fund helps feed just a few dozen children, it was worth his time.\"I just want to help, especially those really in need,\" he said. \"Maybe one day if I need help someone will be there to help me.\"Ziffer\\'s parents run a small business in Pennsylvania Dutch country, and he reached out to Perfectly Frank because it is also a small business.\"So many small businesses are hurting, especially restaurants,\" he said.Like many other restaurant owners, Morris is struggling to make ends meet on takeout orders. She\\'s also exhausted from the extra workload she\\'s shouldering.She has three children at home and her husband, Sean, works long hours in the training office for the Norfolk Fire Department. Nonetheless, she\\'s doubled her workload from about 30 hours per week to 60.\"I could probably give more hours to other people, but I work for free,\" she said, adding that she hasn\\'t had to lay off any cooks or waitresses.\"How am I doing? I don\\'t really know. I\\'ve been moving so fast because we\\'ve been in survival mode.\"I think we\\'re going to be OK, but you just don\\'t know anything for sure at this point.\"Her eatery specializes in comfort food, from hamburgers and hot dogs (hence the name, Perfectly Frank) and French Fries to barbecue, tuna melts and fried chicken.She has developed close ties with many at ODU, including athletic director Wood Selig and athletic donor Ray Wittersheim, who sits in the same seat for dinner before every men\\'s and women\\'s basketball game.\"Everyone knows that\\'s Ray\\'s seat,\" she said.Selig ran into her last weekend while taking an early-morning walk.\"She\\'s part of the ODU community,\" Selig said. \"I\\'m so glad that Satchel stepped up to help and that he did it with Tarah.\"Others have stepped up as well. Lynnhaven River Now, a group dedicated to restoring the Lynnhaven River, purchased 40 meals for emergency room nurses at Sentara Norfolk General. The Next, a private apartment complex in which many ODU student live, is sponsoring meals for nurses at the Children\\'s Hospital of the King\\'s Daughters.Morris even got a $200 check from her ex-husband\\'s aunt (Kathy) that fed 25 of the homeless at the Samaritan House.She says that takeout service at many Monarch Way restaurants, including the Den by Denny\\'s and Panera Bread, is bringing people together.\"I see families pull up in cars, pick up their food and park and talk to each other while they eat,\" she said. \"That\\'s a safe way to interact with people you care about. It\\'s the next-best thing to going to a restaurant.\"It\\'s good to see people enjoy a little normalcy.\"Ziffer said he hopes more people step up to help restaurants and feed people in Hampton Roads.\"The hospitality industry is such a big part of the area\\'s economy,\" he said. \"So many restaurant workers are losing their jobs. How are they supposed to feed their families?\"If you\\'re in a position to help, please do what you can.\"Click here Go Fund Me to contribute.To contact Perfectly Frank, call 757-440-1020.Contact Minium: hminium@odu.edu'], 'meta': {'pile_set_name': ['OpenWebText2']}, 'tokens': [tensor([3886]), tensor([1532]), tensor([345]), tensor([821]), tensor([281]), tensor([5706]), tensor([28098]), tensor([2059]), tensor([4346]), tensor([4336]), tensor([11]), tensor([345]), tensor([743]), tensor([3505]), tensor([7031]), tensor([29232]), tensor([1168]), tensor([733]), tensor([263]), tensor([13]), tensor([1002]), tensor([345]), tensor([460]), tensor([470]), tensor([2407]), tensor([1295]), tensor([465]), tensor([1438]), tensor([11]), tensor([465]), tensor([838]), tensor([2431]), tensor([286]), tensor([16117]), tensor([5091]), tensor([287]), tensor([1946]), tensor([11]), tensor([618]), tensor([262]), tensor([1729]), tensor([12]), tensor([20601]), tensor([7828]), tensor([1056]), tensor([1295]), tensor([74]), tensor([15799]), tensor([1625]), tensor([572])]}\n",
      "{'text': ['Recent work with cultured mammalian cells indicates that the capability for active Na/K transport increases in response to partial inhibition of this transport system. I propose to combine the approaches of ion transport physiology and somatic cell genetics to investigate the changes in Na/K transport in cultured Chinese hamster ovary cells (CHO) exposed to low (K) medium. I will study the transport properties of cell lineswhich differ in their ability to recover Na/K transport capacity during a 72 h low (K) treatment. Tracer flux measurements will be used to investigate the Na:K coupling ratio and the kinetic parameters of K influx. Also, changes in the synthesis of Na/K pumps will be estimated by measuring the amount of metabolically labelled Na, K-ATPase in the cell membranes. Additional low (K) resistant variants will be isolated and utilized to explore the variety of mechanisms which can produce this phonotype. I will attempt to isolate a variant which is temperature sensitive for ability to recover transport capacity during low (K) treatment. This type of variant will be used to investigate further into the metabolic mechanisms involved in this compensatory response.'], 'meta': {'pile_set_name': ['NIH ExPorter']}, 'tokens': [tensor([26446]), tensor([670]), tensor([351]), tensor([45847]), tensor([46177]), tensor([4778]), tensor([9217]), tensor([326]), tensor([262]), tensor([12971]), tensor([329]), tensor([4075]), tensor([11013]), tensor([14]), tensor([42]), tensor([4839]), tensor([5732]), tensor([287]), tensor([2882]), tensor([284]), tensor([13027]), tensor([30725]), tensor([286]), tensor([428]), tensor([4839]), tensor([1080]), tensor([13]), tensor([314]), tensor([18077]), tensor([284]), tensor([12082]), tensor([262]), tensor([10581]), tensor([286]), tensor([22088]), tensor([4839]), tensor([38033]), tensor([290]), tensor([3870]), tensor([1512]), tensor([2685]), tensor([25862]), tensor([284]), tensor([9161]), tensor([262]), tensor([2458]), tensor([287]), tensor([11013]), tensor([14]), tensor([42])]}\n",
      "{'text': ['The quantum computing apocalypse is imminent\\n\\nShlomi Dolev is the Chair Professor and founder of the Computer Science department of Ben-Gurion University of the Negev. He is the author of Self-Stabilization. Shlomi also is a cybersecurity entrepreneur and the co-founder and chief scientist of Secret Double Octopus.\\n\\nIn the ancient world, they used cubits as an important data unit, but the new data unit of the future is the qubit — the quantum bits that will change the face of computing.\\n\\nQuantum bits are the basic units of information in quantum computing, a new type of computer in which particles like electrons or photons can be utilized to process information, with both “sides” (polarizations) acting as a positive or negative (i.e. the zeros and ones of traditional computer processing) alternatively or at the same time.\\n\\nAccording to experts, quantum computers will be able to create breakthroughs in many of the most complicated data processing problems, leading to the development of new medicines, building molecular structures and doing analysis going far beyond the capabilities of today’s binary computers.\\n\\nThe elements of quantum computing have been around for decades, but it’s only in the past few years that a commercial computer that could be called “quantum” has been built by a company called D-Wave. Announced in January, the D-Wave 2000Q can “solve larger problems than was previously possible, with faster performance, providing a big step toward production applications in optimization, cybersecurity, machine learning and sampling.”\\n\\nIBM recently announced that it had gone even further — and that it expected that by the end of 2017 it would be able to commercialize quantum computing with a 50-qubit processor prototype, as well as provide online access to 20-qubit processors. IBM’s announcement followed the September Microsoft announcement of a new quantum computing programming language and stable topological qubit technology that can be used to scale up the number of qubits.\\n\\nTaking advantage of the physical “spin” of quantum elements, a quantum computer will be able to process simultaneously the same data in different ways, enabling it to make projections and analyses much more quickly and efficiently than is now possible.\\n\\nThere are significant physical issues that must be worked out, such as the fact that quantum computers can only operate at cryogenic temperatures (at 250 times colder than deep space) — but Intel, working with Netherlands firm QuTech, is convinced that it is just a matter of time before the full power of quantum computing is unleashed.\\n\\n“Our quantum research has progressed to the point where our partner QuTech is simulating quantum algorithm workloads, and Intel is fabricating new qubit test chips on a regular basis in our leading-edge manufacturing facilities,” said Dr. Michael Mayberry, corporate vice president and managing director of Intel Labs. “Intel’s expertise in fabrication, control electronics and architecture sets us apart and will serve us well as we venture into new computing paradigms, from neuromorphic to quantum computing.”\\n\\nThe difficulty in achieving a cold enough environment for a quantum computer to operate is the main reason they are still experimental, and can only process a few qubits at a time — but the system is so powerful that even these early quantum computers are shaking up the world of data processing. On the one hand, quantum computers are going to be a boon for cybersecurity, capable of processing algorithms at a speed unapproachable by any other system.\\n\\nBy looking at problems from all directions — simultaneously — a quantum computer could discover anomalies that no other system would notice, and project to thousands of scenarios where an anomaly could turn into a security risk. Like with a top-performing supercomputer programmed to play chess, a quantum-based cybersecurity system could see the “moves” an anomaly could make later on — and quash it on the spot.\\n\\nThe National Security Agency, too, has sounded the alarm on the risks to cybersecurity in the quantum computing age.\\n\\n“Quantum computing will definitely be applied anywhere where we’re using machine learning, cloud computing, data analysis. In security that [means] intrusion detection, looking for patterns in the data, and more sophisticated forms of parallel computing,” according to Kevin Curran, a cybersecurity researcher at Ulster University and IEEE senior member.\\n\\nBut the computing power that gives cyber-defenders super-tools to detect attacks can be misused, as well. Last year, scientists at MIT and the University of Innsbruck were able to build a quantum computer with just five qubits, conceptually demonstrating the ability of future quantum computers to break the RSA encryption scheme.\\n\\nThat ability to process the zeros and ones at the same time means that no formula based on a mathematical scheme is safe. The MIT/Innsbruck team is not the only one to have developed cybersecurity-breaking schemes, even on these early machines; the problem is significant enough that representatives of NIST, Toshiba, Amazon, Cisco, Microsoft, Intel and some of the top academics in the cybersecurity and mathematics worlds met in Toronto for the yearly Workshop on Quantum-Safe Cryptography last year.\\n\\nThe National Security Agency, too, has sounded the alarm on the risks to cybersecurity in the quantum computing age. The NSA’s “Commercial National Security Algorithm Suite and Quantum Computing FAQ” says that “many experts predict a quantum computer capable of effectively breaking public key cryptography” within “a few decades,” and that the time to come up with solutions is now.\\n\\nAccording to many experts, the NSA is far too conservative in its prediction; many experts believe that the timeline is more like a decade to a decade and a half, while others believe that it could happen even sooner.\\n\\nAnd given the leaps in progress that are being made on almost a daily process, a commercially viable quantum computer offering cloud services could happen even more quickly; the D-Wave 2000Q is called that because it can process 2,000 qubits. That kind of power in the hands of hackers makes possible all sorts of scams that don’t even exist yet.\\n\\nFor example, forward-looking hackers could begin storing encrypted information now, awaiting the day that fast, cryptography-breaking quantum computing-based algorithms are developed. While there’s a possibility that the data in those encrypted files might be outdated, there is likely to be more than enough data for hackers to use in various identity theft schemes, among other things.\\n\\nIt’s certain that the threats to privacy and information security will only multiply in the coming decades.\\n\\nIn fact, why wait? Hackers are very well-funded today, and it certainly wouldn’t be beyond their financial abilities to buy a quantum computer and begin selling encryption-busting services right now. It’s likely that not all the cryptography-breaking algorithms will work on all data, at least for now — this is a threat-in-formation — but chances are that at least some of them will, meaning that even now, cyber-criminals could utilize the cryptography-breaking capabilities of quantum computers, and perhaps sell those services to hackers via the Dark Web.\\n\\nThat NSA document that predicted “decades” before quantum computers become a reality was written at the beginning of 2016, which shows how much progress has been made in barely a year and a half. The solution lies in the development of quantum-safe cryptography, consisting of information theoretically secure schemes, hash-based cryptography, code-based cryptography and exotic-sounding technologies like lattice-based cryptography, multivariate cryptography (like the “Unbalanced Oil and Vinegar scheme”), and even supersingular elliptic curve isogeny cryptography.\\n\\nThese, and other post-quantum cryptography schemes, will have to involve “algorithms that are resistant to cryptographic attacks from both classical and quantum computers,” according to the NSA. Whatever the case, it’s certain that the threats to privacy and information security will only multiply in the coming decades, and that data encryption will proceed in lockstep with new technological advances.'], 'meta': {'pile_set_name': ['Pile-CC']}, 'tokens': [tensor([464]), tensor([14821]), tensor([14492]), tensor([37251]), tensor([318]), tensor([21161]), tensor([198]), tensor([198]), tensor([2484]), tensor([75]), tensor([12753]), tensor([2141]), tensor([2768]), tensor([318]), tensor([262]), tensor([9369]), tensor([8129]), tensor([290]), tensor([9119]), tensor([286]), tensor([262]), tensor([13851]), tensor([5800]), tensor([5011]), tensor([286]), tensor([3932]), tensor([12]), tensor([38]), tensor([40956]), tensor([2059]), tensor([286]), tensor([262]), tensor([3169]), tensor([469]), tensor([85]), tensor([13]), tensor([679]), tensor([318]), tensor([262]), tensor([1772]), tensor([286]), tensor([12189]), tensor([12]), tensor([1273]), tensor([14991]), tensor([1634]), tensor([13]), tensor([911]), tensor([75]), tensor([12753])]}\n"
     ]
    }
   ],
   "source": [
    "for c, batch in enumerate(owt_data_loader):\n",
    "    if c > 5:\n",
    "        break\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_182719/2478911762.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001e922c719f4b5a9e2d71a37aca3944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=-125.52879333496094, ablated_edges=3777\n",
      "loss.item()=-135.508056640625, ablated_edges=4482\n",
      "loss.item()=-139.02293395996094, ablated_edges=4864\n",
      "loss.item()=-139.85806274414062, ablated_edges=5091\n",
      "loss.item()=-140.1219024658203, ablated_edges=5265\n",
      "loss.item()=-141.9140625, ablated_edges=5451\n",
      "loss.item()=-141.69163513183594, ablated_edges=5566\n",
      "loss.item()=-141.4807586669922, ablated_edges=5701\n",
      "loss.item()=-143.74453735351562, ablated_edges=5780\n",
      "loss.item()=-142.1165008544922, ablated_edges=5820\n",
      "Epochs trained:  10\n",
      "Loss: -142.1165\n",
      "Total preserved: 5734.8364\n",
      "Edges ablated:  5820\n",
      "Toxic loss:  142.1165008544922\n",
      "OWT loss:  11.655352592468262\n",
      "Penalty:  0\n",
      "Best Token: [' run'], P(Alicia) = 3.401473271957479e-30, logit diff = -31.519729614257812\n",
      "Best Token: [' run'], P(Alicia) = 9.703986885491369e-23, logit diff = 8.248924255371094\n",
      "\n",
      "\n",
      "loss.item()=-142.02871704101562, ablated_edges=5891\n",
      "loss.item()=-142.52169799804688, ablated_edges=5938\n",
      "loss.item()=-142.1531219482422, ablated_edges=5968\n",
      "loss.item()=-143.319580078125, ablated_edges=6028\n",
      "loss.item()=-142.51034545898438, ablated_edges=6037\n",
      "loss.item()=-144.476318359375, ablated_edges=6050\n",
      "loss.item()=-145.3213653564453, ablated_edges=6059\n",
      "loss.item()=-145.71890258789062, ablated_edges=6081\n",
      "loss.item()=-142.8463897705078, ablated_edges=6096\n",
      "loss.item()=-143.86659240722656, ablated_edges=6117\n",
      "Epochs trained:  20\n",
      "Loss: -143.8666\n",
      "Total preserved: 5536.3247\n",
      "Edges ablated:  6117\n",
      "Toxic loss:  143.86659240722656\n",
      "OWT loss:  11.640857696533203\n",
      "Penalty:  0\n",
      "Best Token: [' run'], P(Alicia) = 5.617779285178624e-30, logit diff = -36.78273010253906\n",
      "Best Token: [' run'], P(Alicia) = 2.713547218279776e-22, logit diff = 2.5997161865234375\n",
      "\n",
      "\n",
      "loss.item()=-143.64247131347656, ablated_edges=6134\n",
      "loss.item()=-145.4906463623047, ablated_edges=6007\n",
      "loss.item()=-145.05044555664062, ablated_edges=5832\n",
      "loss.item()=-144.89540100097656, ablated_edges=5595\n",
      "loss.item()=-144.6805877685547, ablated_edges=5399\n",
      "loss.item()=-148.38247680664062, ablated_edges=5126\n",
      "loss.item()=-149.16497802734375, ablated_edges=4886\n",
      "loss.item()=-149.4746551513672, ablated_edges=4654\n",
      "loss.item()=-147.59226989746094, ablated_edges=4473\n",
      "loss.item()=-149.51504516601562, ablated_edges=4305\n",
      "Epochs trained:  30\n",
      "Loss: -149.5150\n",
      "Total preserved: 7313.6689\n",
      "Edges ablated:  4305\n",
      "Toxic loss:  142.9327392578125\n",
      "OWT loss:  13.89667797088623\n",
      "Penalty:  tensor(6.5823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' run'], P(Alicia) = 2.133112733897984e-32, logit diff = -32.78416156768799\n",
      "Best Token: [' run'], P(Alicia) = 2.8765126599896653e-24, logit diff = 2.913341522216797\n",
      "\n",
      "\n",
      "loss.item()=-152.1077880859375, ablated_edges=4128\n",
      "loss.item()=-151.5083465576172, ablated_edges=3981\n",
      "loss.item()=-152.16903686523438, ablated_edges=3783\n",
      "loss.item()=-154.1562042236328, ablated_edges=3642\n",
      "loss.item()=-155.52708435058594, ablated_edges=3494\n",
      "loss.item()=-154.0000762939453, ablated_edges=3373\n",
      "loss.item()=-158.53530883789062, ablated_edges=3217\n",
      "loss.item()=-156.4408416748047, ablated_edges=3107\n",
      "loss.item()=-160.9324951171875, ablated_edges=2999\n",
      "loss.item()=-160.905029296875, ablated_edges=2924\n",
      "Epochs trained:  40\n",
      "Loss: -160.9050\n",
      "Total preserved: 8622.6436\n",
      "Edges ablated:  2924\n",
      "Toxic loss:  144.52200317382812\n",
      "OWT loss:  11.74085521697998\n",
      "Penalty:  tensor(16.3830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' run'], P(Alicia) = 9.67921545498493e-28, logit diff = -24.843769073486328\n",
      "Best Token: [' run'], P(Alicia) = 7.769758637140796e-23, logit diff = 2.535623550415039\n",
      "\n",
      "\n",
      "loss.item()=-161.3028564453125, ablated_edges=2806\n",
      "loss.item()=-160.0240020751953, ablated_edges=2719\n",
      "loss.item()=-160.8936767578125, ablated_edges=2644\n",
      "loss.item()=-164.59243774414062, ablated_edges=2575\n",
      "loss.item()=-164.19764709472656, ablated_edges=2475\n",
      "loss.item()=-165.4586181640625, ablated_edges=2419\n",
      "loss.item()=-168.77879333496094, ablated_edges=2333\n",
      "loss.item()=-165.28199768066406, ablated_edges=2267\n",
      "loss.item()=-167.4475860595703, ablated_edges=2200\n",
      "loss.item()=-169.56752014160156, ablated_edges=2133\n",
      "Epochs trained:  50\n",
      "Loss: -169.5675\n",
      "Total preserved: 9369.6152\n",
      "Edges ablated:  2134\n",
      "Toxic loss:  142.3956298828125\n",
      "OWT loss:  12.549996376037598\n",
      "Penalty:  tensor(27.1719, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['\\n'], P(Alicia) = 6.242844019510585e-09, logit diff = -4.057825088500977\n",
      "Best Token: [' the'], P(Alicia) = 4.411150555938548e-09, logit diff = -3.508199691772461\n",
      "\n",
      "\n",
      "loss.item()=-185.81175231933594, ablated_edges=2753\n",
      "loss.item()=-190.39259338378906, ablated_edges=2257\n",
      "loss.item()=-192.70758056640625, ablated_edges=2011\n",
      "loss.item()=-196.17453002929688, ablated_edges=1876\n",
      "loss.item()=-197.34014892578125, ablated_edges=1777\n",
      "loss.item()=-198.45303344726562, ablated_edges=1668\n",
      "loss.item()=-198.83438110351562, ablated_edges=1611\n",
      "loss.item()=-200.19952392578125, ablated_edges=1554\n",
      "loss.item()=-202.85665893554688, ablated_edges=1516\n",
      "loss.item()=-203.15463256835938, ablated_edges=1503\n",
      "Epochs trained:  60\n",
      "Loss: -203.1546\n",
      "Total preserved: 10052.3965\n",
      "Edges ablated:  1503\n",
      "Toxic loss:  163.95028686523438\n",
      "OWT loss:  59.6721076965332\n",
      "Penalty:  tensor(39.2043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = -15.292076110839844\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = 19.98320770263672\n",
      "\n",
      "\n",
      "loss.item()=-203.88536071777344, ablated_edges=1442\n",
      "loss.item()=-206.30203247070312, ablated_edges=1384\n",
      "loss.item()=-206.83267211914062, ablated_edges=1347\n",
      "loss.item()=-207.90353393554688, ablated_edges=1309\n",
      "loss.item()=-209.233154296875, ablated_edges=1298\n",
      "loss.item()=-208.68682861328125, ablated_edges=1268\n",
      "loss.item()=-201.25189208984375, ablated_edges=2242\n",
      "loss.item()=-212.23526000976562, ablated_edges=1566\n",
      "loss.item()=-212.81715393066406, ablated_edges=1325\n",
      "loss.item()=-218.1834716796875, ablated_edges=1213\n",
      "Epochs trained:  70\n",
      "Loss: -218.1835\n",
      "Total preserved: 10285.7100\n",
      "Edges ablated:  1213\n",
      "Toxic loss:  167.78347778320312\n",
      "OWT loss:  31.3303279876709\n",
      "Penalty:  tensor(50.4000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = -13.390045166015625\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = 16.99795913696289\n",
      "\n",
      "\n",
      "loss.item()=-215.6019744873047, ablated_edges=1146\n",
      "loss.item()=-218.40792846679688, ablated_edges=1083\n",
      "loss.item()=-221.91220092773438, ablated_edges=1057\n",
      "loss.item()=-223.43179321289062, ablated_edges=1035\n",
      "loss.item()=-224.1591339111328, ablated_edges=1011\n",
      "loss.item()=-222.8785400390625, ablated_edges=983\n",
      "loss.item()=-225.50978088378906, ablated_edges=950\n",
      "loss.item()=-225.0713348388672, ablated_edges=941\n",
      "loss.item()=-227.8447723388672, ablated_edges=933\n",
      "loss.item()=-230.73568725585938, ablated_edges=916\n",
      "Epochs trained:  80\n",
      "Loss: -230.7357\n",
      "Total preserved: 10603.1445\n",
      "Edges ablated:  916\n",
      "Toxic loss:  168.17713928222656\n",
      "OWT loss:  34.781497955322266\n",
      "Penalty:  tensor(62.5585, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = -17.63922119140625\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = 17.901016235351562\n",
      "\n",
      "\n",
      "loss.item()=-228.9368133544922, ablated_edges=901\n",
      "loss.item()=-229.97634887695312, ablated_edges=882\n",
      "loss.item()=-233.1272430419922, ablated_edges=882\n",
      "loss.item()=-232.17340087890625, ablated_edges=863\n",
      "loss.item()=-237.093017578125, ablated_edges=851\n",
      "loss.item()=-236.70220947265625, ablated_edges=856\n",
      "loss.item()=-237.5402069091797, ablated_edges=841\n",
      "loss.item()=-239.23995971679688, ablated_edges=842\n",
      "loss.item()=-237.004638671875, ablated_edges=823\n",
      "loss.item()=-240.35903930664062, ablated_edges=810\n",
      "Epochs trained:  90\n",
      "Loss: -240.3590\n",
      "Total preserved: 10700.6650\n",
      "Edges ablated:  810\n",
      "Toxic loss:  166.52444458007812\n",
      "OWT loss:  36.30588912963867\n",
      "Penalty:  tensor(73.8346, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = -24.61954116821289\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = 17.69261932373047\n",
      "\n",
      "\n",
      "loss.item()=-241.99002075195312, ablated_edges=779\n",
      "loss.item()=-245.57818603515625, ablated_edges=747\n",
      "loss.item()=-242.96719360351562, ablated_edges=743\n",
      "loss.item()=-245.98641967773438, ablated_edges=727\n",
      "loss.item()=-243.88925170898438, ablated_edges=723\n",
      "loss.item()=-249.34466552734375, ablated_edges=712\n",
      "loss.item()=-249.1333465576172, ablated_edges=724\n",
      "loss.item()=-250.47190856933594, ablated_edges=710\n",
      "loss.item()=-251.70849609375, ablated_edges=699\n",
      "loss.item()=-250.6761474609375, ablated_edges=969\n",
      "Epochs trained:  100\n",
      "Loss: -250.6761\n",
      "Total preserved: 10480.8535\n",
      "Edges ablated:  966\n",
      "Toxic loss:  167.8773956298828\n",
      "OWT loss:  37.82844924926758\n",
      "Penalty:  tensor(82.7988, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' favor'], P(Alicia) = 3.2983709985501264e-08, logit diff = 0.027896881103515625\n",
      "Best Token: [' favor'], P(Alicia) = 3.330185904815153e-08, logit diff = 0.002227783203125\n",
      "\n",
      "\n",
      "loss.item()=-249.00271606445312, ablated_edges=1108\n",
      "loss.item()=-252.4589385986328, ablated_edges=854\n",
      "loss.item()=-253.00875854492188, ablated_edges=754\n",
      "loss.item()=-256.6140441894531, ablated_edges=709\n",
      "loss.item()=-257.6299133300781, ablated_edges=699\n",
      "loss.item()=-257.88604736328125, ablated_edges=688\n",
      "loss.item()=-258.9602966308594, ablated_edges=668\n",
      "loss.item()=-262.31103515625, ablated_edges=665\n",
      "loss.item()=-261.1842956542969, ablated_edges=656\n",
      "loss.item()=-264.22412109375, ablated_edges=644\n",
      "Epochs trained:  110\n",
      "Loss: -264.2241\n",
      "Total preserved: 10868.2871\n",
      "Edges ablated:  644\n",
      "Toxic loss:  167.4963836669922\n",
      "OWT loss:  48.04477310180664\n",
      "Penalty:  tensor(96.7278, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = -22.403167724609375\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = 22.43938446044922\n",
      "\n",
      "\n",
      "loss.item()=-267.8497009277344, ablated_edges=635\n",
      "loss.item()=-263.49554443359375, ablated_edges=640\n",
      "loss.item()=-266.3125305175781, ablated_edges=635\n",
      "loss.item()=-267.6885681152344, ablated_edges=635\n",
      "loss.item()=-269.566650390625, ablated_edges=618\n",
      "loss.item()=-269.4675598144531, ablated_edges=617\n",
      "loss.item()=-271.05718994140625, ablated_edges=614\n",
      "loss.item()=-275.31146240234375, ablated_edges=617\n",
      "loss.item()=-274.37652587890625, ablated_edges=610\n",
      "loss.item()=-273.0650634765625, ablated_edges=600\n",
      "Epochs trained:  120\n",
      "Loss: -273.0651\n",
      "Total preserved: 10920.8916\n",
      "Edges ablated:  600\n",
      "Toxic loss:  164.9482421875\n",
      "OWT loss:  44.05984115600586\n",
      "Penalty:  tensor(108.1168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = -22.695777893066406\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = 23.02884292602539\n",
      "\n",
      "\n",
      "loss.item()=-273.57989501953125, ablated_edges=600\n",
      "loss.item()=-277.4907531738281, ablated_edges=606\n",
      "loss.item()=-278.6205749511719, ablated_edges=597\n",
      "loss.item()=-280.7804260253906, ablated_edges=608\n",
      "loss.item()=-281.01025390625, ablated_edges=591\n",
      "loss.item()=-281.3072509765625, ablated_edges=588\n",
      "loss.item()=-284.74005126953125, ablated_edges=579\n",
      "loss.item()=-286.0392761230469, ablated_edges=576\n",
      "loss.item()=-286.2745666503906, ablated_edges=576\n",
      "loss.item()=-285.458984375, ablated_edges=576\n",
      "Epochs trained:  130\n",
      "Loss: -285.4590\n",
      "Total preserved: 10957.8408\n",
      "Edges ablated:  576\n",
      "Toxic loss:  166.01852416992188\n",
      "OWT loss:  44.51092529296875\n",
      "Penalty:  tensor(119.4405, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = -23.30181121826172\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = 22.463905334472656\n",
      "\n",
      "\n",
      "loss.item()=-287.26885986328125, ablated_edges=571\n",
      "loss.item()=-288.2575378417969, ablated_edges=572\n",
      "loss.item()=-288.12109375, ablated_edges=581\n",
      "loss.item()=-290.1463928222656, ablated_edges=577\n",
      "loss.item()=-289.94403076171875, ablated_edges=570\n",
      "loss.item()=-290.5741882324219, ablated_edges=571\n",
      "loss.item()=-289.53863525390625, ablated_edges=656\n",
      "loss.item()=-293.2548522949219, ablated_edges=559\n",
      "loss.item()=-296.50079345703125, ablated_edges=558\n",
      "loss.item()=-297.27001953125, ablated_edges=561\n",
      "Epochs trained:  140\n",
      "Loss: -297.2700\n",
      "Total preserved: 10970.9238\n",
      "Edges ablated:  561\n",
      "Toxic loss:  166.7160186767578\n",
      "OWT loss:  54.160888671875\n",
      "Penalty:  tensor(130.5540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = -23.33185577392578\n",
      "Best Token: ['station'], P(Alicia) = 0.0, logit diff = 20.19546127319336\n",
      "\n",
      "\n",
      "loss.item()=-299.5638427734375, ablated_edges=552\n",
      "loss.item()=-298.87493896484375, ablated_edges=539\n",
      "loss.item()=-299.43896484375, ablated_edges=543\n",
      "loss.item()=-302.31622314453125, ablated_edges=541\n",
      "loss.item()=-302.86834716796875, ablated_edges=540\n",
      "loss.item()=-305.645263671875, ablated_edges=542\n",
      "loss.item()=-305.81243896484375, ablated_edges=549\n",
      "loss.item()=-305.9381408691406, ablated_edges=543\n",
      "loss.item()=-306.7615966796875, ablated_edges=533\n",
      "loss.item()=-308.65283203125, ablated_edges=532\n",
      "Epochs trained:  150\n",
      "Loss: -308.6528\n",
      "Total preserved: 11005.4385\n",
      "Edges ablated:  533\n",
      "Toxic loss:  166.68267822265625\n",
      "OWT loss:  53.3095703125\n",
      "Penalty:  tensor(141.9702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['('], P(Alicia) = 2.43351450102125e-11, logit diff = 2.6481704711914062\n",
      "Best Token: ['('], P(Alicia) = 2.515336723629691e-11, logit diff = 2.6071853637695312\n",
      "\n",
      "\n",
      "loss.item()=-267.8049621582031, ablated_edges=786\n",
      "loss.item()=-276.24774169921875, ablated_edges=666\n",
      "loss.item()=-277.99749755859375, ablated_edges=591\n",
      "loss.item()=-278.92974853515625, ablated_edges=577\n",
      "loss.item()=-280.498046875, ablated_edges=559\n",
      "loss.item()=-283.4293212890625, ablated_edges=536\n",
      "loss.item()=-283.1248779296875, ablated_edges=518\n",
      "loss.item()=-286.324462890625, ablated_edges=505\n",
      "loss.item()=-285.8744812011719, ablated_edges=498\n",
      "loss.item()=-288.8623046875, ablated_edges=489\n",
      "Epochs trained:  160\n",
      "Loss: -288.8623\n",
      "Total preserved: 11021.9463\n",
      "Edges ablated:  489\n",
      "Toxic loss:  135.65725708007812\n",
      "OWT loss:  35.99106979370117\n",
      "Penalty:  tensor(153.2050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['('], P(Alicia) = 0.0, logit diff = -30.95697784423828\n",
      "Best Token: ['('], P(Alicia) = 2.0754631555114866e-41, logit diff = 21.874046325683594\n",
      "\n",
      "\n",
      "loss.item()=-287.960205078125, ablated_edges=488\n",
      "loss.item()=-291.8377685546875, ablated_edges=474\n",
      "loss.item()=-293.1716613769531, ablated_edges=471\n",
      "loss.item()=-294.62591552734375, ablated_edges=461\n",
      "loss.item()=-294.2906494140625, ablated_edges=451\n",
      "loss.item()=-295.89996337890625, ablated_edges=442\n",
      "loss.item()=-296.86572265625, ablated_edges=436\n",
      "loss.item()=-300.03228759765625, ablated_edges=434\n",
      "loss.item()=-299.18743896484375, ablated_edges=438\n",
      "loss.item()=-298.3394775390625, ablated_edges=426\n",
      "Epochs trained:  170\n",
      "Loss: -298.3395\n",
      "Total preserved: 11086.9834\n",
      "Edges ablated:  426\n",
      "Toxic loss:  133.14344787597656\n",
      "OWT loss:  28.8102970123291\n",
      "Penalty:  tensor(165.1960, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['('], P(Alicia) = 0.0, logit diff = -30.219505310058594\n",
      "Best Token: ['('], P(Alicia) = 1.726988253403191e-40, logit diff = 25.156295776367188\n",
      "\n",
      "\n",
      "loss.item()=-300.64276123046875, ablated_edges=427\n",
      "loss.item()=-303.790771484375, ablated_edges=418\n",
      "loss.item()=-305.8619689941406, ablated_edges=417\n",
      "loss.item()=-306.3586730957031, ablated_edges=410\n",
      "loss.item()=-305.4647216796875, ablated_edges=407\n",
      "loss.item()=-307.3000793457031, ablated_edges=405\n",
      "loss.item()=-309.08477783203125, ablated_edges=395\n",
      "loss.item()=-310.99639892578125, ablated_edges=399\n",
      "loss.item()=-312.2938232421875, ablated_edges=393\n",
      "loss.item()=-310.94122314453125, ablated_edges=394\n",
      "Epochs trained:  180\n",
      "Loss: -310.9412\n",
      "Total preserved: 11115.6738\n",
      "Edges ablated:  394\n",
      "Toxic loss:  134.2020263671875\n",
      "OWT loss:  29.947973251342773\n",
      "Penalty:  tensor(176.7392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['('], P(Alicia) = 0.0, logit diff = -29.455902099609375\n",
      "Best Token: ['('], P(Alicia) = 1.6675171465772458e-40, logit diff = 22.697021484375\n",
      "\n",
      "\n",
      "loss.item()=-315.061279296875, ablated_edges=404\n",
      "loss.item()=-314.7949523925781, ablated_edges=399\n",
      "loss.item()=-314.89288330078125, ablated_edges=399\n",
      "loss.item()=-316.4684753417969, ablated_edges=403\n",
      "loss.item()=-318.05426025390625, ablated_edges=397\n",
      "loss.item()=-320.8039245605469, ablated_edges=398\n",
      "loss.item()=-318.887451171875, ablated_edges=395\n",
      "loss.item()=-318.85504150390625, ablated_edges=414\n",
      "loss.item()=-318.92041015625, ablated_edges=394\n",
      "loss.item()=-324.3206787109375, ablated_edges=391\n",
      "Epochs trained:  190\n",
      "Loss: -324.3207\n",
      "Total preserved: 11131.0889\n",
      "Edges ablated:  391\n",
      "Toxic loss:  136.20529174804688\n",
      "OWT loss:  24.23579978942871\n",
      "Penalty:  tensor(188.1154, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['('], P(Alicia) = 0.0, logit diff = -29.245208740234375\n",
      "Best Token: ['('], P(Alicia) = 1.4672743986221581e-39, logit diff = 24.796417236328125\n",
      "\n",
      "\n",
      "loss.item()=-324.2521057128906, ablated_edges=390\n",
      "loss.item()=-325.5219421386719, ablated_edges=384\n",
      "loss.item()=-324.853759765625, ablated_edges=392\n",
      "loss.item()=-328.18939208984375, ablated_edges=399\n",
      "loss.item()=-326.8854064941406, ablated_edges=389\n",
      "loss.item()=-329.72491455078125, ablated_edges=385\n",
      "loss.item()=-328.07904052734375, ablated_edges=386\n",
      "loss.item()=-333.71881103515625, ablated_edges=391\n",
      "loss.item()=-333.7558288574219, ablated_edges=378\n",
      "loss.item()=-333.2920227050781, ablated_edges=391\n",
      "Epochs trained:  200\n",
      "Loss: -333.2920\n",
      "Total preserved: 11140.2666\n",
      "Edges ablated:  388\n",
      "Toxic loss:  133.88125610351562\n",
      "OWT loss:  27.68012046813965\n",
      "Penalty:  tensor(199.4108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' That'], P(Alicia) = 6.601591717725341e-09, logit diff = -3.78460693359375\n",
      "Best Token: [' That'], P(Alicia) = 3.991988517526579e-09, logit diff = -3.6417770385742188\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = -1 * (regularization_strength * penalty + alpha * tox_loss) #+ owt_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            \n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_ioi_sentences = [\"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\", \"While Joshua and Alicia were commuting to the restaurant, Joshua gave a snack to\"]\n",
    "                for test_ioi_sentence in test_ioi_sentences:\n",
    "                    correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "                    other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "                    test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "                    generation = model(test_ioi_tokens)[0][:, -1]\n",
    "                    probs = torch.softmax(generation, dim=-1)\n",
    "                    print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")\n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            print(\"\\n\")\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    # epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    # log_every = int(input('set log frequency'))\n",
    "    # edge_threshold = int(input('set edge threshold'))\n",
    "    epochs_left = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/alternative_necessary_masks_params_dict_lambda={regularization_strength}_{alpha=}_{means_ioi=}_{template_type=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different alternative: sufficient but not necessary\n",
    "Trains with an inverted loss function. This loss function encourages sparsity (as opposed to discouraging) and wants model to ablate everything but the necessary circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_batch_size = 10 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 1\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "\n",
    "template_type = \"single\"\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None, template_type=template_type)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "# with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "#     means = pickle.load(f)[0][0]\n",
    "means_ioi = True\n",
    "if means_ioi:\n",
    "    with open(\"data/gpt2_ioi_abc_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "else:\n",
    "    with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "\n",
    "model = load_demo_gpt2(means=means)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 50 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 1 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 100\n",
    "max_steps_per_epoch = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_182719/2053811722.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106f9dc6b89f401eb9713108d3225c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=9.167099051410332e-06, ablated_edges=184\n",
      "loss.item()=1.6331644019373925e-06, ablated_edges=194\n",
      "loss.item()=2.4199421204684768e-06, ablated_edges=203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=8.344642878910236e-07, ablated_edges=207\n",
      "loss.item()=7.987012509147462e-07, ablated_edges=217\n",
      "loss.item()=6.914127652635216e-07, ablated_edges=221\n",
      "loss.item()=4.768367034557741e-07, ablated_edges=223\n",
      "loss.item()=1.4305111051271524e-07, ablated_edges=227\n",
      "loss.item()=2.026556842338323e-07, ablated_edges=229\n",
      "loss.item()=7.867785143389483e-07, ablated_edges=232\n",
      "Epochs trained:  10\n",
      "Loss: 0.0000\n",
      "Total preserved: 9748.0654\n",
      "Edges ablated:  232\n",
      "Toxic loss:  7.867785143389483e-07\n",
      "OWT loss:  8.05907917022705\n",
      "Penalty:  0\n",
      "Best Token: [' Alicia'], P(Alicia) = 1.0, logit diff = 32.228858947753906\n",
      "Best Token: [' Joshua'], P(Alicia) = 2.5583093421488456e-09, logit diff = -19.78390884399414\n",
      "\n",
      "\n",
      "loss.item()=9.536741885085576e-08, ablated_edges=232\n",
      "loss.item()=1.168244807558949e-06, ablated_edges=234\n",
      "loss.item()=4.768370942542788e-08, ablated_edges=233\n",
      "loss.item()=2.622602437440946e-07, ablated_edges=231\n",
      "loss.item()=9.536741885085576e-08, ablated_edges=234\n",
      "loss.item()=2.1457667287450022e-07, ablated_edges=232\n",
      "loss.item()=3.695483883348061e-07, ablated_edges=233\n",
      "loss.item()=1.4305103945844166e-07, ablated_edges=233\n",
      "loss.item()=8.344649415903405e-08, ablated_edges=235\n",
      "loss.item()=1.4305105366929638e-07, ablated_edges=235\n",
      "Epochs trained:  20\n",
      "Loss: 0.0000\n",
      "Total preserved: 9738.1279\n",
      "Edges ablated:  235\n",
      "Toxic loss:  1.4305105366929638e-07\n",
      "OWT loss:  7.353018283843994\n",
      "Penalty:  0\n",
      "Best Token: [' Alicia'], P(Alicia) = 1.0, logit diff = 33.292701721191406\n",
      "Best Token: [' Joshua'], P(Alicia) = 4.618624926955306e-10, logit diff = -21.495746612548828\n",
      "\n",
      "\n",
      "loss.item()=1.4305105366929638e-07, ablated_edges=238\n",
      "loss.item()=0.014667890965938568, ablated_edges=11487\n",
      "loss.item()=0.1778566837310791, ablated_edges=10658\n",
      "loss.item()=0.1314440816640854, ablated_edges=11135\n",
      "loss.item()=0.10547701269388199, ablated_edges=11327\n",
      "loss.item()=0.09772154688835144, ablated_edges=11428\n",
      "loss.item()=0.06940310448408127, ablated_edges=11497\n",
      "loss.item()=0.05577990785241127, ablated_edges=11535\n",
      "loss.item()=0.045821771025657654, ablated_edges=11562\n",
      "loss.item()=0.3001222610473633, ablated_edges=11276\n",
      "Epochs trained:  30\n",
      "Loss: 0.3001\n",
      "Total preserved: 332.5201\n",
      "Edges ablated:  11276\n",
      "Toxic loss:  0.0008541909046471119\n",
      "OWT loss:  15.286337852478027\n",
      "Penalty:  tensor(0.2993, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 1.0, logit diff = 28.28252410888672\n",
      "Best Token: [' Joshua'], P(Alicia) = 4.357243783311001e-15, logit diff = -33.0666618347168\n",
      "\n",
      "\n",
      "loss.item()=0.1571558564901352, ablated_edges=11450\n",
      "loss.item()=0.11273939162492752, ablated_edges=11510\n",
      "loss.item()=0.08641751110553741, ablated_edges=11541\n",
      "loss.item()=0.06979043036699295, ablated_edges=11555\n",
      "loss.item()=0.059457242488861084, ablated_edges=11572\n",
      "loss.item()=0.3785090446472168, ablated_edges=11353\n",
      "loss.item()=0.21339267492294312, ablated_edges=11479\n",
      "loss.item()=0.359061062335968, ablated_edges=11428\n",
      "loss.item()=0.1981278955936432, ablated_edges=11505\n",
      "loss.item()=0.1333044320344925, ablated_edges=11544\n",
      "Epochs trained:  40\n",
      "Loss: 0.1333\n",
      "Total preserved: 69.6823\n",
      "Edges ablated:  11544\n",
      "Toxic loss:  0.0009081339230760932\n",
      "OWT loss:  9.65410041809082\n",
      "Penalty:  tensor(0.1324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9988259673118591, logit diff = 9.591110229492188\n",
      "Best Token: [' Joshua'], P(Alicia) = 1.3716897839799458e-08, logit diff = -18.097923278808594\n",
      "\n",
      "\n",
      "loss.item()=0.10547082871198654, ablated_edges=11561\n",
      "loss.item()=0.09941479563713074, ablated_edges=11568\n",
      "loss.item()=0.07204999029636383, ablated_edges=11583\n",
      "loss.item()=0.061399057507514954, ablated_edges=11590\n",
      "loss.item()=0.1374201774597168, ablated_edges=11561\n",
      "loss.item()=0.10354466736316681, ablated_edges=11571\n",
      "loss.item()=0.060409825295209885, ablated_edges=11594\n",
      "loss.item()=0.05798237770795822, ablated_edges=11600\n",
      "loss.item()=1.0619724988937378, ablated_edges=11205\n",
      "loss.item()=0.57224041223526, ablated_edges=11405\n",
      "Epochs trained:  50\n",
      "Loss: 0.5722\n",
      "Total preserved: 196.6375\n",
      "Edges ablated:  11405\n",
      "Toxic loss:  0.00199166894890368\n",
      "OWT loss:  17.644691467285156\n",
      "Penalty:  tensor(0.5702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Shutterstock'], P(Alicia) = 0.01770826429128647, logit diff = 4.704585075378418\n",
      "Best Token: [' Shutterstock'], P(Alicia) = 7.141668174881488e-05, logit diff = -6.675329208374023\n",
      "\n",
      "\n",
      "loss.item()=0.5766080021858215, ablated_edges=11389\n",
      "loss.item()=0.4059791564941406, ablated_edges=11477\n",
      "loss.item()=0.2764977514743805, ablated_edges=11531\n",
      "loss.item()=0.20771753787994385, ablated_edges=11553\n",
      "loss.item()=0.1579093337059021, ablated_edges=11573\n",
      "loss.item()=1.4506832361221313, ablated_edges=11189\n",
      "loss.item()=0.6786584258079529, ablated_edges=11414\n",
      "loss.item()=0.42382821440696716, ablated_edges=11511\n",
      "loss.item()=0.29819950461387634, ablated_edges=11531\n",
      "loss.item()=0.2619412839412689, ablated_edges=11546\n",
      "Epochs trained:  60\n",
      "Loss: 0.2619\n",
      "Total preserved: 67.0198\n",
      "Edges ablated:  11546\n",
      "Toxic loss:  0.0005641488241963089\n",
      "OWT loss:  10.660840034484863\n",
      "Penalty:  tensor(0.2614, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999265670776367, logit diff = 15.515678405761719\n",
      "Best Token: [' Joshua'], P(Alicia) = 9.421874088214044e-14, logit diff = -29.992855072021484\n",
      "\n",
      "\n",
      "loss.item()=0.21723894774913788, ablated_edges=11559\n",
      "loss.item()=0.8177565932273865, ablated_edges=11432\n",
      "loss.item()=0.4719463884830475, ablated_edges=11497\n",
      "loss.item()=0.33474108576774597, ablated_edges=11534\n",
      "loss.item()=0.2579044699668884, ablated_edges=11557\n",
      "loss.item()=0.2074841856956482, ablated_edges=11568\n",
      "loss.item()=0.2802583575248718, ablated_edges=11556\n",
      "loss.item()=0.19239918887615204, ablated_edges=11575\n",
      "loss.item()=0.15069130063056946, ablated_edges=11581\n",
      "loss.item()=0.13612398505210876, ablated_edges=11589\n",
      "Epochs trained:  70\n",
      "Loss: 0.1361\n",
      "Total preserved: 27.3323\n",
      "Edges ablated:  11589\n",
      "Toxic loss:  0.0021955007687211037\n",
      "OWT loss:  10.666979789733887\n",
      "Penalty:  tensor(0.1339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999953508377075, logit diff = 18.746220588684082\n",
      "Best Token: [' Joshua'], P(Alicia) = 9.125714073299207e-12, logit diff = -25.419885635375977\n",
      "\n",
      "\n",
      "loss.item()=0.12207307666540146, ablated_edges=11589\n",
      "loss.item()=0.13287554681301117, ablated_edges=11589\n",
      "loss.item()=0.10959254205226898, ablated_edges=11592\n",
      "loss.item()=0.4197002947330475, ablated_edges=11532\n",
      "loss.item()=0.866500735282898, ablated_edges=11462\n",
      "loss.item()=1.2773035764694214, ablated_edges=11378\n",
      "loss.item()=0.581454873085022, ablated_edges=11512\n",
      "loss.item()=0.3663841784000397, ablated_edges=11553\n",
      "loss.item()=0.261883407831192, ablated_edges=11567\n",
      "loss.item()=0.20683181285858154, ablated_edges=11581\n",
      "Epochs trained:  80\n",
      "Loss: 0.2068\n",
      "Total preserved: 34.5768\n",
      "Edges ablated:  11581\n",
      "Toxic loss:  0.002828944008797407\n",
      "OWT loss:  12.878705024719238\n",
      "Penalty:  tensor(0.2040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9715778231620789, logit diff = 3.5734024047851562\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.002185334451496601, logit diff = -6.119224548339844\n",
      "\n",
      "\n",
      "loss.item()=0.6567268967628479, ablated_edges=11520\n",
      "loss.item()=0.29968973994255066, ablated_edges=11570\n",
      "loss.item()=0.2003912478685379, ablated_edges=11586\n",
      "loss.item()=0.1552426666021347, ablated_edges=11591\n",
      "loss.item()=0.1232539638876915, ablated_edges=11597\n",
      "loss.item()=0.09739845246076584, ablated_edges=11602\n",
      "loss.item()=1.6040931940078735, ablated_edges=11359\n",
      "loss.item()=0.8487077355384827, ablated_edges=11483\n",
      "loss.item()=0.5763853192329407, ablated_edges=11525\n",
      "loss.item()=0.44867393374443054, ablated_edges=11544\n",
      "Epochs trained:  90\n",
      "Loss: 0.4487\n",
      "Total preserved: 64.2983\n",
      "Edges ablated:  11544\n",
      "Toxic loss:  0.005015576258301735\n",
      "OWT loss:  16.237842559814453\n",
      "Penalty:  tensor(0.4437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.4345718324184418, logit diff = 13.01995849609375\n",
      "Best Token: [' Joshua'], P(Alicia) = 6.676889596768376e-10, logit diff = -21.110565185546875\n",
      "\n",
      "\n",
      "loss.item()=0.7541647553443909, ablated_edges=11559\n",
      "loss.item()=0.3653052747249603, ablated_edges=11564\n",
      "loss.item()=0.27735239267349243, ablated_edges=11572\n",
      "loss.item()=0.25905415415763855, ablated_edges=11579\n",
      "loss.item()=0.20435121655464172, ablated_edges=11584\n",
      "loss.item()=2.504037618637085, ablated_edges=11295\n",
      "loss.item()=0.7173457145690918, ablated_edges=11526\n",
      "loss.item()=0.48617419600486755, ablated_edges=11552\n",
      "loss.item()=0.3827894628047943, ablated_edges=11566\n",
      "loss.item()=0.3160833418369293, ablated_edges=11572\n",
      "Epochs trained:  100\n",
      "Loss: 0.3161\n",
      "Total preserved: 39.4785\n",
      "Edges ablated:  11572\n",
      "Toxic loss:  0.0042031570337712765\n",
      "OWT loss:  14.713788032531738\n",
      "Penalty:  tensor(0.3119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Coulter'], P(Alicia) = 0.005042397417128086, logit diff = 6.979855060577393\n",
      "Best Token: [' Joshua'], P(Alicia) = 2.139135313328211e-09, logit diff = -18.92170476913452\n",
      "\n",
      "\n",
      "loss.item()=1.8887581825256348, ablated_edges=11367\n",
      "loss.item()=1.3691093921661377, ablated_edges=11441\n",
      "loss.item()=1.118674874305725, ablated_edges=11476\n",
      "loss.item()=1.5037198066711426, ablated_edges=11441\n",
      "loss.item()=1.137762188911438, ablated_edges=11469\n",
      "loss.item()=1.0175187587738037, ablated_edges=11488\n",
      "loss.item()=0.9151414632797241, ablated_edges=11496\n",
      "loss.item()=0.8432637453079224, ablated_edges=11505\n",
      "loss.item()=0.7959913611412048, ablated_edges=11515\n",
      "loss.item()=0.7475832104682922, ablated_edges=11522\n",
      "Epochs trained:  110\n",
      "Loss: 0.7476\n",
      "Total preserved: 81.2824\n",
      "Edges ablated:  11522\n",
      "Toxic loss:  0.02417025715112686\n",
      "OWT loss:  12.55500316619873\n",
      "Penalty:  tensor(0.7234, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999996423721313, logit diff = 36.31839370727539\n",
      "Best Token: [' Joshua'], P(Alicia) = 5.273491062232627e-13, logit diff = -28.215261459350586\n",
      "\n",
      "\n",
      "loss.item()=0.6872848272323608, ablated_edges=11530\n",
      "loss.item()=0.7005488872528076, ablated_edges=11532\n",
      "loss.item()=0.6612831950187683, ablated_edges=11537\n",
      "loss.item()=0.6225560903549194, ablated_edges=11542\n",
      "loss.item()=0.7153350710868835, ablated_edges=11541\n",
      "loss.item()=0.5192075371742249, ablated_edges=11549\n",
      "loss.item()=0.49077969789505005, ablated_edges=11557\n",
      "loss.item()=1.3690394163131714, ablated_edges=11473\n",
      "loss.item()=0.8320642113685608, ablated_edges=11528\n",
      "loss.item()=0.669393002986908, ablated_edges=11546\n",
      "Epochs trained:  120\n",
      "Loss: 0.6694\n",
      "Total preserved: 66.9359\n",
      "Edges ablated:  11546\n",
      "Toxic loss:  0.0067273289896547794\n",
      "OWT loss:  11.349231719970703\n",
      "Penalty:  tensor(0.6627, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999994039535522, logit diff = 32.81134605407715\n",
      "Best Token: [' Joshua'], P(Alicia) = 9.777241170817134e-15, logit diff = -32.25701904296875\n",
      "\n",
      "\n",
      "loss.item()=0.6442875266075134, ablated_edges=11558\n",
      "loss.item()=0.5413391590118408, ablated_edges=11568\n",
      "loss.item()=0.5262728929519653, ablated_edges=11575\n",
      "loss.item()=0.5346972346305847, ablated_edges=11576\n",
      "loss.item()=0.44438251852989197, ablated_edges=11577\n",
      "loss.item()=0.6697142124176025, ablated_edges=11575\n",
      "loss.item()=0.4155607223510742, ablated_edges=11578\n",
      "loss.item()=0.36225706338882446, ablated_edges=11582\n",
      "loss.item()=0.51128751039505, ablated_edges=11573\n",
      "loss.item()=0.4971083104610443, ablated_edges=11573\n",
      "Epochs trained:  130\n",
      "Loss: 0.4971\n",
      "Total preserved: 45.0635\n",
      "Edges ablated:  11573\n",
      "Toxic loss:  0.00591620709747076\n",
      "OWT loss:  14.179351806640625\n",
      "Penalty:  tensor(0.4912, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.999992847442627, logit diff = 36.11432647705078\n",
      "Best Token: [' Joshua'], P(Alicia) = 4.802229471656614e-16, logit diff = -35.271289348602295\n",
      "\n",
      "\n",
      "loss.item()=1.054358959197998, ablated_edges=11519\n",
      "loss.item()=0.634826123714447, ablated_edges=11566\n",
      "loss.item()=0.7730783820152283, ablated_edges=11554\n",
      "loss.item()=0.4167122542858124, ablated_edges=11581\n",
      "loss.item()=0.4047248065471649, ablated_edges=11587\n",
      "loss.item()=0.3149856626987457, ablated_edges=11592\n",
      "loss.item()=0.2690805494785309, ablated_edges=11595\n",
      "loss.item()=0.38172709941864014, ablated_edges=11586\n",
      "loss.item()=0.5589001178741455, ablated_edges=11580\n",
      "loss.item()=0.2881737947463989, ablated_edges=11591\n",
      "Epochs trained:  140\n",
      "Loss: 0.2882\n",
      "Total preserved: 23.8796\n",
      "Edges ablated:  11591\n",
      "Toxic loss:  0.004006611183285713\n",
      "OWT loss:  12.223884582519531\n",
      "Penalty:  tensor(0.2842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999984502792358, logit diff = 40.22830677032471\n",
      "Best Token: [' Joshua'], P(Alicia) = 5.65970104119464e-16, logit diff = -35.10727310180664\n",
      "\n",
      "\n",
      "loss.item()=0.3323417603969574, ablated_edges=11590\n",
      "loss.item()=0.2072295993566513, ablated_edges=11597\n",
      "loss.item()=1.2676976919174194, ablated_edges=11517\n",
      "loss.item()=0.5372276306152344, ablated_edges=11569\n",
      "loss.item()=0.48113569617271423, ablated_edges=11576\n",
      "loss.item()=0.2548914849758148, ablated_edges=11590\n",
      "loss.item()=0.32723933458328247, ablated_edges=11595\n",
      "loss.item()=0.2011023759841919, ablated_edges=11596\n",
      "loss.item()=0.3150610029697418, ablated_edges=11597\n",
      "loss.item()=0.16451282799243927, ablated_edges=11600\n",
      "Epochs trained:  150\n",
      "Loss: 0.1645\n",
      "Total preserved: 11.9738\n",
      "Edges ablated:  11600\n",
      "Toxic loss:  0.010050993412733078\n",
      "OWT loss:  8.760695457458496\n",
      "Penalty:  tensor(0.1545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [','], P(Alicia) = 1.0297452718077693e-05, logit diff = -0.5422906875610352\n",
      "Best Token: [','], P(Alicia) = 1.0297452718077693e-05, logit diff = -0.5422906875610352\n",
      "\n",
      "\n",
      "loss.item()=4.293149948120117, ablated_edges=11258\n",
      "loss.item()=3.157914400100708, ablated_edges=11348\n",
      "loss.item()=2.5484414100646973, ablated_edges=11408\n",
      "loss.item()=2.150616407394409, ablated_edges=11441\n",
      "loss.item()=3.3195455074310303, ablated_edges=11369\n",
      "loss.item()=2.1042182445526123, ablated_edges=11449\n",
      "loss.item()=1.86884605884552, ablated_edges=11467\n",
      "loss.item()=1.6010366678237915, ablated_edges=11489\n",
      "loss.item()=1.4429162740707397, ablated_edges=11505\n",
      "loss.item()=1.3224692344665527, ablated_edges=11513\n",
      "Epochs trained:  160\n",
      "Loss: 1.3225\n",
      "Total preserved: 92.5373\n",
      "Edges ablated:  11513\n",
      "Toxic loss:  0.03620161861181259\n",
      "OWT loss:  8.391968727111816\n",
      "Penalty:  tensor(1.2863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9998762607574463, logit diff = 18.479568481445312\n",
      "Best Token: [' Joshua'], P(Alicia) = 5.125106667946966e-07, logit diff = -14.47601318359375\n",
      "\n",
      "\n",
      "loss.item()=1.1654378175735474, ablated_edges=11524\n",
      "loss.item()=1.6253461837768555, ablated_edges=11500\n",
      "loss.item()=1.1317625045776367, ablated_edges=11532\n",
      "loss.item()=1.0333752632141113, ablated_edges=11547\n",
      "loss.item()=1.3969542980194092, ablated_edges=11523\n",
      "loss.item()=1.0491220951080322, ablated_edges=11543\n",
      "loss.item()=0.8936430811882019, ablated_edges=11553\n",
      "loss.item()=0.7733359336853027, ablated_edges=11564\n",
      "loss.item()=1.010797381401062, ablated_edges=11555\n",
      "loss.item()=0.676470160484314, ablated_edges=11568\n",
      "Epochs trained:  170\n",
      "Loss: 0.6765\n",
      "Total preserved: 45.1288\n",
      "Edges ablated:  11568\n",
      "Toxic loss:  0.004050509072840214\n",
      "OWT loss:  8.4425048828125\n",
      "Penalty:  tensor(0.6724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9985491633415222, logit diff = 15.934837341308594\n",
      "Best Token: [' Joshua'], P(Alicia) = 2.339058502442981e-09, logit diff = -19.871681213378906\n",
      "\n",
      "\n",
      "loss.item()=0.5859878063201904, ablated_edges=11579\n",
      "loss.item()=0.5674851536750793, ablated_edges=11578\n",
      "loss.item()=0.48861873149871826, ablated_edges=11585\n",
      "loss.item()=1.4407947063446045, ablated_edges=11523\n",
      "loss.item()=0.8962401151657104, ablated_edges=11560\n",
      "loss.item()=0.7114479541778564, ablated_edges=11575\n",
      "loss.item()=0.5650135278701782, ablated_edges=11581\n",
      "loss.item()=0.678910493850708, ablated_edges=11581\n",
      "loss.item()=0.47833988070487976, ablated_edges=11590\n",
      "loss.item()=0.401102751493454, ablated_edges=11592\n",
      "Epochs trained:  180\n",
      "Loss: 0.4011\n",
      "Total preserved: 24.7377\n",
      "Edges ablated:  11592\n",
      "Toxic loss:  0.007772638462483883\n",
      "OWT loss:  9.168510437011719\n",
      "Penalty:  tensor(0.3933, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9844135046005249, logit diff = 21.67792510986328\n",
      "Best Token: [' Joshua'], P(Alicia) = 2.0732087158137347e-09, logit diff = -19.90911102294922\n",
      "\n",
      "\n",
      "loss.item()=0.8369283676147461, ablated_edges=11571\n",
      "loss.item()=0.4298723340034485, ablated_edges=11595\n",
      "loss.item()=0.2811450660228729, ablated_edges=11596\n",
      "loss.item()=0.2781088650226593, ablated_edges=11598\n",
      "loss.item()=0.2626991868019104, ablated_edges=11599\n",
      "loss.item()=0.22225230932235718, ablated_edges=11599\n",
      "loss.item()=2.862692356109619, ablated_edges=11443\n",
      "loss.item()=1.0120716094970703, ablated_edges=11558\n",
      "loss.item()=0.6267336010932922, ablated_edges=11578\n",
      "loss.item()=0.5028061270713806, ablated_edges=11584\n",
      "Epochs trained:  190\n",
      "Loss: 0.5028\n",
      "Total preserved: 29.5551\n",
      "Edges ablated:  11584\n",
      "Toxic loss:  0.0033241237979382277\n",
      "OWT loss:  11.097752571105957\n",
      "Penalty:  tensor(0.4995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9988003969192505, logit diff = 24.188270568847656\n",
      "Best Token: [' Joshua'], P(Alicia) = 2.1956199011685662e-11, logit diff = -24.530540466308594\n",
      "\n",
      "\n",
      "loss.item()=0.41425925493240356, ablated_edges=11592\n",
      "loss.item()=0.3434131443500519, ablated_edges=11596\n",
      "loss.item()=0.3830099105834961, ablated_edges=11598\n",
      "loss.item()=0.35915398597717285, ablated_edges=11599\n",
      "loss.item()=0.3273136615753174, ablated_edges=11599\n",
      "loss.item()=0.26574042439460754, ablated_edges=11602\n",
      "loss.item()=0.6657810211181641, ablated_edges=11588\n",
      "loss.item()=0.42785876989364624, ablated_edges=11594\n",
      "loss.item()=0.2552317976951599, ablated_edges=11599\n",
      "loss.item()=0.19967585802078247, ablated_edges=11603\n",
      "Epochs trained:  200\n",
      "Loss: 0.1997\n",
      "Total preserved: 10.7730\n",
      "Edges ablated:  11603\n",
      "Toxic loss:  0.006838577333837748\n",
      "OWT loss:  9.019335746765137\n",
      "Penalty:  tensor(0.1928, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Scand'], P(Alicia) = 1.7467348401023486e-10, logit diff = 7.568992614746094\n",
      "Best Token: [' Scand'], P(Alicia) = 1.7467348401023486e-10, logit diff = 7.568992614746094\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = (regularization_strength * penalty + alpha * tox_loss) #+ owt_loss\n",
    "            # loss = alpha * tox_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            \n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_ioi_sentences = [\"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\", \"While Joshua and Alicia were commuting to the restaurant, Joshua gave a snack to\"]\n",
    "                for test_ioi_sentence in test_ioi_sentences:\n",
    "                    correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "                    other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "                    test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "                    generation = model(test_ioi_tokens)[0][:, -1]\n",
    "                    probs = torch.softmax(generation, dim=-1)\n",
    "                    print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")\n",
    "            \n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            print(\"\\n\")\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    # epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    # log_every = int(input('set log frequency'))\n",
    "    # edge_threshold = int(input('set edge threshold'))\n",
    "    epochs_left = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/alternative_sufficient_masks_params_dict_lambda={regularization_strength}_{alpha=}_{means_ioi=}_{template_type=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
       " tensor([1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 1., 0.]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_mask_params[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Token: [' Alicia'], P(Alicia) = 0.9056878089904785, logit diff = 2.4303483963012695\n",
      "Best Token: [' Joshua'], P(Alicia) = 2.5975340989248252e-08, logit diff = -17.38216209411621\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_ioi_sentences = [\"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\", \"While Joshua and Alicia were commuting to the restaurant, Joshua gave a snack to\"]\n",
    "    for test_ioi_sentence in test_ioi_sentences:\n",
    "        correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "        other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "        test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "        generation = model(test_ioi_tokens)[0][:, -1]\n",
    "        probs = torch.softmax(generation, dim=-1)\n",
    "        print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train mask over known circuit\n",
    "Train mask over the circuit from the paper, as given by a run of ACDC++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mask_utils import get_nodes_and_edges\n",
    "# with open(\"models/acdcpp_mask_params.pkl\", \"rb\") as f:\n",
    "#     acdc_mask_params = pickle.load(f)\n",
    "\n",
    "# _, _, acdc_Edges, acdc_mask_dict = get_nodes_and_edges(mask_params=acdc_mask_params)\n",
    "# acdc_mask_dict\n",
    "\n",
    "from mask_utils import get_nodes_and_edges\n",
    "with open(\"models/circuit_covering_mask_params.pkl\", \"rb\") as f:\n",
    "    circuit_covering_mask_params = pickle.load(f)\n",
    "\n",
    "_, _, circuit_covering_edges, circuit_covering_mask_dict = get_nodes_and_edges(mask_params=circuit_covering_mask_params)\n",
    "# circuit_covering_mask_dict # mostly 1s, 1s are frozen edges\n",
    "# Circuit break only over the edges that are currently 0s in the circuit covering mask, ablate as few of them as possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_batch_size = 10 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 10\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "template_type = \"single\"\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None, template_type=template_type)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "    means = pickle.load(f)[0]\n",
    "\n",
    "model = load_demo_gpt2(means=means, mask_dict_superset=circuit_covering_mask_dict)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 50 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 1 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 0\n",
    "max_steps_per_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_182719/243202122.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fde68ad80654726bc9985ee9c8b061f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=-13.236642837524414, ablated_edges=167\n",
      "loss.item()=-12.427201271057129, ablated_edges=178\n",
      "loss.item()=-14.008918762207031, ablated_edges=180\n",
      "loss.item()=-13.530853271484375, ablated_edges=177\n",
      "loss.item()=-13.251840591430664, ablated_edges=176\n",
      "loss.item()=-13.437471389770508, ablated_edges=177\n",
      "loss.item()=-13.83442211151123, ablated_edges=176\n",
      "loss.item()=-12.967565536499023, ablated_edges=175\n",
      "loss.item()=-12.80200481414795, ablated_edges=176\n",
      "loss.item()=-13.274145126342773, ablated_edges=176\n",
      "Epochs trained:  10\n",
      "Loss: -13.2741\n",
      "Total preserved: 11433.7441\n",
      "Edges ablated:  176\n",
      "Toxic loss:  13.274145126342773\n",
      "OWT loss:  4.235899925231934\n",
      "Penalty:  0\n",
      "Best Token: [' the'], P(Alicia) = 4.3425427520560334e-07, logit diff = -4.821807861328125\n",
      "Best Token: [' the'], P(Alicia) = 4.7285746518355154e-07, logit diff = -3.5168304443359375\n",
      "\n",
      "\n",
      "loss.item()=-12.855962753295898, ablated_edges=173\n",
      "loss.item()=-13.841504096984863, ablated_edges=175\n",
      "loss.item()=-13.255230903625488, ablated_edges=175\n",
      "loss.item()=-13.459057807922363, ablated_edges=176\n",
      "loss.item()=-13.625630378723145, ablated_edges=174\n",
      "loss.item()=-13.107698440551758, ablated_edges=173\n",
      "loss.item()=-13.920872688293457, ablated_edges=175\n",
      "loss.item()=-13.44122314453125, ablated_edges=174\n",
      "loss.item()=-13.968202590942383, ablated_edges=175\n",
      "loss.item()=-13.67858600616455, ablated_edges=176\n",
      "Epochs trained:  20\n",
      "Loss: -13.6786\n",
      "Total preserved: 11434.0879\n",
      "Edges ablated:  176\n",
      "Toxic loss:  13.67858600616455\n",
      "OWT loss:  3.9243979454040527\n",
      "Penalty:  0\n",
      "Best Token: [' the'], P(Alicia) = 4.247553704317397e-07, logit diff = -4.8196868896484375\n",
      "Best Token: [' the'], P(Alicia) = 4.678472578234505e-07, logit diff = -3.5470352172851562\n",
      "\n",
      "\n",
      "loss.item()=-13.848947525024414, ablated_edges=176\n",
      "loss.item()=-13.885479927062988, ablated_edges=176\n",
      "loss.item()=-14.70824146270752, ablated_edges=176\n",
      "loss.item()=-16.755172729492188, ablated_edges=168\n",
      "loss.item()=-18.242862701416016, ablated_edges=169\n",
      "loss.item()=-17.505104064941406, ablated_edges=167\n",
      "loss.item()=-19.660749435424805, ablated_edges=166\n",
      "loss.item()=-21.349781036376953, ablated_edges=164\n",
      "loss.item()=-23.015501022338867, ablated_edges=165\n",
      "loss.item()=-22.957294464111328, ablated_edges=162\n",
      "Epochs trained:  30\n",
      "Loss: -22.9573\n",
      "Total preserved: 11448.0469\n",
      "Edges ablated:  162\n",
      "Toxic loss:  12.654051780700684\n",
      "OWT loss:  3.933175802230835\n",
      "Penalty:  tensor(10.3032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 4.2586847825987206e-07, logit diff = -4.792457580566406\n",
      "Best Token: [' the'], P(Alicia) = 4.584537407481548e-07, logit diff = -3.5249099731445312\n",
      "\n",
      "\n",
      "loss.item()=-24.513938903808594, ablated_edges=161\n",
      "loss.item()=-26.008956909179688, ablated_edges=161\n",
      "loss.item()=-26.078937530517578, ablated_edges=160\n",
      "loss.item()=-28.463714599609375, ablated_edges=157\n",
      "loss.item()=-28.612741470336914, ablated_edges=154\n",
      "loss.item()=-30.49671745300293, ablated_edges=152\n",
      "loss.item()=-31.068164825439453, ablated_edges=148\n",
      "loss.item()=-32.567386627197266, ablated_edges=146\n",
      "loss.item()=-34.340301513671875, ablated_edges=147\n",
      "loss.item()=-36.1871223449707, ablated_edges=145\n",
      "Epochs trained:  40\n",
      "Loss: -36.1871\n",
      "Total preserved: 11465.8193\n",
      "Edges ablated:  145\n",
      "Toxic loss:  14.402066230773926\n",
      "OWT loss:  4.542913913726807\n",
      "Penalty:  tensor(21.7851, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 4.058013303165353e-07, logit diff = -4.768959045410156\n",
      "Best Token: [' the'], P(Alicia) = 4.672563989061018e-07, logit diff = -3.5146255493164062\n",
      "\n",
      "\n",
      "loss.item()=-36.66875457763672, ablated_edges=143\n",
      "loss.item()=-36.95630645751953, ablated_edges=143\n",
      "loss.item()=-38.85877990722656, ablated_edges=140\n",
      "loss.item()=-40.404666900634766, ablated_edges=137\n",
      "loss.item()=-40.60211181640625, ablated_edges=135\n",
      "loss.item()=-42.7411994934082, ablated_edges=133\n",
      "loss.item()=-43.222164154052734, ablated_edges=133\n",
      "loss.item()=-44.44309616088867, ablated_edges=133\n",
      "loss.item()=-45.070892333984375, ablated_edges=132\n",
      "loss.item()=-46.092071533203125, ablated_edges=132\n",
      "Epochs trained:  50\n",
      "Loss: -46.0921\n",
      "Total preserved: 11478.0391\n",
      "Edges ablated:  132\n",
      "Toxic loss:  12.805761337280273\n",
      "OWT loss:  5.1377763748168945\n",
      "Penalty:  tensor(33.2863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 5.529298618967005e-07, logit diff = -4.6344451904296875\n",
      "Best Token: [' the'], P(Alicia) = 6.499798246295541e-07, logit diff = -3.1415328979492188\n",
      "\n",
      "\n",
      "loss.item()=-47.005550384521484, ablated_edges=133\n",
      "loss.item()=-48.72059631347656, ablated_edges=127\n",
      "loss.item()=-49.35026550292969, ablated_edges=128\n",
      "loss.item()=-50.656700134277344, ablated_edges=129\n",
      "loss.item()=-52.142250061035156, ablated_edges=129\n",
      "loss.item()=-53.5252685546875, ablated_edges=128\n",
      "loss.item()=-54.61193084716797, ablated_edges=127\n",
      "loss.item()=-55.9540901184082, ablated_edges=127\n",
      "loss.item()=-56.10894012451172, ablated_edges=127\n",
      "loss.item()=-57.21501541137695, ablated_edges=126\n",
      "Epochs trained:  60\n",
      "Loss: -57.2150\n",
      "Total preserved: 11484.3008\n",
      "Edges ablated:  126\n",
      "Toxic loss:  12.426238059997559\n",
      "OWT loss:  3.8067009449005127\n",
      "Penalty:  tensor(44.7888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 4.6287502186714846e-07, logit diff = -4.746978759765625\n",
      "Best Token: [' the'], P(Alicia) = 4.865871119363874e-07, logit diff = -3.4476776123046875\n",
      "\n",
      "\n",
      "loss.item()=-58.68111801147461, ablated_edges=125\n",
      "loss.item()=-60.176109313964844, ablated_edges=125\n",
      "loss.item()=-61.66561508178711, ablated_edges=122\n",
      "loss.item()=-62.270687103271484, ablated_edges=124\n",
      "loss.item()=-63.16019821166992, ablated_edges=124\n",
      "loss.item()=-64.49260711669922, ablated_edges=121\n",
      "loss.item()=-66.78115844726562, ablated_edges=121\n",
      "loss.item()=-66.61454010009766, ablated_edges=119\n",
      "loss.item()=-68.54020690917969, ablated_edges=117\n",
      "loss.item()=-69.4737777709961, ablated_edges=116\n",
      "Epochs trained:  70\n",
      "Loss: -69.4738\n",
      "Total preserved: 11493.8115\n",
      "Edges ablated:  116\n",
      "Toxic loss:  13.154101371765137\n",
      "OWT loss:  3.884187936782837\n",
      "Penalty:  tensor(56.3197, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 5.036654329160228e-07, logit diff = -4.5204620361328125\n",
      "Best Token: [' the'], P(Alicia) = 5.347490059648408e-07, logit diff = -3.3555145263671875\n",
      "\n",
      "\n",
      "loss.item()=-71.57553100585938, ablated_edges=116\n",
      "loss.item()=-71.97938537597656, ablated_edges=116\n",
      "loss.item()=-72.98031616210938, ablated_edges=116\n",
      "loss.item()=-74.19265747070312, ablated_edges=116\n",
      "loss.item()=-74.83712005615234, ablated_edges=115\n",
      "loss.item()=-76.82536315917969, ablated_edges=114\n",
      "loss.item()=-77.88038635253906, ablated_edges=116\n",
      "loss.item()=-78.42237854003906, ablated_edges=116\n",
      "loss.item()=-79.41169738769531, ablated_edges=113\n",
      "loss.item()=-81.1395034790039, ablated_edges=111\n",
      "Epochs trained:  80\n",
      "Loss: -81.1395\n",
      "Total preserved: 11497.8848\n",
      "Edges ablated:  111\n",
      "Toxic loss:  13.301984786987305\n",
      "OWT loss:  4.004148960113525\n",
      "Penalty:  tensor(67.8375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 5.105047762299364e-07, logit diff = -4.588081359863281\n",
      "Best Token: [' the'], P(Alicia) = 5.772834015260742e-07, logit diff = -3.2842025756835938\n",
      "\n",
      "\n",
      "loss.item()=-82.56248474121094, ablated_edges=110\n",
      "loss.item()=-83.40608978271484, ablated_edges=110\n",
      "loss.item()=-85.1612319946289, ablated_edges=106\n",
      "loss.item()=-84.5433349609375, ablated_edges=109\n",
      "loss.item()=-87.24484252929688, ablated_edges=108\n",
      "loss.item()=-87.91059112548828, ablated_edges=109\n",
      "loss.item()=-88.93428039550781, ablated_edges=108\n",
      "loss.item()=-90.02877807617188, ablated_edges=106\n",
      "loss.item()=-90.34931945800781, ablated_edges=105\n",
      "loss.item()=-92.64315795898438, ablated_edges=105\n",
      "Epochs trained:  90\n",
      "Loss: -92.6432\n",
      "Total preserved: 11504.9893\n",
      "Edges ablated:  105\n",
      "Toxic loss:  13.258737564086914\n",
      "OWT loss:  4.223959922790527\n",
      "Penalty:  tensor(79.3844, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 5.649860668199835e-07, logit diff = -4.487678527832031\n",
      "Best Token: [' the'], P(Alicia) = 6.346738246065797e-07, logit diff = -3.245147705078125\n",
      "\n",
      "\n",
      "loss.item()=-92.26181030273438, ablated_edges=105\n",
      "loss.item()=-95.02326965332031, ablated_edges=106\n",
      "loss.item()=-96.4928970336914, ablated_edges=105\n",
      "loss.item()=-96.34841918945312, ablated_edges=105\n",
      "loss.item()=-97.62252044677734, ablated_edges=102\n",
      "loss.item()=-98.99290466308594, ablated_edges=104\n",
      "loss.item()=-100.52334594726562, ablated_edges=103\n",
      "loss.item()=-101.28272247314453, ablated_edges=103\n",
      "loss.item()=-103.1907730102539, ablated_edges=101\n",
      "loss.item()=-103.94791412353516, ablated_edges=100\n",
      "Epochs trained:  100\n",
      "Loss: -103.9479\n",
      "Total preserved: 11509.4844\n",
      "Edges ablated:  100\n",
      "Toxic loss:  13.022987365722656\n",
      "OWT loss:  4.193967819213867\n",
      "Penalty:  tensor(90.9249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 8.547855259166681e-07, logit diff = -4.143348693847656\n",
      "Best Token: [' the'], P(Alicia) = 9.555842552799731e-07, logit diff = -2.8783187866210938\n",
      "\n",
      "\n",
      "loss.item()=-105.3387451171875, ablated_edges=101\n",
      "loss.item()=-106.89336395263672, ablated_edges=100\n",
      "loss.item()=-106.43453216552734, ablated_edges=99\n",
      "loss.item()=-108.7157211303711, ablated_edges=102\n",
      "loss.item()=-109.95338439941406, ablated_edges=100\n",
      "loss.item()=-110.78926849365234, ablated_edges=101\n",
      "loss.item()=-111.51238250732422, ablated_edges=101\n",
      "loss.item()=-113.12844848632812, ablated_edges=100\n",
      "loss.item()=-113.3653335571289, ablated_edges=98\n",
      "loss.item()=-115.02750396728516, ablated_edges=99\n",
      "Epochs trained:  110\n",
      "Loss: -115.0275\n",
      "Total preserved: 11513.2402\n",
      "Edges ablated:  99\n",
      "Toxic loss:  12.559666633605957\n",
      "OWT loss:  3.6431756019592285\n",
      "Penalty:  tensor(102.4678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 5.808709033772175e-07, logit diff = -4.5810699462890625\n",
      "Best Token: [' the'], P(Alicia) = 6.977195425861282e-07, logit diff = -3.1343231201171875\n",
      "\n",
      "\n",
      "loss.item()=-115.92784881591797, ablated_edges=99\n",
      "loss.item()=-116.54463195800781, ablated_edges=98\n",
      "loss.item()=-119.41181182861328, ablated_edges=98\n",
      "loss.item()=-121.12705993652344, ablated_edges=96\n",
      "loss.item()=-121.62355041503906, ablated_edges=97\n",
      "loss.item()=-121.74147033691406, ablated_edges=96\n",
      "loss.item()=-122.74212646484375, ablated_edges=97\n",
      "loss.item()=-124.53524017333984, ablated_edges=94\n",
      "loss.item()=-126.29042053222656, ablated_edges=95\n",
      "loss.item()=-126.7813720703125, ablated_edges=94\n",
      "Epochs trained:  120\n",
      "Loss: -126.7814\n",
      "Total preserved: 11516.5391\n",
      "Edges ablated:  94\n",
      "Toxic loss:  12.76762580871582\n",
      "OWT loss:  4.579450607299805\n",
      "Penalty:  tensor(114.0137, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 6.480682372966839e-07, logit diff = -4.388969421386719\n",
      "Best Token: [' the'], P(Alicia) = 6.928844982212468e-07, logit diff = -3.11639404296875\n",
      "\n",
      "\n",
      "loss.item()=-127.94602966308594, ablated_edges=93\n",
      "loss.item()=-128.79969787597656, ablated_edges=95\n",
      "loss.item()=-130.09271240234375, ablated_edges=93\n",
      "loss.item()=-130.9403839111328, ablated_edges=95\n",
      "loss.item()=-133.34471130371094, ablated_edges=95\n",
      "loss.item()=-133.7942657470703, ablated_edges=92\n",
      "loss.item()=-134.7262725830078, ablated_edges=91\n",
      "loss.item()=-136.41575622558594, ablated_edges=93\n",
      "loss.item()=-136.9857940673828, ablated_edges=92\n",
      "loss.item()=-138.68731689453125, ablated_edges=89\n",
      "Epochs trained:  130\n",
      "Loss: -138.6873\n",
      "Total preserved: 11521.3750\n",
      "Edges ablated:  89\n",
      "Toxic loss:  13.104331970214844\n",
      "OWT loss:  3.7181525230407715\n",
      "Penalty:  tensor(125.5830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 6.702366590616293e-07, logit diff = -4.359352111816406\n",
      "Best Token: [' the'], P(Alicia) = 7.543304150203767e-07, logit diff = -2.9630508422851562\n",
      "\n",
      "\n",
      "loss.item()=-140.29449462890625, ablated_edges=91\n",
      "loss.item()=-141.10479736328125, ablated_edges=92\n",
      "loss.item()=-141.9321746826172, ablated_edges=91\n",
      "loss.item()=-142.29318237304688, ablated_edges=91\n",
      "loss.item()=-144.09124755859375, ablated_edges=91\n",
      "loss.item()=-146.16098022460938, ablated_edges=91\n",
      "loss.item()=-146.34945678710938, ablated_edges=91\n",
      "loss.item()=-147.26583862304688, ablated_edges=89\n",
      "loss.item()=-149.03138732910156, ablated_edges=92\n",
      "loss.item()=-149.683349609375, ablated_edges=91\n",
      "Epochs trained:  140\n",
      "Loss: -149.6833\n",
      "Total preserved: 11520.6914\n",
      "Edges ablated:  91\n",
      "Toxic loss:  12.587115287780762\n",
      "OWT loss:  3.6495916843414307\n",
      "Penalty:  tensor(137.0962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 6.589097552023304e-07, logit diff = -4.407264709472656\n",
      "Best Token: [' the'], P(Alicia) = 8.048506856539461e-07, logit diff = -2.91510009765625\n",
      "\n",
      "\n",
      "loss.item()=-150.97669982910156, ablated_edges=91\n",
      "loss.item()=-152.42947387695312, ablated_edges=89\n",
      "loss.item()=-153.6796417236328, ablated_edges=90\n",
      "loss.item()=-153.34532165527344, ablated_edges=86\n",
      "loss.item()=-154.64151000976562, ablated_edges=88\n",
      "loss.item()=-156.8426971435547, ablated_edges=90\n",
      "loss.item()=-157.39642333984375, ablated_edges=90\n",
      "loss.item()=-160.1011962890625, ablated_edges=87\n",
      "loss.item()=-159.9700927734375, ablated_edges=87\n",
      "loss.item()=-161.6258544921875, ablated_edges=85\n",
      "Epochs trained:  150\n",
      "Loss: -161.6259\n",
      "Total preserved: 11525.3926\n",
      "Edges ablated:  85\n",
      "Toxic loss:  12.948275566101074\n",
      "OWT loss:  4.221805572509766\n",
      "Penalty:  tensor(148.6776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 8.955948374023137e-07, logit diff = -4.271461486816406\n",
      "Best Token: [' the'], P(Alicia) = 1.1008818319169222e-06, logit diff = -2.65594482421875\n",
      "\n",
      "\n",
      "loss.item()=-162.74783325195312, ablated_edges=86\n",
      "loss.item()=-163.5485076904297, ablated_edges=84\n",
      "loss.item()=-165.10946655273438, ablated_edges=83\n",
      "loss.item()=-166.32643127441406, ablated_edges=83\n",
      "loss.item()=-167.84376525878906, ablated_edges=80\n",
      "loss.item()=-168.7957305908203, ablated_edges=83\n",
      "loss.item()=-169.68283081054688, ablated_edges=80\n",
      "loss.item()=-171.43804931640625, ablated_edges=80\n",
      "loss.item()=-171.56591796875, ablated_edges=81\n",
      "loss.item()=-173.12464904785156, ablated_edges=78\n",
      "Epochs trained:  160\n",
      "Loss: -173.1246\n",
      "Total preserved: 11530.3906\n",
      "Edges ablated:  78\n",
      "Toxic loss:  12.8522310256958\n",
      "OWT loss:  4.1804890632629395\n",
      "Penalty:  tensor(160.2724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 6.659329301328398e-07, logit diff = -4.2660369873046875\n",
      "Best Token: [' the'], P(Alicia) = 7.6916842317587e-07, logit diff = -2.9383544921875\n",
      "\n",
      "\n",
      "loss.item()=-173.64007568359375, ablated_edges=79\n",
      "loss.item()=-175.5027313232422, ablated_edges=80\n",
      "loss.item()=-175.644287109375, ablated_edges=79\n",
      "loss.item()=-177.35296630859375, ablated_edges=79\n",
      "loss.item()=-178.7149658203125, ablated_edges=77\n",
      "loss.item()=-180.45343017578125, ablated_edges=78\n",
      "loss.item()=-180.3551025390625, ablated_edges=77\n",
      "loss.item()=-182.15196228027344, ablated_edges=77\n",
      "loss.item()=-183.15333557128906, ablated_edges=77\n",
      "loss.item()=-184.37474060058594, ablated_edges=79\n",
      "Epochs trained:  170\n",
      "Loss: -184.3747\n",
      "Total preserved: 11532.6348\n",
      "Edges ablated:  79\n",
      "Toxic loss:  12.53846263885498\n",
      "OWT loss:  4.135136127471924\n",
      "Penalty:  tensor(171.8363, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 7.365035799011821e-07, logit diff = -4.195274353027344\n",
      "Best Token: [' the'], P(Alicia) = 7.865641009630053e-07, logit diff = -2.9011917114257812\n",
      "\n",
      "\n",
      "loss.item()=-185.72958374023438, ablated_edges=77\n",
      "loss.item()=-186.49400329589844, ablated_edges=77\n",
      "loss.item()=-187.80078125, ablated_edges=77\n",
      "loss.item()=-189.00685119628906, ablated_edges=78\n",
      "loss.item()=-190.92724609375, ablated_edges=76\n",
      "loss.item()=-191.5331573486328, ablated_edges=76\n",
      "loss.item()=-192.57113647460938, ablated_edges=76\n",
      "loss.item()=-193.543701171875, ablated_edges=74\n",
      "loss.item()=-195.31983947753906, ablated_edges=74\n",
      "loss.item()=-195.28924560546875, ablated_edges=73\n",
      "Epochs trained:  180\n",
      "Loss: -195.2892\n",
      "Total preserved: 11536.9688\n",
      "Edges ablated:  73\n",
      "Toxic loss:  11.851434707641602\n",
      "OWT loss:  4.2791290283203125\n",
      "Penalty:  tensor(183.4378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 7.636747341166483e-07, logit diff = -4.065376281738281\n",
      "Best Token: [' the'], P(Alicia) = 7.525021032961376e-07, logit diff = -3.0545806884765625\n",
      "\n",
      "\n",
      "loss.item()=-196.90086364746094, ablated_edges=72\n",
      "loss.item()=-198.61868286132812, ablated_edges=72\n",
      "loss.item()=-200.31546020507812, ablated_edges=72\n",
      "loss.item()=-200.2914276123047, ablated_edges=71\n",
      "loss.item()=-202.61178588867188, ablated_edges=73\n",
      "loss.item()=-203.4361572265625, ablated_edges=71\n",
      "loss.item()=-204.4598388671875, ablated_edges=71\n",
      "loss.item()=-205.51669311523438, ablated_edges=72\n",
      "loss.item()=-206.59266662597656, ablated_edges=70\n",
      "loss.item()=-207.98147583007812, ablated_edges=70\n",
      "Epochs trained:  190\n",
      "Loss: -207.9815\n",
      "Total preserved: 11540.6777\n",
      "Edges ablated:  70\n",
      "Toxic loss:  12.944040298461914\n",
      "OWT loss:  3.966722249984741\n",
      "Penalty:  tensor(195.0374, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 7.860719506425085e-07, logit diff = -4.152061462402344\n",
      "Best Token: [' the'], P(Alicia) = 8.110899329949461e-07, logit diff = -2.9906692504882812\n",
      "\n",
      "\n",
      "loss.item()=-207.76214599609375, ablated_edges=71\n",
      "loss.item()=-209.87078857421875, ablated_edges=70\n",
      "loss.item()=-209.6343231201172, ablated_edges=68\n",
      "loss.item()=-212.59815979003906, ablated_edges=70\n",
      "loss.item()=-213.78065490722656, ablated_edges=70\n",
      "loss.item()=-214.0928497314453, ablated_edges=69\n",
      "loss.item()=-215.7135467529297, ablated_edges=69\n",
      "loss.item()=-216.8952178955078, ablated_edges=69\n",
      "loss.item()=-217.69415283203125, ablated_edges=68\n",
      "loss.item()=-219.7125244140625, ablated_edges=67\n",
      "Epochs trained:  200\n",
      "Loss: -219.7125\n",
      "Total preserved: 11543.6748\n",
      "Edges ablated:  67\n",
      "Toxic loss:  13.080759048461914\n",
      "OWT loss:  4.303644180297852\n",
      "Penalty:  tensor(206.6318, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 8.163637517100142e-07, logit diff = -4.327522277832031\n",
      "Best Token: [' the'], P(Alicia) = 9.230402042703645e-07, logit diff = -2.9377822875976562\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = -1 * (regularization_strength * penalty + alpha * tox_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            with torch.no_grad():\n",
    "                test_ioi_sentences = [\"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\", \"While Joshua and Alicia were commuting to the restaurant, Joshua gave a snack to\"]\n",
    "                for test_ioi_sentence in test_ioi_sentences:\n",
    "                    correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "                    other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "                    test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "                    generation = model(test_ioi_tokens)[0][:, -1]\n",
    "                    probs = torch.softmax(generation, dim=-1)\n",
    "                    print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    # epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    epochs_left = -1\n",
    "    # log_every = int(input('set log frequency'))\n",
    "    # edge_threshold = int(input('set edge threshold'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/circuit_covering_alternative_necessary_params_dict_lambda={regularization_strength}_{alpha=}_means_ioi=False_{template_type=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model before and after circuit breaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/ioi_sentences_test.pkl\", \"rb\") as f:\n",
    "    ioi_sentences_test = pickle.load(f)\n",
    "    # ioi_sentences_test = [t[2] for t in ioi_sentences_test]\n",
    "\n",
    "with open(\"data/eval_uniform.pkl\", \"rb\") as f:\n",
    "    uniform_samples = pickle.load(f)\n",
    "    uniform_sentences = [t[2] for t in uniform_samples]\n",
    "\n",
    "original_model = load_demo_gpt2(means=False)\n",
    "\n",
    "# with open(\"models/masked_gpt2_mean_ablation_v6.pkl\", \"rb\") as f:\n",
    "#     model.state_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on an ioi_sentence\n",
    "ioi_sentence = ioi_sentences_test[0]\n",
    "print(ioi_sentence)\n",
    "# ioi_tokens = tokenizer(ioi_sentence, return_tensors='pt').input_ids.to('cuda')\n",
    "\n",
    "original_model.eval()\n",
    "original_model.to('cuda')\n",
    "def get_last_token(model, prompt, topk=5):\n",
    "    # generate last token\n",
    "    tokens = tokenizer(prompt, return_tensors='pt').input_ids[:, :-1]\n",
    "\n",
    "    # generate one token, decode original_model(ioi_tokens[:, :-1])\n",
    "    model_outputs = model(tokens)[0]\n",
    "    model_outputs = model_outputs.squeeze(0)[-1]\n",
    "    probs = torch.nn.functional.softmax(model_outputs, dim=-1)\n",
    "\n",
    "    topk_outputs = torch.topk(model_outputs, topk)\n",
    "    topk_tokens = topk_outputs.indices\n",
    "    topk_probs = probs[topk_outputs.indices]\n",
    "    \n",
    "    # decode tokens\n",
    "    for i in range(topk):\n",
    "        print(f\"{tokenizer.decode(topk_tokens[i].unsqueeze(0))}, probability of {topk_probs[i]}\")\n",
    "    topk_tokens_decoded = tokenizer.batch_decode(topk_tokens)\n",
    "    return topk_tokens_decoded, topk_probs\n",
    "\n",
    "print(\"Before ablation\")\n",
    "_ = get_last_token(original_model, ioi_sentence)\n",
    "print()\n",
    "print()\n",
    "print(\"After ablation\")\n",
    "_ = get_last_token(model, ioi_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try on uniform samples\n",
    "for idx in range(3):\n",
    "    print(uniform_sentences[idx])\n",
    "    print(\"Before ablation\")\n",
    "    _ = get_last_token(original_model, uniform_sentences[idx])\n",
    "    print()\n",
    "    print(\"After ablation\")\n",
    "    _ = get_last_token(model, uniform_sentences[idx])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize mask\n",
    "Create the computational graphs in edge attribution patching paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate which nodes will be in the graph\n",
    "connected_nodes = set()\n",
    "# add embed node at position\n",
    "# connected_nodes.add((-1, \"embed\"))\n",
    "n_heads = 12\n",
    "n_layers = 12\n",
    "\n",
    "# associate each node with a position\n",
    "all_possible_nodes = [(-1, \"embed\")]\n",
    "mask_dict = {}\n",
    "# empty tensor\n",
    "mask_dict[\"embed\"] = torch.zeros(size=(0,))\n",
    "for idx in range(len(mask_params)):\n",
    "    if \"attention\" in param_names[idx]:\n",
    "        layer = int(param_names[idx].split(\".\")[1])\n",
    "        for i in range(n_heads):\n",
    "            all_possible_nodes.append((layer, f\"a{layer}.{i}\"))\n",
    "            mask_dict[f\"a{layer}.{i}\"] = mask_params[idx][:,i].detach().cpu()\n",
    "    elif \"mlp\" in param_names[idx]:\n",
    "        layer = int(param_names[idx].split(\".\")[1])\n",
    "        all_possible_nodes.append((layer, f\"m{layer}\"))\n",
    "        mask_dict[f\"m{layer}\"] = mask_params[idx].detach().cpu()\n",
    "all_possible_nodes.append((n_heads, \"output\"))\n",
    "mask_dict[\"output\"] = mask_params[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate where edges are based on the mask\n",
    "# Edge between node i and node j if mask_dict[i][all_possible_nodes.index(j)] == 0\n",
    "sufficient = True\n",
    "\n",
    "edges = []\n",
    "for i in range(len(all_possible_nodes)):\n",
    "    for j in range(len(all_possible_nodes)):\n",
    "        j_index = all_possible_nodes.index(all_possible_nodes[j])\n",
    "        if j_index < len(mask_dict[all_possible_nodes[i][1]]) and mask_dict[all_possible_nodes[i][1]][all_possible_nodes.index(all_possible_nodes[j])] == (1 if sufficient else 0):\n",
    "            edges.append((all_possible_nodes[i], all_possible_nodes[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aligned_graph(all_possible_nodes, edges):\n",
    "    G = pgv.AGraph(strict=False, directed=True)\n",
    "\n",
    "    # Find the maximum layer number for adjusting the graph\n",
    "    max_layer = max(layer for layer, _ in all_possible_nodes if isinstance(layer, int))\n",
    "    nodes_with_edges = set([node for edge in edges for node in edge])\n",
    "    print(nodes_with_edges)\n",
    "    # Add nodes and edges to the graph\n",
    "    # for node in all_possible_nodes:\n",
    "    #     if node in [edge[0] for edge in edges] or node in [edge[1] for edge in edges]:\n",
    "    #         G.add_node(node[1], layer=str(max_layer - node[0]))\n",
    "\n",
    "    for edge in edges:\n",
    "        G.add_edge(edge[1][1], edge[0][1])\n",
    "\n",
    "    # Create subgraphs to ensure nodes of the same layer have the same rank\n",
    "    for layer in range(max_layer, -2, -1):\n",
    "        with G.subgraph(name=f'cluster_{layer}') as s:\n",
    "            s.graph_attr['rank'] = 'same'\n",
    "            for node in nodes_with_edges:\n",
    "                if node[0] == layer:\n",
    "                    s.add_node(node[1])\n",
    "\n",
    "    # Apply layout and render the graph\n",
    "    G.layout(prog='dot')\n",
    "    G.draw('aligned_graph.png')\n",
    "    return Image('aligned_graph.png')\n",
    "\n",
    "# Call the function with your nodes and edges\n",
    "flipped_graph_image = create_aligned_graph(all_possible_nodes, edges)\n",
    "\n",
    "# To display the graph in Jupyter Notebook\n",
    "flipped_graph_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "unlrn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
