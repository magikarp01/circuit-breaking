{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train different kinds of masks over IOI edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from models import load_gpt2_weights, load_demo_gpt2, tokenizer\n",
    "from data import retrieve_toxic_data, retrieve_owt_data, retrieve_toxic_data_low_loss, retrieve_toxic_filtered_data, FILTER_DEMO_LEN, CONTEXT_LENGTH\n",
    "from inference import infer_batch_with_owt, infer_batch, prepare_fixed_demo, criterion\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import pickle\n",
    "import datasets\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from itertools import cycle\n",
    "# from eval import evaluate_model\n",
    "from data import batch_text_to_tokens\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train params of mask\n",
    "Train without the original D_train loss term (only mask loss and IOI data loss)\n",
    "Finds necessary (but not sufficient) edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_batch_size = 1 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 1\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "means_ioi = True\n",
    "if means_ioi:\n",
    "    with open(\"data/gpt2_ioi_abc_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "else:\n",
    "    with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "\n",
    "model = load_demo_gpt2(means=means)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 20 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 1 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 100\n",
    "max_steps_per_epoch = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29994/474617925.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e24fdb4b2049358c6972e7be9335e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=-92.44132995605469, ablated_edges=3041\n",
      "loss.item()=-90.90975952148438, ablated_edges=4004\n",
      "loss.item()=-92.3156509399414, ablated_edges=4468\n",
      "loss.item()=-103.08588409423828, ablated_edges=4713\n",
      "loss.item()=-100.81427764892578, ablated_edges=4920\n",
      "loss.item()=-106.47522735595703, ablated_edges=5045\n",
      "loss.item()=-94.9225845336914, ablated_edges=5198\n",
      "loss.item()=-102.33749389648438, ablated_edges=5186\n",
      "loss.item()=-106.84143829345703, ablated_edges=5279\n",
      "loss.item()=-102.71851348876953, ablated_edges=5379\n",
      "Epochs trained:  10\n",
      "Loss: -102.7185\n",
      "Total preserved: 6091.4785\n",
      "Edges ablated:  5379\n",
      "Toxic loss:  102.71851348876953\n",
      "OWT loss:  34.21355056762695\n",
      "Penalty:  0\n",
      "Best Token: [' be'], P(Alicia) = 8.407790785948902e-45, logit diff = -2.9167213439941406\n",
      "\n",
      "\n",
      "loss.item()=-101.58848571777344, ablated_edges=5468\n",
      "loss.item()=-98.39625549316406, ablated_edges=5456\n",
      "loss.item()=-101.33850860595703, ablated_edges=5534\n",
      "loss.item()=-109.25448608398438, ablated_edges=5524\n",
      "loss.item()=-106.44743347167969, ablated_edges=5582\n",
      "loss.item()=-106.34718322753906, ablated_edges=5589\n",
      "loss.item()=-109.37702941894531, ablated_edges=5595\n",
      "loss.item()=-107.44593811035156, ablated_edges=5634\n",
      "loss.item()=-87.65074920654297, ablated_edges=5668\n",
      "loss.item()=-108.56571960449219, ablated_edges=5661\n",
      "Epochs trained:  20\n",
      "Loss: -108.5657\n",
      "Total preserved: 5918.6128\n",
      "Edges ablated:  5673\n",
      "Toxic loss:  108.56571960449219\n",
      "OWT loss:  31.522239685058594\n",
      "Penalty:  0\n",
      "Best Token: [' be'], P(Alicia) = 1.1791325336262905e-21, logit diff = -2.89947509765625\n",
      "\n",
      "\n",
      "loss.item()=-99.86837768554688, ablated_edges=5726\n",
      "loss.item()=-101.97744750976562, ablated_edges=5310\n",
      "loss.item()=-107.37273406982422, ablated_edges=4873\n",
      "loss.item()=-104.88069915771484, ablated_edges=4445\n",
      "loss.item()=-113.649169921875, ablated_edges=4196\n",
      "loss.item()=-110.55256652832031, ablated_edges=3957\n",
      "loss.item()=-116.81430053710938, ablated_edges=3666\n",
      "loss.item()=-114.21040344238281, ablated_edges=3497\n",
      "loss.item()=-116.2144775390625, ablated_edges=3338\n",
      "loss.item()=-111.6000747680664, ablated_edges=3210\n",
      "Epochs trained:  30\n",
      "Loss: -111.6001\n",
      "Total preserved: 8298.7119\n",
      "Edges ablated:  3210\n",
      "Toxic loss:  104.13123321533203\n",
      "OWT loss:  23.01451301574707\n",
      "Penalty:  tensor(7.4688, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' be'], P(Alicia) = 0.0, logit diff = -16.27617645263672\n",
      "\n",
      "\n",
      "loss.item()=-115.50196838378906, ablated_edges=3102\n",
      "loss.item()=-122.13426971435547, ablated_edges=3021\n",
      "loss.item()=-115.32892608642578, ablated_edges=2881\n",
      "loss.item()=-119.78565979003906, ablated_edges=2736\n",
      "loss.item()=-113.23202514648438, ablated_edges=2658\n",
      "loss.item()=-108.43406677246094, ablated_edges=2547\n",
      "loss.item()=-120.790771484375, ablated_edges=2461\n",
      "loss.item()=-121.39958953857422, ablated_edges=2353\n",
      "loss.item()=-121.21941375732422, ablated_edges=2393\n",
      "loss.item()=-122.47407531738281, ablated_edges=2282\n",
      "Epochs trained:  40\n",
      "Loss: -122.4741\n",
      "Total preserved: 9149.3125\n",
      "Edges ablated:  2280\n",
      "Toxic loss:  105.09037780761719\n",
      "OWT loss:  30.122894287109375\n",
      "Penalty:  tensor(17.3837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' be'], P(Alicia) = 1.8517995758529084e-16, logit diff = -2.7570629119873047\n",
      "\n",
      "\n",
      "loss.item()=-124.59349822998047, ablated_edges=2577\n",
      "loss.item()=-126.23872375488281, ablated_edges=2208\n",
      "loss.item()=-126.01573181152344, ablated_edges=2131\n",
      "loss.item()=-119.08729553222656, ablated_edges=2082\n",
      "loss.item()=-127.72577667236328, ablated_edges=1966\n",
      "loss.item()=-128.75067138671875, ablated_edges=1929\n",
      "loss.item()=-130.32984924316406, ablated_edges=1816\n",
      "loss.item()=-134.871826171875, ablated_edges=1810\n",
      "loss.item()=-128.2957000732422, ablated_edges=1739\n",
      "loss.item()=-138.2187957763672, ablated_edges=1746\n",
      "Epochs trained:  50\n",
      "Loss: -138.2188\n",
      "Total preserved: 9711.0469\n",
      "Edges ablated:  1746\n",
      "Toxic loss:  110.05675506591797\n",
      "OWT loss:  30.98063850402832\n",
      "Penalty:  tensor(28.1620, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' be'], P(Alicia) = 0.0, logit diff = -5.063457489013672\n",
      "\n",
      "\n",
      "loss.item()=-132.0901641845703, ablated_edges=1758\n",
      "loss.item()=-130.10235595703125, ablated_edges=1694\n",
      "loss.item()=-141.49893188476562, ablated_edges=1671\n",
      "loss.item()=-140.14358520507812, ablated_edges=1647\n",
      "loss.item()=-119.37608337402344, ablated_edges=1639\n",
      "loss.item()=-138.77005004882812, ablated_edges=1590\n",
      "loss.item()=-140.4674072265625, ablated_edges=1533\n",
      "loss.item()=-141.68264770507812, ablated_edges=1538\n",
      "loss.item()=-139.0724639892578, ablated_edges=1561\n",
      "loss.item()=-133.6548309326172, ablated_edges=1545\n",
      "Epochs trained:  60\n",
      "Loss: -133.6548\n",
      "Total preserved: 9916.4121\n",
      "Edges ablated:  1545\n",
      "Toxic loss:  94.98081970214844\n",
      "OWT loss:  23.500368118286133\n",
      "Penalty:  tensor(38.6740, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' be'], P(Alicia) = 6.230181394288055e-21, logit diff = 8.883743286132812\n",
      "\n",
      "\n",
      "loss.item()=-145.50181579589844, ablated_edges=1607\n",
      "loss.item()=-147.21340942382812, ablated_edges=1521\n",
      "loss.item()=-147.45091247558594, ablated_edges=1476\n",
      "loss.item()=-150.25083923339844, ablated_edges=1395\n",
      "loss.item()=-150.2332763671875, ablated_edges=1409\n",
      "loss.item()=-145.74444580078125, ablated_edges=1406\n",
      "loss.item()=-148.60333251953125, ablated_edges=1361\n",
      "loss.item()=-145.0133056640625, ablated_edges=1318\n",
      "loss.item()=-149.07217407226562, ablated_edges=1310\n",
      "loss.item()=-152.36404418945312, ablated_edges=1327\n",
      "Epochs trained:  70\n",
      "Loss: -152.3640\n",
      "Total preserved: 10144.8457\n",
      "Edges ablated:  1327\n",
      "Toxic loss:  102.654296875\n",
      "OWT loss:  37.0340576171875\n",
      "Penalty:  tensor(49.7097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' be'], P(Alicia) = 0.0, logit diff = -4.3809814453125\n",
      "\n",
      "\n",
      "loss.item()=-159.9199676513672, ablated_edges=1295\n",
      "loss.item()=-157.300537109375, ablated_edges=1278\n",
      "loss.item()=-158.8067169189453, ablated_edges=1259\n",
      "loss.item()=-154.5634765625, ablated_edges=1255\n",
      "loss.item()=-161.94668579101562, ablated_edges=1217\n",
      "loss.item()=-164.52244567871094, ablated_edges=1216\n",
      "loss.item()=-154.16635131835938, ablated_edges=1230\n",
      "loss.item()=-160.5937957763672, ablated_edges=1229\n",
      "loss.item()=-166.86691284179688, ablated_edges=1220\n",
      "loss.item()=-166.69847106933594, ablated_edges=1138\n",
      "Epochs trained:  80\n",
      "Loss: -166.6985\n",
      "Total preserved: 10337.4375\n",
      "Edges ablated:  1135\n",
      "Toxic loss:  105.70758819580078\n",
      "OWT loss:  29.98273277282715\n",
      "Penalty:  tensor(60.9909, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' be'], P(Alicia) = 2.837908131148731e-13, logit diff = 1.4386138916015625\n",
      "\n",
      "\n",
      "loss.item()=-165.5328826904297, ablated_edges=1265\n",
      "loss.item()=-172.78610229492188, ablated_edges=1159\n",
      "loss.item()=-166.36782836914062, ablated_edges=1135\n",
      "loss.item()=-163.71823120117188, ablated_edges=1134\n",
      "loss.item()=-174.5654754638672, ablated_edges=1085\n",
      "loss.item()=-170.394287109375, ablated_edges=1089\n",
      "loss.item()=-180.5911407470703, ablated_edges=1123\n",
      "loss.item()=-168.2398681640625, ablated_edges=1073\n",
      "loss.item()=-164.28628540039062, ablated_edges=1021\n",
      "loss.item()=-173.49334716796875, ablated_edges=1059\n",
      "Epochs trained:  90\n",
      "Loss: -173.4933\n",
      "Total preserved: 10416.3994\n",
      "Edges ablated:  1059\n",
      "Toxic loss:  101.62018585205078\n",
      "OWT loss:  31.974336624145508\n",
      "Penalty:  tensor(71.8732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' be'], P(Alicia) = 0.0, logit diff = -20.704193115234375\n",
      "\n",
      "\n",
      "loss.item()=-178.20062255859375, ablated_edges=1032\n",
      "loss.item()=-182.46664428710938, ablated_edges=992\n",
      "loss.item()=-184.08010864257812, ablated_edges=964\n",
      "loss.item()=-166.4796600341797, ablated_edges=1019\n",
      "loss.item()=-170.63156127929688, ablated_edges=1006\n",
      "loss.item()=-173.96365356445312, ablated_edges=973\n",
      "loss.item()=-180.6466064453125, ablated_edges=962\n",
      "loss.item()=-194.23057556152344, ablated_edges=962\n",
      "loss.item()=-188.30752563476562, ablated_edges=984\n",
      "loss.item()=-179.58322143554688, ablated_edges=954\n",
      "Epochs trained:  100\n",
      "Loss: -179.5832\n",
      "Total preserved: 10544.6436\n",
      "Edges ablated:  955\n",
      "Toxic loss:  96.28052520751953\n",
      "OWT loss:  29.245271682739258\n",
      "Penalty:  tensor(83.3027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' be'], P(Alicia) = 9.306607134246248e-13, logit diff = 8.972152709960938\n",
      "\n",
      "\n",
      "loss.item()=-191.84059143066406, ablated_edges=1035\n",
      "loss.item()=-179.6178741455078, ablated_edges=976\n",
      "loss.item()=-191.42056274414062, ablated_edges=935\n",
      "loss.item()=-182.20538330078125, ablated_edges=918\n",
      "loss.item()=-189.68292236328125, ablated_edges=956\n",
      "loss.item()=-189.52053833007812, ablated_edges=949\n",
      "loss.item()=-194.20529174804688, ablated_edges=942\n",
      "loss.item()=-196.9206085205078, ablated_edges=906\n",
      "loss.item()=-201.01345825195312, ablated_edges=900\n",
      "loss.item()=-187.35995483398438, ablated_edges=889\n",
      "Epochs trained:  110\n",
      "Loss: -187.3600\n",
      "Total preserved: 10577.0830\n",
      "Edges ablated:  889\n",
      "Toxic loss:  93.22393798828125\n",
      "OWT loss:  24.572851181030273\n",
      "Penalty:  tensor(94.1360, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' be'], P(Alicia) = 1.3984958673961674e-42, logit diff = -3.5816192626953125\n",
      "\n",
      "\n",
      "loss.item()=-194.74142456054688, ablated_edges=890\n",
      "loss.item()=-205.070556640625, ablated_edges=875\n",
      "loss.item()=-207.18173217773438, ablated_edges=895\n",
      "loss.item()=-194.04269409179688, ablated_edges=860\n",
      "loss.item()=-198.04373168945312, ablated_edges=847\n",
      "loss.item()=-207.0074920654297, ablated_edges=848\n",
      "loss.item()=-207.48898315429688, ablated_edges=873\n",
      "loss.item()=-202.26925659179688, ablated_edges=845\n",
      "loss.item()=-202.31536865234375, ablated_edges=840\n",
      "loss.item()=-209.64443969726562, ablated_edges=830\n",
      "Epochs trained:  120\n",
      "Loss: -209.6444\n",
      "Total preserved: 10695.9062\n",
      "Edges ablated:  828\n",
      "Toxic loss:  103.75498962402344\n",
      "OWT loss:  28.776575088500977\n",
      "Penalty:  tensor(105.8895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' be'], P(Alicia) = 4.786251320072295e-17, logit diff = -1.7457504272460938\n",
      "\n",
      "\n",
      "loss.item()=-203.93780517578125, ablated_edges=872\n",
      "loss.item()=-209.78518676757812, ablated_edges=844\n",
      "loss.item()=-219.00802612304688, ablated_edges=827\n",
      "loss.item()=-219.9264678955078, ablated_edges=816\n",
      "loss.item()=-205.64703369140625, ablated_edges=820\n",
      "loss.item()=-220.24462890625, ablated_edges=824\n",
      "loss.item()=-214.46812438964844, ablated_edges=818\n",
      "loss.item()=-217.8345947265625, ablated_edges=787\n",
      "loss.item()=-224.0679473876953, ablated_edges=762\n",
      "loss.item()=-224.64341735839844, ablated_edges=764\n",
      "Epochs trained:  130\n",
      "Loss: -224.6434\n",
      "Total preserved: 10753.0107\n",
      "Edges ablated:  764\n",
      "Toxic loss:  107.43561553955078\n",
      "OWT loss:  34.21518325805664\n",
      "Penalty:  tensor(117.2078, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' be'], P(Alicia) = 0.0, logit diff = -9.156261444091797\n",
      "\n",
      "\n",
      "loss.item()=-220.03515625, ablated_edges=763\n",
      "loss.item()=-219.82887268066406, ablated_edges=765\n",
      "loss.item()=-220.74996948242188, ablated_edges=758\n",
      "loss.item()=-230.34207153320312, ablated_edges=750\n",
      "loss.item()=-217.77932739257812, ablated_edges=770\n",
      "loss.item()=-239.68020629882812, ablated_edges=719\n",
      "loss.item()=-223.8897705078125, ablated_edges=732\n",
      "loss.item()=-234.97396850585938, ablated_edges=698\n",
      "loss.item()=-225.52523803710938, ablated_edges=730\n",
      "loss.item()=-223.08843994140625, ablated_edges=712\n",
      "Epochs trained:  140\n",
      "Loss: -223.0884\n",
      "Total preserved: 10784.1992\n",
      "Edges ablated:  714\n",
      "Toxic loss:  94.75647735595703\n",
      "OWT loss:  15.312424659729004\n",
      "Penalty:  tensor(128.3320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' is'], P(Alicia) = 4.086369997935435e-08, logit diff = 7.997398376464844\n",
      "\n",
      "\n",
      "loss.item()=-233.46559143066406, ablated_edges=763\n",
      "loss.item()=-241.89840698242188, ablated_edges=731\n",
      "loss.item()=-233.15374755859375, ablated_edges=712\n",
      "loss.item()=-226.0502166748047, ablated_edges=739\n",
      "loss.item()=-234.62158203125, ablated_edges=721\n",
      "loss.item()=-239.542724609375, ablated_edges=694\n",
      "loss.item()=-227.26760864257812, ablated_edges=707\n",
      "loss.item()=-242.1531524658203, ablated_edges=698\n",
      "loss.item()=-241.11959838867188, ablated_edges=710\n",
      "loss.item()=-250.21620178222656, ablated_edges=695\n",
      "Epochs trained:  150\n",
      "Loss: -250.2162\n",
      "Total preserved: 10816.4502\n",
      "Edges ablated:  695\n",
      "Toxic loss:  110.68399047851562\n",
      "OWT loss:  29.43450927734375\n",
      "Penalty:  tensor(139.5322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' be'], P(Alicia) = 0.0, logit diff = -11.286197662353516\n",
      "\n",
      "\n",
      "loss.item()=-242.90579223632812, ablated_edges=691\n",
      "loss.item()=-238.96279907226562, ablated_edges=696\n",
      "loss.item()=-239.45846557617188, ablated_edges=677\n",
      "loss.item()=-242.35220336914062, ablated_edges=704\n",
      "loss.item()=-242.97061157226562, ablated_edges=655\n",
      "loss.item()=-240.59320068359375, ablated_edges=649\n",
      "loss.item()=-252.18228149414062, ablated_edges=648\n",
      "loss.item()=-248.90243530273438, ablated_edges=651\n",
      "loss.item()=-258.3960266113281, ablated_edges=659\n",
      "loss.item()=-258.2515869140625, ablated_edges=660\n",
      "Epochs trained:  160\n",
      "Loss: -258.2516\n",
      "Total preserved: 10863.5977\n",
      "Edges ablated:  658\n",
      "Toxic loss:  107.24759674072266\n",
      "OWT loss:  29.795133590698242\n",
      "Penalty:  tensor(151.0040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' will'], P(Alicia) = 1.5326498750689588e-12, logit diff = 4.783916473388672\n",
      "\n",
      "\n",
      "loss.item()=-246.3875274658203, ablated_edges=696\n",
      "loss.item()=-249.16726684570312, ablated_edges=647\n",
      "loss.item()=-262.78131103515625, ablated_edges=683\n",
      "loss.item()=-250.66281127929688, ablated_edges=648\n",
      "loss.item()=-255.13934326171875, ablated_edges=607\n",
      "loss.item()=-258.20135498046875, ablated_edges=618\n",
      "loss.item()=-257.19403076171875, ablated_edges=632\n",
      "loss.item()=-268.50726318359375, ablated_edges=618\n",
      "loss.item()=-256.31781005859375, ablated_edges=604\n",
      "loss.item()=-265.478515625, ablated_edges=609\n",
      "Epochs trained:  170\n",
      "Loss: -265.4785\n",
      "Total preserved: 10900.7705\n",
      "Edges ablated:  609\n",
      "Toxic loss:  103.05706787109375\n",
      "OWT loss:  26.33960723876953\n",
      "Penalty:  tensor(162.4214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' be'], P(Alicia) = 0.0, logit diff = -14.405715942382812\n",
      "\n",
      "\n",
      "loss.item()=-261.76434326171875, ablated_edges=595\n",
      "loss.item()=-266.0794677734375, ablated_edges=596\n",
      "loss.item()=-268.1497802734375, ablated_edges=583\n",
      "loss.item()=-268.260009765625, ablated_edges=579\n",
      "loss.item()=-273.332763671875, ablated_edges=594\n",
      "loss.item()=-268.65130615234375, ablated_edges=582\n",
      "loss.item()=-271.91143798828125, ablated_edges=577\n",
      "loss.item()=-275.36663818359375, ablated_edges=586\n",
      "loss.item()=-265.07598876953125, ablated_edges=597\n",
      "loss.item()=-276.65045166015625, ablated_edges=563\n",
      "Epochs trained:  180\n",
      "Loss: -276.6505\n",
      "Total preserved: 10977.0000\n",
      "Edges ablated:  562\n",
      "Toxic loss:  102.11614227294922\n",
      "OWT loss:  30.2384033203125\n",
      "Penalty:  tensor(174.5343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' would'], P(Alicia) = 2.286787787261638e-15, logit diff = 0.9006462097167969\n",
      "\n",
      "\n",
      "loss.item()=-274.4586181640625, ablated_edges=610\n",
      "loss.item()=-282.58270263671875, ablated_edges=562\n",
      "loss.item()=-276.66717529296875, ablated_edges=561\n",
      "loss.item()=-285.0693359375, ablated_edges=555\n",
      "loss.item()=-280.48822021484375, ablated_edges=561\n",
      "loss.item()=-280.21368408203125, ablated_edges=548\n",
      "loss.item()=-273.1119384765625, ablated_edges=552\n",
      "loss.item()=-290.64776611328125, ablated_edges=555\n",
      "loss.item()=-289.23724365234375, ablated_edges=520\n",
      "loss.item()=-277.1679992675781, ablated_edges=537\n",
      "Epochs trained:  190\n",
      "Loss: -277.1680\n",
      "Total preserved: 10986.3477\n",
      "Edges ablated:  537\n",
      "Toxic loss:  91.49871063232422\n",
      "OWT loss:  31.47937774658203\n",
      "Penalty:  tensor(185.6693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' be'], P(Alicia) = 0.0, logit diff = -7.254058837890625\n",
      "\n",
      "\n",
      "loss.item()=-290.664794921875, ablated_edges=549\n",
      "loss.item()=-274.59307861328125, ablated_edges=539\n",
      "loss.item()=-298.7602844238281, ablated_edges=540\n",
      "loss.item()=-284.3714599609375, ablated_edges=522\n",
      "loss.item()=-298.76202392578125, ablated_edges=517\n",
      "loss.item()=-294.51715087890625, ablated_edges=518\n",
      "loss.item()=-288.75128173828125, ablated_edges=511\n",
      "loss.item()=-296.91436767578125, ablated_edges=526\n",
      "loss.item()=-295.0049133300781, ablated_edges=518\n",
      "loss.item()=-300.6507263183594, ablated_edges=524\n",
      "Epochs trained:  200\n",
      "Loss: -300.6507\n",
      "Total preserved: 10998.3955\n",
      "Edges ablated:  529\n",
      "Toxic loss:  103.77947998046875\n",
      "OWT loss:  37.906620025634766\n",
      "Penalty:  tensor(196.8712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['.'], P(Alicia) = 3.3197064346589256e-14, logit diff = -6.7275390625\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = -1 * (regularization_strength * penalty + alpha * tox_loss) #+ owt_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            \n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_ioi_sentence = \"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\"\n",
    "                correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "                other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "                test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "                generation = model(test_ioi_tokens)[0][:, -1]\n",
    "                probs = torch.softmax(generation, dim=-1)\n",
    "                print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")\n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            print(\"\\n\")\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    # epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    # log_every = int(input('set log frequency'))\n",
    "    # edge_threshold = int(input('set edge threshold'))\n",
    "    epochs_left = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/alternative_necessary_masks_params_dict_lambda=1_{means_ioi=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different alternative: sufficient but not necessary\n",
    "Trains with an inverted loss function. This loss function encourages sparsity (as opposed to discouraging) and wants model to ablate everything but the necessary circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29994/2186668631.py:55: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78829644642e4117a5e36619580465d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=9.298280929215252e-06, ablated_edges=2378\n",
      "loss.item()=0.8124661445617676, ablated_edges=4025\n",
      "loss.item()=1.0549850463867188, ablated_edges=4580\n",
      "loss.item()=7.629103492945433e-05, ablated_edges=4605\n",
      "loss.item()=8.415821503149346e-05, ablated_edges=4602\n",
      "loss.item()=0.0018838769756257534, ablated_edges=4826\n",
      "loss.item()=1.3828182090946939e-05, ablated_edges=5100\n",
      "loss.item()=1.811964830267243e-05, ablated_edges=5196\n",
      "loss.item()=6.318072337307967e-06, ablated_edges=5296\n",
      "loss.item()=7.152555099310121e-07, ablated_edges=5359\n",
      "Epochs trained:  10\n",
      "Loss: 0.0000\n",
      "Total preserved: 6199.9873\n",
      "Edges ablated:  5359\n",
      "Toxic loss:  7.152555099310121e-07\n",
      "OWT loss:  12.513823509216309\n",
      "Penalty:  0\n",
      "Best Token: [' Alicia'], P(Alicia) = 1.0\n",
      "\n",
      "\n",
      "loss.item()=0.015571675263345242, ablated_edges=5390\n",
      "loss.item()=0.010316380299627781, ablated_edges=5361\n",
      "loss.item()=0.16412676870822906, ablated_edges=5406\n",
      "loss.item()=3.7431014789035544e-05, ablated_edges=5421\n",
      "loss.item()=0.016909589990973473, ablated_edges=5393\n",
      "loss.item()=7.705535888671875, ablated_edges=5474\n",
      "loss.item()=1.7089588642120361, ablated_edges=5487\n",
      "loss.item()=0.18428602814674377, ablated_edges=5586\n",
      "loss.item()=0.17842960357666016, ablated_edges=5538\n",
      "loss.item()=0.002898303559049964, ablated_edges=5612\n",
      "Epochs trained:  20\n",
      "Loss: 0.0029\n",
      "Total preserved: 5979.0703\n",
      "Edges ablated:  5608\n",
      "Toxic loss:  0.002898303559049964\n",
      "OWT loss:  10.100081443786621\n",
      "Penalty:  0\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9480682015419006\n",
      "\n",
      "\n",
      "loss.item()=4.291525328881107e-06, ablated_edges=5657\n",
      "loss.item()=0.5429532527923584, ablated_edges=6103\n",
      "loss.item()=0.9266508221626282, ablated_edges=6980\n",
      "loss.item()=1.351397156715393, ablated_edges=7223\n",
      "loss.item()=1.9225729703903198, ablated_edges=7791\n",
      "loss.item()=1.842686653137207, ablated_edges=8072\n",
      "loss.item()=2.124994993209839, ablated_edges=8284\n",
      "loss.item()=4.6525774002075195, ablated_edges=8570\n",
      "loss.item()=2.6930556297302246, ablated_edges=8441\n",
      "loss.item()=2.5517163276672363, ablated_edges=8929\n",
      "Epochs trained:  30\n",
      "Loss: 2.5517\n",
      "Total preserved: 2833.9285\n",
      "Edges ablated:  8929\n",
      "Toxic loss:  0.0011804286623373628\n",
      "OWT loss:  9.422680854797363\n",
      "Penalty:  tensor(2.5505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999996423721313\n",
      "\n",
      "\n",
      "loss.item()=2.052722454071045, ablated_edges=9689\n",
      "loss.item()=4.497923374176025, ablated_edges=9724\n",
      "loss.item()=2.0055134296417236, ablated_edges=10073\n",
      "loss.item()=1.9165370464324951, ablated_edges=10385\n",
      "loss.item()=2.2197554111480713, ablated_edges=10241\n",
      "loss.item()=2.531726837158203, ablated_edges=10300\n",
      "loss.item()=2.309950828552246, ablated_edges=10370\n",
      "loss.item()=2.215712785720825, ablated_edges=10518\n",
      "loss.item()=2.7718491554260254, ablated_edges=10216\n",
      "loss.item()=2.6770846843719482, ablated_edges=10327\n",
      "Epochs trained:  40\n",
      "Loss: 2.6771\n",
      "Total preserved: 1408.9899\n",
      "Edges ablated:  10330\n",
      "Toxic loss:  3.933898824470816e-06\n",
      "OWT loss:  8.161986351013184\n",
      "Penalty:  tensor(2.6771, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.996860146522522\n",
      "\n",
      "\n",
      "loss.item()=6.133469104766846, ablated_edges=10418\n",
      "loss.item()=12.501707077026367, ablated_edges=10457\n",
      "loss.item()=2.387819766998291, ablated_edges=10607\n",
      "loss.item()=2.5603182315826416, ablated_edges=10684\n",
      "loss.item()=3.807394504547119, ablated_edges=10479\n",
      "loss.item()=2.803102493286133, ablated_edges=10623\n",
      "loss.item()=7.647099494934082, ablated_edges=10578\n",
      "loss.item()=2.814410924911499, ablated_edges=10671\n",
      "loss.item()=3.5178422927856445, ablated_edges=10802\n",
      "loss.item()=2.441697359085083, ablated_edges=10868\n",
      "Epochs trained:  50\n",
      "Loss: 2.4417\n",
      "Total preserved: 841.5560\n",
      "Edges ablated:  10868\n",
      "Toxic loss:  0.0011845960980281234\n",
      "OWT loss:  10.50698471069336\n",
      "Penalty:  tensor(2.4405, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 1.0\n",
      "\n",
      "\n",
      "loss.item()=5.043083667755127, ablated_edges=10818\n",
      "loss.item()=6.251418113708496, ablated_edges=10997\n",
      "loss.item()=2.376936912536621, ablated_edges=10950\n",
      "loss.item()=2.4905128479003906, ablated_edges=10969\n",
      "loss.item()=2.431356906890869, ablated_edges=10983\n",
      "loss.item()=5.118549346923828, ablated_edges=11017\n",
      "loss.item()=5.998018264770508, ablated_edges=10611\n",
      "loss.item()=3.3699278831481934, ablated_edges=10847\n",
      "loss.item()=3.1299071311950684, ablated_edges=11007\n",
      "loss.item()=2.3216195106506348, ablated_edges=11075\n",
      "Epochs trained:  60\n",
      "Loss: 2.3216\n",
      "Total preserved: 595.2418\n",
      "Edges ablated:  11078\n",
      "Toxic loss:  0.00017653337272349745\n",
      "OWT loss:  10.60989761352539\n",
      "Penalty:  tensor(2.3214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 8.17616463422155e-09\n",
      "\n",
      "\n",
      "loss.item()=2.878173828125, ablated_edges=11005\n",
      "loss.item()=6.820333003997803, ablated_edges=10850\n",
      "loss.item()=3.164989471435547, ablated_edges=10970\n",
      "loss.item()=5.9178996086120605, ablated_edges=10980\n",
      "loss.item()=8.721148490905762, ablated_edges=10992\n",
      "loss.item()=2.7861735820770264, ablated_edges=11065\n",
      "loss.item()=2.450958490371704, ablated_edges=11166\n",
      "loss.item()=8.100809097290039, ablated_edges=11198\n",
      "loss.item()=2.713026523590088, ablated_edges=11158\n",
      "loss.item()=3.436030626296997, ablated_edges=11120\n",
      "Epochs trained:  70\n",
      "Loss: 3.4360\n",
      "Total preserved: 568.0965\n",
      "Edges ablated:  11120\n",
      "Toxic loss:  0.6523579955101013\n",
      "OWT loss:  7.927525043487549\n",
      "Penalty:  tensor(2.7837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.7556838393211365\n",
      "\n",
      "\n",
      "loss.item()=3.029451370239258, ablated_edges=11087\n",
      "loss.item()=3.3395135402679443, ablated_edges=11047\n",
      "loss.item()=2.945695161819458, ablated_edges=11158\n",
      "loss.item()=2.54701828956604, ablated_edges=11181\n",
      "loss.item()=9.877503395080566, ablated_edges=11092\n",
      "loss.item()=5.403866767883301, ablated_edges=11085\n",
      "loss.item()=3.5721893310546875, ablated_edges=11043\n",
      "loss.item()=2.4676544666290283, ablated_edges=11219\n",
      "loss.item()=5.426513671875, ablated_edges=11169\n",
      "loss.item()=2.2157483100891113, ablated_edges=11287\n",
      "Epochs trained:  80\n",
      "Loss: 2.2157\n",
      "Total preserved: 359.6310\n",
      "Edges ablated:  11286\n",
      "Toxic loss:  0.09392533451318741\n",
      "OWT loss:  14.407151222229004\n",
      "Penalty:  tensor(2.1218, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Andrew'], P(Alicia) = 0.11465757340192795\n",
      "\n",
      "\n",
      "loss.item()=4.610217094421387, ablated_edges=11242\n",
      "loss.item()=4.389345645904541, ablated_edges=11240\n",
      "loss.item()=4.841150283813477, ablated_edges=11181\n",
      "loss.item()=3.3376200199127197, ablated_edges=11153\n",
      "loss.item()=3.5982868671417236, ablated_edges=11123\n",
      "loss.item()=3.7962920665740967, ablated_edges=11194\n",
      "loss.item()=16.98774528503418, ablated_edges=11167\n",
      "loss.item()=7.4601521492004395, ablated_edges=11120\n",
      "loss.item()=4.160419464111328, ablated_edges=11065\n",
      "loss.item()=8.103639602661133, ablated_edges=11215\n",
      "Epochs trained:  90\n",
      "Loss: 8.1036\n",
      "Total preserved: 456.2944\n",
      "Edges ablated:  11215\n",
      "Toxic loss:  4.955207824707031\n",
      "OWT loss:  8.780998229980469\n",
      "Penalty:  tensor(3.1484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9995129108428955\n",
      "\n",
      "\n",
      "loss.item()=9.05671215057373, ablated_edges=11240\n",
      "loss.item()=3.1360044479370117, ablated_edges=11243\n",
      "loss.item()=5.008615493774414, ablated_edges=11279\n",
      "loss.item()=10.569381713867188, ablated_edges=11125\n",
      "loss.item()=6.596777439117432, ablated_edges=11101\n",
      "loss.item()=4.363281726837158, ablated_edges=11227\n",
      "loss.item()=2.828944683074951, ablated_edges=11294\n",
      "loss.item()=2.87058687210083, ablated_edges=11311\n",
      "loss.item()=3.4151313304901123, ablated_edges=11283\n",
      "loss.item()=6.500441551208496, ablated_edges=11265\n",
      "Epochs trained:  100\n",
      "Loss: 6.5004\n",
      "Total preserved: 428.8679\n",
      "Edges ablated:  11268\n",
      "Toxic loss:  3.1123850345611572\n",
      "OWT loss:  10.208186149597168\n",
      "Penalty:  tensor(3.3881, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.0881943479180336\n",
      "\n",
      "\n",
      "loss.item()=6.302900314331055, ablated_edges=11204\n",
      "loss.item()=7.708355903625488, ablated_edges=11252\n",
      "loss.item()=3.2059097290039062, ablated_edges=11287\n",
      "loss.item()=3.400064468383789, ablated_edges=11284\n",
      "loss.item()=4.293734550476074, ablated_edges=11264\n",
      "loss.item()=2.938434362411499, ablated_edges=11338\n",
      "loss.item()=2.9611399173736572, ablated_edges=11350\n",
      "loss.item()=4.939145565032959, ablated_edges=11251\n",
      "loss.item()=3.29935884475708, ablated_edges=11296\n",
      "loss.item()=3.9821152687072754, ablated_edges=11273\n",
      "Epochs trained:  110\n",
      "Loss: 3.9821\n",
      "Total preserved: 405.6955\n",
      "Edges ablated:  11273\n",
      "Toxic loss:  0.37142544984817505\n",
      "OWT loss:  8.019838333129883\n",
      "Penalty:  tensor(3.6107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.7416729927062988\n",
      "\n",
      "\n",
      "loss.item()=5.678231239318848, ablated_edges=11289\n",
      "loss.item()=4.260480880737305, ablated_edges=11300\n",
      "loss.item()=6.881374359130859, ablated_edges=11342\n",
      "loss.item()=5.114087104797363, ablated_edges=11350\n",
      "loss.item()=4.156470775604248, ablated_edges=11255\n",
      "loss.item()=4.439241409301758, ablated_edges=11299\n",
      "loss.item()=6.8277740478515625, ablated_edges=11339\n",
      "loss.item()=3.3592278957366943, ablated_edges=11340\n",
      "loss.item()=2.6482791900634766, ablated_edges=11395\n",
      "loss.item()=9.739608764648438, ablated_edges=11399\n",
      "Epochs trained:  120\n",
      "Loss: 9.7396\n",
      "Total preserved: 263.6131\n",
      "Edges ablated:  11399\n",
      "Toxic loss:  7.129838943481445\n",
      "OWT loss:  8.750910758972168\n",
      "Penalty:  tensor(2.6098, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.006058431230485439\n",
      "\n",
      "\n",
      "loss.item()=6.7574968338012695, ablated_edges=11365\n",
      "loss.item()=10.378132820129395, ablated_edges=11345\n",
      "loss.item()=7.412003517150879, ablated_edges=11302\n",
      "loss.item()=3.220329999923706, ablated_edges=11358\n",
      "loss.item()=3.1480464935302734, ablated_edges=11360\n",
      "loss.item()=11.227720260620117, ablated_edges=11269\n",
      "loss.item()=8.253031730651855, ablated_edges=11316\n",
      "loss.item()=8.204963684082031, ablated_edges=11354\n",
      "loss.item()=5.415456771850586, ablated_edges=11334\n",
      "loss.item()=4.9943060874938965, ablated_edges=11319\n",
      "Epochs trained:  130\n",
      "Loss: 4.9943\n",
      "Total preserved: 353.3073\n",
      "Edges ablated:  11319\n",
      "Toxic loss:  1.1432563066482544\n",
      "OWT loss:  7.489620685577393\n",
      "Penalty:  tensor(3.8510, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.8385574817657471\n",
      "\n",
      "\n",
      "loss.item()=6.9442291259765625, ablated_edges=11380\n",
      "loss.item()=3.3415567874908447, ablated_edges=11347\n",
      "loss.item()=3.922614097595215, ablated_edges=11344\n",
      "loss.item()=4.048449993133545, ablated_edges=11326\n",
      "loss.item()=4.278237819671631, ablated_edges=11329\n",
      "loss.item()=8.596158981323242, ablated_edges=11346\n",
      "loss.item()=8.799127578735352, ablated_edges=11410\n",
      "loss.item()=9.98762321472168, ablated_edges=11384\n",
      "loss.item()=7.414649963378906, ablated_edges=11370\n",
      "loss.item()=6.419157981872559, ablated_edges=11395\n",
      "Epochs trained:  140\n",
      "Loss: 6.4192\n",
      "Total preserved: 257.4541\n",
      "Edges ablated:  11396\n",
      "Toxic loss:  3.355454444885254\n",
      "OWT loss:  7.997813701629639\n",
      "Penalty:  tensor(3.0637, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Stephanie'], P(Alicia) = 0.05948441103100777\n",
      "\n",
      "\n",
      "loss.item()=3.569265604019165, ablated_edges=11376\n",
      "loss.item()=4.794463157653809, ablated_edges=11389\n",
      "loss.item()=5.722081184387207, ablated_edges=11400\n",
      "loss.item()=7.6849236488342285, ablated_edges=11351\n",
      "loss.item()=3.480085611343384, ablated_edges=11374\n",
      "loss.item()=5.455089569091797, ablated_edges=11304\n",
      "loss.item()=3.832315683364868, ablated_edges=11368\n",
      "loss.item()=3.9957897663116455, ablated_edges=11359\n",
      "loss.item()=3.3114209175109863, ablated_edges=11404\n",
      "loss.item()=5.948113441467285, ablated_edges=11356\n",
      "Epochs trained:  150\n",
      "Loss: 5.9481\n",
      "Total preserved: 292.5880\n",
      "Edges ablated:  11356\n",
      "Toxic loss:  2.1737284660339355\n",
      "OWT loss:  7.524228572845459\n",
      "Penalty:  tensor(3.7744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.48567208647727966\n",
      "\n",
      "\n",
      "loss.item()=3.47283673286438, ablated_edges=11397\n",
      "loss.item()=4.566978454589844, ablated_edges=11390\n",
      "loss.item()=3.3935139179229736, ablated_edges=11408\n",
      "loss.item()=2.8993093967437744, ablated_edges=11421\n",
      "loss.item()=3.4240846633911133, ablated_edges=11395\n",
      "loss.item()=5.995235443115234, ablated_edges=11350\n",
      "loss.item()=3.975675344467163, ablated_edges=11384\n",
      "loss.item()=4.036617279052734, ablated_edges=11389\n",
      "loss.item()=3.268899440765381, ablated_edges=11414\n",
      "loss.item()=9.365535736083984, ablated_edges=11352\n",
      "Epochs trained:  160\n",
      "Loss: 9.3655\n",
      "Total preserved: 298.2783\n",
      "Edges ablated:  11353\n",
      "Toxic loss:  5.2194671630859375\n",
      "OWT loss:  10.34267520904541\n",
      "Penalty:  tensor(4.1461, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Josh'], P(Alicia) = 0.004551437217742205\n",
      "\n",
      "\n",
      "loss.item()=4.616892337799072, ablated_edges=11328\n",
      "loss.item()=4.041748523712158, ablated_edges=11366\n",
      "loss.item()=4.5740509033203125, ablated_edges=11385\n",
      "loss.item()=2.9774904251098633, ablated_edges=11423\n",
      "loss.item()=8.324371337890625, ablated_edges=11409\n",
      "loss.item()=7.8469672203063965, ablated_edges=11404\n",
      "loss.item()=4.757166862487793, ablated_edges=11413\n",
      "loss.item()=3.3804237842559814, ablated_edges=11424\n",
      "loss.item()=4.364067077636719, ablated_edges=11392\n",
      "loss.item()=7.922146797180176, ablated_edges=11418\n",
      "Epochs trained:  170\n",
      "Loss: 7.9221\n",
      "Total preserved: 238.4478\n",
      "Edges ablated:  11418\n",
      "Toxic loss:  4.3692755699157715\n",
      "OWT loss:  8.432934761047363\n",
      "Penalty:  tensor(3.5529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.01373360026627779\n",
      "\n",
      "\n",
      "loss.item()=9.589149475097656, ablated_edges=11353\n",
      "loss.item()=3.092648983001709, ablated_edges=11437\n",
      "loss.item()=9.909322738647461, ablated_edges=11414\n",
      "loss.item()=10.739590644836426, ablated_edges=11439\n",
      "loss.item()=7.125602722167969, ablated_edges=11406\n",
      "loss.item()=3.867140531539917, ablated_edges=11436\n",
      "loss.item()=3.3442697525024414, ablated_edges=11441\n",
      "loss.item()=7.873605251312256, ablated_edges=11414\n",
      "loss.item()=8.757461547851562, ablated_edges=11420\n",
      "loss.item()=6.078608512878418, ablated_edges=11465\n",
      "Epochs trained:  180\n",
      "Loss: 6.0786\n",
      "Total preserved: 181.8776\n",
      "Edges ablated:  11464\n",
      "Toxic loss:  3.186753749847412\n",
      "OWT loss:  8.437674522399902\n",
      "Penalty:  tensor(2.8919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.003479000646620989\n",
      "\n",
      "\n",
      "loss.item()=5.89391565322876, ablated_edges=11471\n",
      "loss.item()=4.424604415893555, ablated_edges=11478\n",
      "loss.item()=3.979879856109619, ablated_edges=11478\n",
      "loss.item()=6.665318965911865, ablated_edges=11478\n",
      "loss.item()=3.7068357467651367, ablated_edges=11472\n",
      "loss.item()=5.107507228851318, ablated_edges=11457\n",
      "loss.item()=3.405787467956543, ablated_edges=11451\n",
      "loss.item()=9.894067764282227, ablated_edges=11452\n",
      "loss.item()=7.242390155792236, ablated_edges=11460\n",
      "loss.item()=9.038299560546875, ablated_edges=11479\n",
      "Epochs trained:  190\n",
      "Loss: 9.0383\n",
      "Total preserved: 187.4810\n",
      "Edges ablated:  11479\n",
      "Toxic loss:  5.869870185852051\n",
      "OWT loss:  9.146526336669922\n",
      "Penalty:  tensor(3.1684, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.02190464921295643\n",
      "\n",
      "\n",
      "loss.item()=3.2692477703094482, ablated_edges=11460\n",
      "loss.item()=9.32164478302002, ablated_edges=11470\n",
      "loss.item()=7.642899036407471, ablated_edges=11486\n",
      "loss.item()=8.282837867736816, ablated_edges=11489\n",
      "loss.item()=12.46438980102539, ablated_edges=11486\n",
      "loss.item()=7.229970455169678, ablated_edges=11374\n",
      "loss.item()=11.564188957214355, ablated_edges=11433\n",
      "loss.item()=14.523955345153809, ablated_edges=11440\n",
      "loss.item()=4.203866004943848, ablated_edges=11420\n",
      "loss.item()=11.156993865966797, ablated_edges=11388\n",
      "Epochs trained:  200\n",
      "Loss: 11.1570\n",
      "Total preserved: 257.6039\n",
      "Edges ablated:  11390\n",
      "Toxic loss:  6.545882701873779\n",
      "OWT loss:  8.044315338134766\n",
      "Penalty:  tensor(4.6111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.999439537525177\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toxic_batch_size = 1 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 1\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "means_ioi = True\n",
    "if means_ioi:\n",
    "    with open(\"data/gpt2_ioi_abc_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "else:\n",
    "    with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "\n",
    "model = load_demo_gpt2(means=means)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 20 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "# alpha = 0.2 # free\n",
    "alpha = 1\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 100\n",
    "max_steps_per_epoch = 100\n",
    "\n",
    "\n",
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = regularization_strength * penalty + alpha * tox_loss # inverted loss function #+ owt_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "                        # evaluate model on \"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to \"\n",
    "            with torch.no_grad():\n",
    "                test_ioi_sentence = \"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\"\n",
    "                correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "                other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "                test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "                generation = model(test_ioi_tokens)[0][:, -1]\n",
    "                probs = torch.softmax(generation, dim=-1)\n",
    "                print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")\n",
    "\n",
    "            print(\"\\n\")\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "\n",
    "    epochs_left = -1\n",
    "    # epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    # log_every = int(input('set log frequency'))\n",
    "    # edge_threshold = int(input('set edge threshold'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/alternative_sufficient_masks_params_dict_lambda=1_{means_ioi=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train mask over known circuit\n",
    "Train mask over the circuit from the paper, as given by a run of ACDC++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed': tensor([]),\n",
       " 'a0.0': tensor([1.]),\n",
       " 'a0.1': tensor([0.]),\n",
       " 'a0.2': tensor([1.]),\n",
       " 'a0.3': tensor([0.]),\n",
       " 'a0.4': tensor([1.]),\n",
       " 'a0.5': tensor([0.]),\n",
       " 'a0.6': tensor([1.]),\n",
       " 'a0.7': tensor([1.]),\n",
       " 'a0.8': tensor([1.]),\n",
       " 'a0.9': tensor([1.]),\n",
       " 'a0.10': tensor([0.]),\n",
       " 'a0.11': tensor([1.]),\n",
       " 'm0': tensor([0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.]),\n",
       " 'a1.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm2': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1.]),\n",
       " 'a3.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0.]),\n",
       " 'a3.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'm3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "         1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.]),\n",
       " 'a5.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.]),\n",
       " 'a5.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         0., 1., 0., 0., 0., 1.]),\n",
       " 'a6.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 1.]),\n",
       " 'a6.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.9': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         0., 1., 1., 1., 1., 1., 0.]),\n",
       " 'a6.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
       "         1.]),\n",
       " 'a7.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.9': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0.]),\n",
       " 'a7.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'm7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.]),\n",
       " 'a8.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1.]),\n",
       " 'a9.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]),\n",
       " 'a9.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0.]),\n",
       " 'a9.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]),\n",
       " 'a9.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1.]),\n",
       " 'a10.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 1.]),\n",
       " 'a10.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.7': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 1.]),\n",
       " 'a10.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'm10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.10': tensor([1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.]),\n",
       " 'a11.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'output': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mask_utils import get_nodes_and_edges\n",
    "with open(\"models/acdcpp_mask_params.pkl\", \"rb\") as f:\n",
    "    acdc_mask_params = pickle.load(f)\n",
    "\n",
    "_, _, acdc_Edges, acdc_mask_dict = get_nodes_and_edges(mask_params=acdc_mask_params)\n",
    "acdc_mask_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([157])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acdc_mask_dict['output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22863/1329432646.py:48: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10673f0ad7234b04a3294330b8b5f2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=1.5056736469268799, ablated_edges=80\n",
      "loss.item()=-0.500084400177002, ablated_edges=87\n",
      "loss.item()=-0.0825948715209961, ablated_edges=87\n",
      "loss.item()=0.15455102920532227, ablated_edges=88\n",
      "loss.item()=0.7475423812866211, ablated_edges=95\n",
      "loss.item()=0.7848529815673828, ablated_edges=94\n",
      "loss.item()=0.06402826309204102, ablated_edges=98\n",
      "loss.item()=-0.9390511512756348, ablated_edges=96\n",
      "loss.item()=0.6501531600952148, ablated_edges=98\n",
      "loss.item()=0.626798152923584, ablated_edges=98\n",
      "Epochs trained:  10\n",
      "Loss: 0.6268\n",
      "Total preserved: 11512.8232\n",
      "Edges ablated:  98\n",
      "Toxic loss:  23.036602020263672\n",
      "OWT loss:  5.234118461608887\n",
      "Penalty:  0\n",
      "\n",
      "\n",
      "loss.item()=-3.0186614990234375, ablated_edges=103\n",
      "loss.item()=-0.8208189010620117, ablated_edges=93\n",
      "loss.item()=0.038724422454833984, ablated_edges=94\n",
      "loss.item()=-0.08931589126586914, ablated_edges=95\n",
      "loss.item()=-0.5814499855041504, ablated_edges=98\n",
      "loss.item()=-0.1508636474609375, ablated_edges=102\n",
      "loss.item()=2.3883941173553467, ablated_edges=98\n",
      "loss.item()=1.5119729042053223, ablated_edges=96\n",
      "loss.item()=-0.7001581192016602, ablated_edges=100\n",
      "loss.item()=-0.5811986923217773, ablated_edges=97\n",
      "Epochs trained:  20\n",
      "Loss: -0.5812\n",
      "Total preserved: 11511.8525\n",
      "Edges ablated:  97\n",
      "Toxic loss:  27.467458724975586\n",
      "OWT loss:  4.912292957305908\n",
      "Penalty:  0\n",
      "\n",
      "\n",
      "loss.item()=1.1091012954711914, ablated_edges=100\n",
      "loss.item()=-0.7198200225830078, ablated_edges=98\n",
      "loss.item()=-1.066877841949463, ablated_edges=100\n",
      "loss.item()=-4.244425296783447, ablated_edges=105\n",
      "loss.item()=-5.552443981170654, ablated_edges=98\n",
      "loss.item()=-6.534358024597168, ablated_edges=91\n",
      "loss.item()=-7.882603168487549, ablated_edges=89\n",
      "loss.item()=-8.790934562683105, ablated_edges=88\n",
      "loss.item()=-8.656744003295898, ablated_edges=87\n",
      "loss.item()=-8.970634460449219, ablated_edges=89\n",
      "Epochs trained:  30\n",
      "Loss: -8.9706\n",
      "Total preserved: 11524.6582\n",
      "Edges ablated:  89\n",
      "Toxic loss:  19.04596519470215\n",
      "OWT loss:  5.210751056671143\n",
      "Penalty:  tensor(10.3722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-10.965009689331055, ablated_edges=88\n",
      "loss.item()=-13.547128677368164, ablated_edges=90\n",
      "loss.item()=-13.717792510986328, ablated_edges=87\n",
      "loss.item()=-16.68755340576172, ablated_edges=86\n",
      "loss.item()=-16.444446563720703, ablated_edges=82\n",
      "loss.item()=-16.821895599365234, ablated_edges=82\n",
      "loss.item()=-17.48927116394043, ablated_edges=80\n",
      "loss.item()=-21.36284828186035, ablated_edges=81\n",
      "loss.item()=-21.52306365966797, ablated_edges=80\n",
      "loss.item()=-22.231990814208984, ablated_edges=77\n",
      "Epochs trained:  40\n",
      "Loss: -22.2320\n",
      "Total preserved: 11534.5127\n",
      "Edges ablated:  78\n",
      "Toxic loss:  26.378662109375\n",
      "OWT loss:  4.959319114685059\n",
      "Penalty:  tensor(21.9156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-22.69232940673828, ablated_edges=77\n",
      "loss.item()=-23.534677505493164, ablated_edges=78\n",
      "loss.item()=-25.442670822143555, ablated_edges=76\n",
      "loss.item()=-27.155630111694336, ablated_edges=79\n",
      "loss.item()=-28.782812118530273, ablated_edges=76\n",
      "loss.item()=-28.164342880249023, ablated_edges=77\n",
      "loss.item()=-29.027385711669922, ablated_edges=72\n",
      "loss.item()=-30.837371826171875, ablated_edges=80\n",
      "loss.item()=-32.40890121459961, ablated_edges=72\n",
      "loss.item()=-32.99948501586914, ablated_edges=72\n",
      "Epochs trained:  50\n",
      "Loss: -32.9995\n",
      "Total preserved: 11537.1387\n",
      "Edges ablated:  72\n",
      "Toxic loss:  25.70037269592285\n",
      "OWT loss:  5.598297119140625\n",
      "Penalty:  tensor(33.4577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-33.814735412597656, ablated_edges=70\n",
      "loss.item()=-34.977874755859375, ablated_edges=74\n",
      "loss.item()=-38.72045135498047, ablated_edges=76\n",
      "loss.item()=-38.59954833984375, ablated_edges=72\n",
      "loss.item()=-37.44197082519531, ablated_edges=75\n",
      "loss.item()=-40.775421142578125, ablated_edges=69\n",
      "loss.item()=-39.43312454223633, ablated_edges=69\n",
      "loss.item()=-43.922523498535156, ablated_edges=70\n",
      "loss.item()=-44.29679870605469, ablated_edges=63\n",
      "loss.item()=-44.6660270690918, ablated_edges=69\n",
      "Epochs trained:  60\n",
      "Loss: -44.6660\n",
      "Total preserved: 11543.3457\n",
      "Edges ablated:  68\n",
      "Toxic loss:  24.17945098876953\n",
      "OWT loss:  5.188914775848389\n",
      "Penalty:  tensor(45.0191, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-45.43226623535156, ablated_edges=68\n",
      "loss.item()=-47.554931640625, ablated_edges=67\n",
      "loss.item()=-48.48405456542969, ablated_edges=70\n",
      "loss.item()=-49.8265495300293, ablated_edges=67\n",
      "loss.item()=-51.74678421020508, ablated_edges=66\n",
      "loss.item()=-51.73463821411133, ablated_edges=69\n",
      "loss.item()=-52.265323638916016, ablated_edges=70\n",
      "loss.item()=-54.762516021728516, ablated_edges=71\n",
      "loss.item()=-55.7078971862793, ablated_edges=68\n",
      "loss.item()=-56.24128723144531, ablated_edges=67\n",
      "Epochs trained:  70\n",
      "Loss: -56.2413\n",
      "Total preserved: 11541.6113\n",
      "Edges ablated:  67\n",
      "Toxic loss:  23.93475341796875\n",
      "OWT loss:  5.099553108215332\n",
      "Penalty:  tensor(56.5539, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-57.00583267211914, ablated_edges=69\n",
      "loss.item()=-57.7839469909668, ablated_edges=70\n",
      "loss.item()=-58.821163177490234, ablated_edges=67\n",
      "loss.item()=-61.0757942199707, ablated_edges=66\n",
      "loss.item()=-63.463558197021484, ablated_edges=66\n",
      "loss.item()=-64.74162292480469, ablated_edges=62\n",
      "loss.item()=-65.59223175048828, ablated_edges=61\n",
      "loss.item()=-65.37863159179688, ablated_edges=66\n",
      "loss.item()=-66.93404388427734, ablated_edges=61\n",
      "loss.item()=-67.35196685791016, ablated_edges=66\n",
      "Epochs trained:  80\n",
      "Loss: -67.3520\n",
      "Total preserved: 11547.4160\n",
      "Edges ablated:  64\n",
      "Toxic loss:  22.907915115356445\n",
      "OWT loss:  5.359364986419678\n",
      "Penalty:  tensor(68.1298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-70.22889709472656, ablated_edges=59\n",
      "loss.item()=-69.60752868652344, ablated_edges=62\n",
      "loss.item()=-71.21782684326172, ablated_edges=64\n",
      "loss.item()=-72.4986801147461, ablated_edges=64\n",
      "loss.item()=-74.5802001953125, ablated_edges=66\n",
      "loss.item()=-74.36848449707031, ablated_edges=65\n",
      "loss.item()=-76.47356414794922, ablated_edges=63\n",
      "loss.item()=-78.4028549194336, ablated_edges=63\n",
      "loss.item()=-77.62654876708984, ablated_edges=66\n",
      "loss.item()=-80.140625, ablated_edges=64\n",
      "Epochs trained:  90\n",
      "Loss: -80.1406\n",
      "Total preserved: 11547.1406\n",
      "Edges ablated:  64\n",
      "Toxic loss:  23.180822372436523\n",
      "OWT loss:  4.170817852020264\n",
      "Penalty:  tensor(79.6753, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-82.50818634033203, ablated_edges=66\n",
      "loss.item()=-81.01814270019531, ablated_edges=64\n",
      "loss.item()=-83.3720474243164, ablated_edges=66\n",
      "loss.item()=-83.98469543457031, ablated_edges=58\n",
      "loss.item()=-87.44886016845703, ablated_edges=61\n",
      "loss.item()=-86.34146881103516, ablated_edges=58\n",
      "loss.item()=-85.54612731933594, ablated_edges=57\n",
      "loss.item()=-90.238525390625, ablated_edges=54\n",
      "loss.item()=-90.92143249511719, ablated_edges=54\n",
      "loss.item()=-92.84722137451172, ablated_edges=57\n",
      "Epochs trained:  100\n",
      "Loss: -92.8472\n",
      "Total preserved: 11554.1641\n",
      "Edges ablated:  57\n",
      "Toxic loss:  33.43305587768555\n",
      "OWT loss:  5.11730432510376\n",
      "Penalty:  tensor(91.2779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-91.27748107910156, ablated_edges=59\n",
      "loss.item()=-93.88461303710938, ablated_edges=54\n",
      "loss.item()=-94.77798461914062, ablated_edges=50\n",
      "loss.item()=-94.69718933105469, ablated_edges=52\n",
      "loss.item()=-95.52283477783203, ablated_edges=53\n",
      "loss.item()=-97.91181182861328, ablated_edges=58\n",
      "loss.item()=-100.29170989990234, ablated_edges=53\n",
      "loss.item()=-101.28544616699219, ablated_edges=52\n",
      "loss.item()=-99.9444808959961, ablated_edges=54\n",
      "loss.item()=-102.86627197265625, ablated_edges=53\n",
      "Epochs trained:  110\n",
      "Loss: -102.8663\n",
      "Total preserved: 11557.9971\n",
      "Edges ablated:  53\n",
      "Toxic loss:  28.86589813232422\n",
      "OWT loss:  5.773064613342285\n",
      "Penalty:  tensor(102.8662, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-103.58384704589844, ablated_edges=52\n",
      "loss.item()=-105.84606170654297, ablated_edges=52\n",
      "loss.item()=-107.2335433959961, ablated_edges=53\n",
      "loss.item()=-106.85431671142578, ablated_edges=53\n",
      "loss.item()=-109.91695404052734, ablated_edges=52\n",
      "loss.item()=-110.24185943603516, ablated_edges=54\n",
      "loss.item()=-111.40071105957031, ablated_edges=50\n",
      "loss.item()=-112.56986999511719, ablated_edges=50\n",
      "loss.item()=-113.40050506591797, ablated_edges=53\n",
      "loss.item()=-113.72864532470703, ablated_edges=52\n",
      "Epochs trained:  120\n",
      "Loss: -113.7286\n",
      "Total preserved: 11557.8770\n",
      "Edges ablated:  52\n",
      "Toxic loss:  28.943449020385742\n",
      "OWT loss:  6.483026027679443\n",
      "Penalty:  tensor(114.4230, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-116.54255676269531, ablated_edges=53\n",
      "loss.item()=-116.79327392578125, ablated_edges=51\n",
      "loss.item()=-115.19482421875, ablated_edges=51\n",
      "loss.item()=-118.75401306152344, ablated_edges=53\n",
      "loss.item()=-122.14277648925781, ablated_edges=49\n",
      "loss.item()=-121.06953430175781, ablated_edges=48\n",
      "loss.item()=-122.05601501464844, ablated_edges=49\n",
      "loss.item()=-123.47509002685547, ablated_edges=48\n",
      "loss.item()=-124.31399536132812, ablated_edges=50\n",
      "loss.item()=-127.36456298828125, ablated_edges=46\n",
      "Epochs trained:  130\n",
      "Loss: -127.3646\n",
      "Total preserved: 11562.5859\n",
      "Edges ablated:  46\n",
      "Toxic loss:  32.48900604248047\n",
      "OWT loss:  5.165423393249512\n",
      "Penalty:  tensor(126.0322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-126.47238159179688, ablated_edges=47\n",
      "loss.item()=-128.5781707763672, ablated_edges=49\n",
      "loss.item()=-130.5258331298828, ablated_edges=46\n",
      "loss.item()=-129.74996948242188, ablated_edges=47\n",
      "loss.item()=-132.48599243164062, ablated_edges=48\n",
      "loss.item()=-133.76837158203125, ablated_edges=47\n",
      "loss.item()=-132.95957946777344, ablated_edges=46\n",
      "loss.item()=-133.96261596679688, ablated_edges=45\n",
      "loss.item()=-135.52638244628906, ablated_edges=47\n",
      "loss.item()=-137.492431640625, ablated_edges=50\n",
      "Epochs trained:  140\n",
      "Loss: -137.4924\n",
      "Total preserved: 11560.9385\n",
      "Edges ablated:  50\n",
      "Toxic loss:  27.15222930908203\n",
      "OWT loss:  5.513188362121582\n",
      "Penalty:  tensor(137.5752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-138.09088134765625, ablated_edges=46\n",
      "loss.item()=-139.6414031982422, ablated_edges=46\n",
      "loss.item()=-140.9293975830078, ablated_edges=45\n",
      "loss.item()=-141.51370239257812, ablated_edges=49\n",
      "loss.item()=-144.06930541992188, ablated_edges=46\n",
      "loss.item()=-143.3538055419922, ablated_edges=47\n",
      "loss.item()=-146.1291046142578, ablated_edges=44\n",
      "loss.item()=-146.28192138671875, ablated_edges=47\n",
      "loss.item()=-148.14944458007812, ablated_edges=48\n",
      "loss.item()=-150.17247009277344, ablated_edges=50\n",
      "Epochs trained:  150\n",
      "Loss: -150.1725\n",
      "Total preserved: 11561.4805\n",
      "Edges ablated:  50\n",
      "Toxic loss:  33.0401725769043\n",
      "OWT loss:  5.578667640686035\n",
      "Penalty:  tensor(149.1431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-152.55580139160156, ablated_edges=49\n",
      "loss.item()=-151.85586547851562, ablated_edges=49\n",
      "loss.item()=-153.99522399902344, ablated_edges=46\n",
      "loss.item()=-153.05007934570312, ablated_edges=47\n",
      "loss.item()=-155.26657104492188, ablated_edges=45\n",
      "loss.item()=-155.52505493164062, ablated_edges=49\n",
      "loss.item()=-158.27174377441406, ablated_edges=46\n",
      "loss.item()=-158.82046508789062, ablated_edges=43\n",
      "loss.item()=-159.7530059814453, ablated_edges=48\n",
      "loss.item()=-161.92526245117188, ablated_edges=46\n",
      "Epochs trained:  160\n",
      "Loss: -161.9253\n",
      "Total preserved: 11562.2822\n",
      "Edges ablated:  46\n",
      "Toxic loss:  33.64940643310547\n",
      "OWT loss:  5.520321846008301\n",
      "Penalty:  tensor(160.7157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-161.7630157470703, ablated_edges=43\n",
      "loss.item()=-162.72824096679688, ablated_edges=43\n",
      "loss.item()=-163.37709045410156, ablated_edges=47\n",
      "loss.item()=-164.8791961669922, ablated_edges=44\n",
      "loss.item()=-167.94241333007812, ablated_edges=45\n",
      "loss.item()=-168.17257690429688, ablated_edges=46\n",
      "loss.item()=-168.0404052734375, ablated_edges=47\n",
      "loss.item()=-169.7171630859375, ablated_edges=49\n",
      "loss.item()=-171.64768981933594, ablated_edges=46\n",
      "loss.item()=-174.03814697265625, ablated_edges=43\n",
      "Epochs trained:  170\n",
      "Loss: -174.0381\n",
      "Total preserved: 11564.3203\n",
      "Edges ablated:  43\n",
      "Toxic loss:  39.49759292602539\n",
      "OWT loss:  6.16975736618042\n",
      "Penalty:  tensor(172.3084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-174.89840698242188, ablated_edges=45\n",
      "loss.item()=-174.6654510498047, ablated_edges=48\n",
      "loss.item()=-174.72088623046875, ablated_edges=45\n",
      "loss.item()=-178.4546661376953, ablated_edges=46\n",
      "loss.item()=-177.6720428466797, ablated_edges=47\n",
      "loss.item()=-179.65830993652344, ablated_edges=48\n",
      "loss.item()=-179.35630798339844, ablated_edges=41\n",
      "loss.item()=-183.06385803222656, ablated_edges=44\n",
      "loss.item()=-184.6969757080078, ablated_edges=46\n",
      "loss.item()=-184.05078125, ablated_edges=44\n",
      "Epochs trained:  180\n",
      "Loss: -184.0508\n",
      "Total preserved: 11566.2373\n",
      "Edges ablated:  44\n",
      "Toxic loss:  28.797325134277344\n",
      "OWT loss:  5.611840724945068\n",
      "Penalty:  tensor(183.9032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-184.4897003173828, ablated_edges=47\n",
      "loss.item()=-185.37271118164062, ablated_edges=46\n",
      "loss.item()=-186.9844207763672, ablated_edges=43\n",
      "loss.item()=-188.4407501220703, ablated_edges=46\n",
      "loss.item()=-189.3105010986328, ablated_edges=44\n",
      "loss.item()=-191.02035522460938, ablated_edges=44\n",
      "loss.item()=-191.65814208984375, ablated_edges=43\n",
      "loss.item()=-191.11868286132812, ablated_edges=45\n",
      "loss.item()=-195.22610473632812, ablated_edges=44\n",
      "loss.item()=-195.29010009765625, ablated_edges=44\n",
      "Epochs trained:  190\n",
      "Loss: -195.2901\n",
      "Total preserved: 11565.8291\n",
      "Edges ablated:  44\n",
      "Toxic loss:  25.891029357910156\n",
      "OWT loss:  5.3506364822387695\n",
      "Penalty:  tensor(195.4625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-197.07337951660156, ablated_edges=46\n",
      "loss.item()=-197.45147705078125, ablated_edges=47\n",
      "loss.item()=-199.31268310546875, ablated_edges=44\n",
      "loss.item()=-198.38023376464844, ablated_edges=43\n",
      "loss.item()=-201.58419799804688, ablated_edges=44\n",
      "loss.item()=-199.99661254882812, ablated_edges=44\n",
      "loss.item()=-202.88490295410156, ablated_edges=46\n",
      "loss.item()=-203.29649353027344, ablated_edges=44\n",
      "loss.item()=-205.46502685546875, ablated_edges=41\n",
      "loss.item()=-207.99468994140625, ablated_edges=43\n",
      "Epochs trained:  200\n",
      "Loss: -207.9947\n",
      "Total preserved: 11567.3525\n",
      "Edges ablated:  43\n",
      "Toxic loss:  30.689029693603516\n",
      "OWT loss:  5.198721885681152\n",
      "Penalty:  tensor(207.0556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m     prev_params \u001b[39m=\u001b[39m mask_params\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m epochs_left \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39minput\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mcontinue training for this number of epochs: \u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m log_every \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(\u001b[39minput\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mset log frequency\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m edge_threshold \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39minput\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mset edge threshold\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "toxic_batch_size = 1 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 5\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "    means = pickle.load(f)[0][0]\n",
    "\n",
    "model = load_demo_gpt2(means=means, mask_dict_superset=acdc_mask_dict)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 20 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 0.2 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 0\n",
    "max_steps_per_epoch = 100\n",
    "\n",
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "            # print(f\"{ablated_edges=}\")\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = -1 * (regularization_strength * penalty + alpha * tox_loss) + owt_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            print(\"\\n\")\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    log_every = int(input('set log frequency'))\n",
    "    edge_threshold = int(input('set edge threshold'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/alternative_acdc_cb_subset_mask_params_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model before and after circuit breaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/ioi_sentences_test.pkl\", \"rb\") as f:\n",
    "    ioi_sentences_test = pickle.load(f)\n",
    "    # ioi_sentences_test = [t[2] for t in ioi_sentences_test]\n",
    "\n",
    "with open(\"data/eval_uniform.pkl\", \"rb\") as f:\n",
    "    uniform_samples = pickle.load(f)\n",
    "    uniform_sentences = [t[2] for t in uniform_samples]\n",
    "\n",
    "original_model = load_demo_gpt2(means=False)\n",
    "\n",
    "# with open(\"models/masked_gpt2_mean_ablation_v6.pkl\", \"rb\") as f:\n",
    "#     model.state_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on an ioi_sentence\n",
    "ioi_sentence = ioi_sentences_test[0]\n",
    "print(ioi_sentence)\n",
    "# ioi_tokens = tokenizer(ioi_sentence, return_tensors='pt').input_ids.to('cuda')\n",
    "\n",
    "original_model.eval()\n",
    "original_model.to('cuda')\n",
    "def get_last_token(model, prompt, topk=5):\n",
    "    # generate last token\n",
    "    tokens = tokenizer(prompt, return_tensors='pt').input_ids[:, :-1]\n",
    "\n",
    "    # generate one token, decode original_model(ioi_tokens[:, :-1])\n",
    "    model_outputs = model(tokens)[0]\n",
    "    model_outputs = model_outputs.squeeze(0)[-1]\n",
    "    probs = torch.nn.functional.softmax(model_outputs, dim=-1)\n",
    "\n",
    "    topk_outputs = torch.topk(model_outputs, topk)\n",
    "    topk_tokens = topk_outputs.indices\n",
    "    topk_probs = probs[topk_outputs.indices]\n",
    "    \n",
    "    # decode tokens\n",
    "    for i in range(topk):\n",
    "        print(f\"{tokenizer.decode(topk_tokens[i].unsqueeze(0))}, probability of {topk_probs[i]}\")\n",
    "    topk_tokens_decoded = tokenizer.batch_decode(topk_tokens)\n",
    "    return topk_tokens_decoded, topk_probs\n",
    "\n",
    "print(\"Before ablation\")\n",
    "_ = get_last_token(original_model, ioi_sentence)\n",
    "print()\n",
    "print()\n",
    "print(\"After ablation\")\n",
    "_ = get_last_token(model, ioi_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try on uniform samples\n",
    "for idx in range(3):\n",
    "    print(uniform_sentences[idx])\n",
    "    print(\"Before ablation\")\n",
    "    _ = get_last_token(original_model, uniform_sentences[idx])\n",
    "    print()\n",
    "    print(\"After ablation\")\n",
    "    _ = get_last_token(model, uniform_sentences[idx])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize mask\n",
    "Create the computational graphs in edge attribution patching paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate which nodes will be in the graph\n",
    "connected_nodes = set()\n",
    "# add embed node at position\n",
    "# connected_nodes.add((-1, \"embed\"))\n",
    "n_heads = 12\n",
    "n_layers = 12\n",
    "\n",
    "# associate each node with a position\n",
    "all_possible_nodes = [(-1, \"embed\")]\n",
    "mask_dict = {}\n",
    "# empty tensor\n",
    "mask_dict[\"embed\"] = torch.zeros(size=(0,))\n",
    "for idx in range(len(mask_params)):\n",
    "    if \"attention\" in param_names[idx]:\n",
    "        layer = int(param_names[idx].split(\".\")[1])\n",
    "        for i in range(n_heads):\n",
    "            all_possible_nodes.append((layer, f\"a{layer}.{i}\"))\n",
    "            mask_dict[f\"a{layer}.{i}\"] = mask_params[idx][:,i].detach().cpu()\n",
    "    elif \"mlp\" in param_names[idx]:\n",
    "        layer = int(param_names[idx].split(\".\")[1])\n",
    "        all_possible_nodes.append((layer, f\"m{layer}\"))\n",
    "        mask_dict[f\"m{layer}\"] = mask_params[idx].detach().cpu()\n",
    "all_possible_nodes.append((n_heads, \"output\"))\n",
    "mask_dict[\"output\"] = mask_params[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate where edges are based on the mask\n",
    "# Edge between node i and node j if mask_dict[i][all_possible_nodes.index(j)] == 0\n",
    "sufficient = True\n",
    "\n",
    "edges = []\n",
    "for i in range(len(all_possible_nodes)):\n",
    "    for j in range(len(all_possible_nodes)):\n",
    "        j_index = all_possible_nodes.index(all_possible_nodes[j])\n",
    "        if j_index < len(mask_dict[all_possible_nodes[i][1]]) and mask_dict[all_possible_nodes[i][1]][all_possible_nodes.index(all_possible_nodes[j])] == (1 if sufficient else 0):\n",
    "            edges.append((all_possible_nodes[i], all_possible_nodes[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aligned_graph(all_possible_nodes, edges):\n",
    "    G = pgv.AGraph(strict=False, directed=True)\n",
    "\n",
    "    # Find the maximum layer number for adjusting the graph\n",
    "    max_layer = max(layer for layer, _ in all_possible_nodes if isinstance(layer, int))\n",
    "    nodes_with_edges = set([node for edge in edges for node in edge])\n",
    "    print(nodes_with_edges)\n",
    "    # Add nodes and edges to the graph\n",
    "    # for node in all_possible_nodes:\n",
    "    #     if node in [edge[0] for edge in edges] or node in [edge[1] for edge in edges]:\n",
    "    #         G.add_node(node[1], layer=str(max_layer - node[0]))\n",
    "\n",
    "    for edge in edges:\n",
    "        G.add_edge(edge[1][1], edge[0][1])\n",
    "\n",
    "    # Create subgraphs to ensure nodes of the same layer have the same rank\n",
    "    for layer in range(max_layer, -2, -1):\n",
    "        with G.subgraph(name=f'cluster_{layer}') as s:\n",
    "            s.graph_attr['rank'] = 'same'\n",
    "            for node in nodes_with_edges:\n",
    "                if node[0] == layer:\n",
    "                    s.add_node(node[1])\n",
    "\n",
    "    # Apply layout and render the graph\n",
    "    G.layout(prog='dot')\n",
    "    G.draw('aligned_graph.png')\n",
    "    return Image('aligned_graph.png')\n",
    "\n",
    "# Call the function with your nodes and edges\n",
    "flipped_graph_image = create_aligned_graph(all_possible_nodes, edges)\n",
    "\n",
    "# To display the graph in Jupyter Notebook\n",
    "flipped_graph_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
