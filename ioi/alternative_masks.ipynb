{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train different kinds of masks over IOI edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir(\"/data/phillip_guo/circuit-breaking/ioi/\")\n",
    "from models import load_gpt2_weights, load_demo_gpt2, tokenizer\n",
    "from data import retrieve_toxic_data, retrieve_owt_data, retrieve_toxic_data_low_loss, retrieve_toxic_filtered_data, FILTER_DEMO_LEN, CONTEXT_LENGTH\n",
    "from inference import infer_batch_with_owt, infer_batch, prepare_fixed_demo, criterion\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import pickle\n",
    "import datasets\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from itertools import cycle\n",
    "# from eval import evaluate_model\n",
    "from data import batch_text_to_tokens\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train params of mask\n",
    "Train without the original D_train loss term (only mask loss and IOI data loss)\n",
    "Finds necessary (but not sufficient) edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_batch_size = 10 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 1\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "\n",
    "template_type = \"double\"\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None, template_type=template_type)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "# with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "#     means = pickle.load(f)[0][0]\n",
    "means_ioi = True\n",
    "if means_ioi:\n",
    "    with open(\"data/gpt2_ioi_abc_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "else:\n",
    "    with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "\n",
    "model = load_demo_gpt2(means=means)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 50 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 1 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 100\n",
    "max_steps_per_epoch = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_188436/2478911762.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f326840c99054a4ab31cdbf1fe653b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=-108.40061950683594, ablated_edges=3668\n",
      "loss.item()=-114.56974029541016, ablated_edges=4250\n",
      "loss.item()=-114.6631851196289, ablated_edges=4626\n",
      "loss.item()=-116.3541259765625, ablated_edges=4958\n",
      "loss.item()=-120.3824234008789, ablated_edges=5236\n",
      "loss.item()=-122.29742431640625, ablated_edges=5419\n",
      "loss.item()=-121.7772216796875, ablated_edges=5520\n",
      "loss.item()=-124.20976257324219, ablated_edges=5663\n",
      "loss.item()=-127.839111328125, ablated_edges=5759\n",
      "loss.item()=-124.56654357910156, ablated_edges=5825\n",
      "Epochs trained:  10\n",
      "Loss: -124.5665\n",
      "Total preserved: 5801.5181\n",
      "Edges ablated:  5825\n",
      "Toxic loss:  124.56654357910156\n",
      "OWT loss:  11.26550579071045\n",
      "Penalty:  0\n",
      "Best Token: [' go'], P(Alicia) = 5.45655396290165e-27, logit diff = -5.302750587463379\n",
      "Best Token: [' go'], P(Alicia) = 4.30406390637422e-28, logit diff = -9.804237365722656\n",
      "\n",
      "\n",
      "loss.item()=-124.6899185180664, ablated_edges=5858\n",
      "loss.item()=-125.3144302368164, ablated_edges=5947\n",
      "loss.item()=-125.79304504394531, ablated_edges=5943\n",
      "loss.item()=-128.12704467773438, ablated_edges=5983\n",
      "loss.item()=-124.62074279785156, ablated_edges=6020\n",
      "loss.item()=-126.07161712646484, ablated_edges=6048\n",
      "loss.item()=-126.05900573730469, ablated_edges=6046\n",
      "loss.item()=-124.06974029541016, ablated_edges=6080\n",
      "loss.item()=-127.4010238647461, ablated_edges=6083\n",
      "loss.item()=-125.58274841308594, ablated_edges=6118\n",
      "Epochs trained:  20\n",
      "Loss: -125.5827\n",
      "Total preserved: 5543.2427\n",
      "Edges ablated:  6118\n",
      "Toxic loss:  125.58274841308594\n",
      "OWT loss:  10.634827613830566\n",
      "Penalty:  0\n",
      "Best Token: [' go'], P(Alicia) = 2.9384797167558257e-18, logit diff = 5.975656509399414\n",
      "Best Token: [' go'], P(Alicia) = 3.5725017849434616e-21, logit diff = -6.5491790771484375\n",
      "\n",
      "\n",
      "loss.item()=-125.14933776855469, ablated_edges=6131\n",
      "loss.item()=-126.8728256225586, ablated_edges=6027\n",
      "loss.item()=-127.12118530273438, ablated_edges=5829\n",
      "loss.item()=-128.84030151367188, ablated_edges=5564\n",
      "loss.item()=-126.78697204589844, ablated_edges=5214\n",
      "loss.item()=-127.9173583984375, ablated_edges=4914\n",
      "loss.item()=-129.8364715576172, ablated_edges=4599\n",
      "loss.item()=-133.5966033935547, ablated_edges=4335\n",
      "loss.item()=-129.6883087158203, ablated_edges=4114\n",
      "loss.item()=-132.43894958496094, ablated_edges=3987\n",
      "Epochs trained:  30\n",
      "Loss: -132.4389\n",
      "Total preserved: 7620.6592\n",
      "Edges ablated:  3987\n",
      "Toxic loss:  125.58036041259766\n",
      "OWT loss:  13.272636413574219\n",
      "Penalty:  tensor(6.8586, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' go'], P(Alicia) = 1.069854825734968e-21, logit diff = 1.0090599060058594\n",
      "Best Token: [' go'], P(Alicia) = 7.255693326176817e-23, logit diff = -1.196502685546875\n",
      "\n",
      "\n",
      "loss.item()=-134.920166015625, ablated_edges=3650\n",
      "loss.item()=-134.15908813476562, ablated_edges=3460\n",
      "loss.item()=-136.598388671875, ablated_edges=3272\n",
      "loss.item()=-137.21426391601562, ablated_edges=3091\n",
      "loss.item()=-136.15805053710938, ablated_edges=2983\n",
      "loss.item()=-138.28294372558594, ablated_edges=2855\n",
      "loss.item()=-139.58653259277344, ablated_edges=2762\n",
      "loss.item()=-141.294189453125, ablated_edges=2643\n",
      "loss.item()=-140.0099334716797, ablated_edges=2539\n",
      "loss.item()=-142.80894470214844, ablated_edges=2469\n",
      "Epochs trained:  40\n",
      "Loss: -142.8089\n",
      "Total preserved: 9070.5479\n",
      "Edges ablated:  2469\n",
      "Toxic loss:  125.57490539550781\n",
      "OWT loss:  11.477269172668457\n",
      "Penalty:  tensor(17.2340, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' go'], P(Alicia) = 3.9543811853972175e-20, logit diff = 3.3187389373779297\n",
      "Best Token: [' go'], P(Alicia) = 9.967618469730615e-21, logit diff = -5.333039283752441\n",
      "\n",
      "\n",
      "loss.item()=-143.86683654785156, ablated_edges=2386\n",
      "loss.item()=-143.16317749023438, ablated_edges=2347\n",
      "loss.item()=-145.74545288085938, ablated_edges=2278\n",
      "loss.item()=-148.27780151367188, ablated_edges=2208\n",
      "loss.item()=-145.60800170898438, ablated_edges=2140\n",
      "loss.item()=-148.58901977539062, ablated_edges=2110\n",
      "loss.item()=-151.49293518066406, ablated_edges=2042\n",
      "loss.item()=-149.13487243652344, ablated_edges=1958\n",
      "loss.item()=-150.35791015625, ablated_edges=1951\n",
      "loss.item()=-152.29669189453125, ablated_edges=1902\n",
      "Epochs trained:  50\n",
      "Loss: -152.2967\n",
      "Total preserved: 9643.2383\n",
      "Edges ablated:  1901\n",
      "Toxic loss:  124.331298828125\n",
      "OWT loss:  12.49522876739502\n",
      "Penalty:  tensor(27.9654, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 2.8700370968692823e-09, logit diff = -4.004571437835693\n",
      "Best Token: [' the'], P(Alicia) = 6.192550916495065e-09, logit diff = -4.083399772644043\n",
      "\n",
      "\n",
      "loss.item()=-149.05711364746094, ablated_edges=2130\n",
      "loss.item()=-152.14561462402344, ablated_edges=1818\n",
      "loss.item()=-154.87608337402344, ablated_edges=1640\n",
      "loss.item()=-156.79554748535156, ablated_edges=1591\n",
      "loss.item()=-159.59556579589844, ablated_edges=1522\n",
      "loss.item()=-158.4182586669922, ablated_edges=1489\n",
      "loss.item()=-160.49407958984375, ablated_edges=1460\n",
      "loss.item()=-160.84400939941406, ablated_edges=1405\n",
      "loss.item()=-161.7876739501953, ablated_edges=1385\n",
      "loss.item()=-163.28280639648438, ablated_edges=1352\n",
      "Epochs trained:  60\n",
      "Loss: -163.2828\n",
      "Total preserved: 10160.1396\n",
      "Edges ablated:  1352\n",
      "Toxic loss:  123.65826416015625\n",
      "OWT loss:  13.810145378112793\n",
      "Penalty:  tensor(39.6245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' go'], P(Alicia) = 2.7016001554191395e-28, logit diff = -20.94980812072754\n",
      "Best Token: [' go'], P(Alicia) = 1.7772196731515658e-18, logit diff = 14.216732025146484\n",
      "\n",
      "\n",
      "loss.item()=-163.25450134277344, ablated_edges=1358\n",
      "loss.item()=-164.072265625, ablated_edges=1319\n",
      "loss.item()=-168.50006103515625, ablated_edges=1307\n",
      "loss.item()=-164.63778686523438, ablated_edges=1284\n",
      "loss.item()=-168.04795837402344, ablated_edges=1283\n",
      "loss.item()=-171.11000061035156, ablated_edges=1258\n",
      "loss.item()=-169.7266387939453, ablated_edges=1232\n",
      "loss.item()=-172.04736328125, ablated_edges=1229\n",
      "loss.item()=-173.89488220214844, ablated_edges=1193\n",
      "loss.item()=-174.49911499023438, ablated_edges=1185\n",
      "Epochs trained:  70\n",
      "Loss: -174.4991\n",
      "Total preserved: 10329.4648\n",
      "Edges ablated:  1185\n",
      "Toxic loss:  123.88472747802734\n",
      "OWT loss:  11.908340454101562\n",
      "Penalty:  tensor(50.6144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' go'], P(Alicia) = 6.712404514720838e-30, logit diff = -20.11920166015625\n",
      "Best Token: [' go'], P(Alicia) = 1.686477481740707e-20, logit diff = 15.046096801757812\n",
      "\n",
      "\n",
      "loss.item()=-173.74972534179688, ablated_edges=1191\n",
      "loss.item()=-174.21615600585938, ablated_edges=1169\n",
      "loss.item()=-175.0550994873047, ablated_edges=1158\n",
      "loss.item()=-177.01217651367188, ablated_edges=1159\n",
      "loss.item()=-181.41839599609375, ablated_edges=1125\n",
      "loss.item()=-182.14881896972656, ablated_edges=1126\n",
      "loss.item()=-182.55393981933594, ablated_edges=1111\n",
      "loss.item()=-184.36679077148438, ablated_edges=1108\n",
      "loss.item()=-181.76441955566406, ablated_edges=1097\n",
      "loss.item()=-185.9786376953125, ablated_edges=1088\n",
      "Epochs trained:  80\n",
      "Loss: -185.9786\n",
      "Total preserved: 10441.1162\n",
      "Edges ablated:  1088\n",
      "Toxic loss:  124.37606048583984\n",
      "OWT loss:  12.543559074401855\n",
      "Penalty:  tensor(61.6026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' go'], P(Alicia) = 5.517159380306231e-23, logit diff = -15.31395435333252\n",
      "Best Token: [' go'], P(Alicia) = 4.543818460053285e-15, logit diff = 13.822006225585938\n",
      "\n",
      "\n",
      "loss.item()=-184.88720703125, ablated_edges=1082\n",
      "loss.item()=-188.74118041992188, ablated_edges=1075\n",
      "loss.item()=-189.152099609375, ablated_edges=1059\n",
      "loss.item()=-188.91085815429688, ablated_edges=1047\n",
      "loss.item()=-192.78623962402344, ablated_edges=1056\n",
      "loss.item()=-190.66677856445312, ablated_edges=1034\n",
      "loss.item()=-193.3803253173828, ablated_edges=1043\n",
      "loss.item()=-193.04779052734375, ablated_edges=1046\n",
      "loss.item()=-194.91110229492188, ablated_edges=1016\n",
      "loss.item()=-195.0379180908203, ablated_edges=1022\n",
      "Epochs trained:  90\n",
      "Loss: -195.0379\n",
      "Total preserved: 10519.6299\n",
      "Edges ablated:  1022\n",
      "Toxic loss:  122.45247650146484\n",
      "OWT loss:  9.738555908203125\n",
      "Penalty:  tensor(72.5854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' go'], P(Alicia) = 1.695228953161935e-23, logit diff = -14.332096099853516\n",
      "Best Token: [' go'], P(Alicia) = 1.6185962118203082e-15, logit diff = 14.638862609863281\n",
      "\n",
      "\n",
      "loss.item()=-197.1904296875, ablated_edges=1014\n",
      "loss.item()=-197.45333862304688, ablated_edges=1005\n",
      "loss.item()=-198.22584533691406, ablated_edges=1003\n",
      "loss.item()=-198.56503295898438, ablated_edges=995\n",
      "loss.item()=-198.64295959472656, ablated_edges=978\n",
      "loss.item()=-203.85830688476562, ablated_edges=975\n",
      "loss.item()=-202.04196166992188, ablated_edges=976\n",
      "loss.item()=-205.07720947265625, ablated_edges=972\n",
      "loss.item()=-202.31431579589844, ablated_edges=945\n",
      "loss.item()=-207.1562042236328, ablated_edges=946\n",
      "Epochs trained:  100\n",
      "Loss: -207.1562\n",
      "Total preserved: 10589.0762\n",
      "Edges ablated:  944\n",
      "Toxic loss:  123.50250244140625\n",
      "OWT loss:  11.207139015197754\n",
      "Penalty:  tensor(83.6537, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' ('], P(Alicia) = 9.557842017837714e-11, logit diff = -2.8650283813476562\n",
      "Best Token: [' go'], P(Alicia) = 3.122445704772936e-11, logit diff = -2.8309288024902344\n",
      "\n",
      "\n",
      "loss.item()=-209.07456970214844, ablated_edges=1280\n",
      "loss.item()=-218.88571166992188, ablated_edges=1027\n",
      "loss.item()=-223.73019409179688, ablated_edges=934\n",
      "loss.item()=-224.16726684570312, ablated_edges=888\n",
      "loss.item()=-226.55494689941406, ablated_edges=845\n",
      "loss.item()=-228.4808349609375, ablated_edges=815\n",
      "loss.item()=-229.20626831054688, ablated_edges=794\n",
      "loss.item()=-227.20106506347656, ablated_edges=769\n",
      "loss.item()=-230.99290466308594, ablated_edges=765\n",
      "loss.item()=-229.81283569335938, ablated_edges=755\n",
      "Epochs trained:  110\n",
      "Loss: -229.8128\n",
      "Total preserved: 10792.4873\n",
      "Edges ablated:  755\n",
      "Toxic loss:  133.7597198486328\n",
      "OWT loss:  19.529857635498047\n",
      "Penalty:  tensor(96.0531, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' cross'], P(Alicia) = 4.733226919562979e-38, logit diff = -19.465773582458496\n",
      "Best Token: [' cross'], P(Alicia) = 5.17378226480083e-38, logit diff = -23.76412010192871\n",
      "\n",
      "\n",
      "loss.item()=-233.1521453857422, ablated_edges=759\n",
      "loss.item()=-234.38580322265625, ablated_edges=758\n",
      "loss.item()=-236.51559448242188, ablated_edges=743\n",
      "loss.item()=-231.7974395751953, ablated_edges=728\n",
      "loss.item()=-238.9693603515625, ablated_edges=738\n",
      "loss.item()=-238.28138732910156, ablated_edges=723\n",
      "loss.item()=-237.64614868164062, ablated_edges=716\n",
      "loss.item()=-243.26162719726562, ablated_edges=718\n",
      "loss.item()=-244.3935089111328, ablated_edges=710\n",
      "loss.item()=-242.5557861328125, ablated_edges=699\n",
      "Epochs trained:  120\n",
      "Loss: -242.5558\n",
      "Total preserved: 10830.7529\n",
      "Edges ablated:  699\n",
      "Toxic loss:  135.33132934570312\n",
      "OWT loss:  13.105650901794434\n",
      "Penalty:  tensor(107.2245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' cross'], P(Alicia) = 8.120201834149095e-33, logit diff = -6.84195613861084\n",
      "Best Token: [' cross'], P(Alicia) = 5.231802783329747e-35, logit diff = -19.607384204864502\n",
      "\n",
      "\n",
      "loss.item()=-244.80166625976562, ablated_edges=701\n",
      "loss.item()=-248.7701416015625, ablated_edges=695\n",
      "loss.item()=-249.00259399414062, ablated_edges=674\n",
      "loss.item()=-247.4827880859375, ablated_edges=686\n",
      "loss.item()=-249.34902954101562, ablated_edges=694\n",
      "loss.item()=-251.4217529296875, ablated_edges=690\n",
      "loss.item()=-251.1705322265625, ablated_edges=690\n",
      "loss.item()=-250.4697265625, ablated_edges=677\n",
      "loss.item()=-251.66836547851562, ablated_edges=671\n",
      "loss.item()=-256.52435302734375, ablated_edges=672\n",
      "Epochs trained:  130\n",
      "Loss: -256.5244\n",
      "Total preserved: 10866.0596\n",
      "Edges ablated:  672\n",
      "Toxic loss:  138.0843048095703\n",
      "OWT loss:  13.888412475585938\n",
      "Penalty:  tensor(118.4400, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' cross'], P(Alicia) = 8.085759000191663e-35, logit diff = -13.389522552490234\n",
      "Best Token: [' cross'], P(Alicia) = 6.51726813189975e-36, logit diff = -21.2564754486084\n",
      "\n",
      "\n",
      "loss.item()=-255.19314575195312, ablated_edges=673\n",
      "loss.item()=-258.94281005859375, ablated_edges=666\n",
      "loss.item()=-254.89358520507812, ablated_edges=662\n",
      "loss.item()=-260.2525939941406, ablated_edges=655\n",
      "loss.item()=-260.5865173339844, ablated_edges=661\n",
      "loss.item()=-267.12249755859375, ablated_edges=652\n",
      "loss.item()=-261.7982482910156, ablated_edges=648\n",
      "loss.item()=-263.9369201660156, ablated_edges=666\n",
      "loss.item()=-264.2378845214844, ablated_edges=656\n",
      "loss.item()=-267.955078125, ablated_edges=651\n",
      "Epochs trained:  140\n",
      "Loss: -267.9551\n",
      "Total preserved: 10893.1865\n",
      "Edges ablated:  651\n",
      "Toxic loss:  138.32615661621094\n",
      "OWT loss:  11.637348175048828\n",
      "Penalty:  tensor(129.6289, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' cross'], P(Alicia) = 7.894365124785148e-33, logit diff = -6.724159240722656\n",
      "Best Token: [' cross'], P(Alicia) = 8.050119940020151e-35, logit diff = -16.21868896484375\n",
      "\n",
      "\n",
      "loss.item()=-268.42535400390625, ablated_edges=653\n",
      "loss.item()=-269.2855224609375, ablated_edges=652\n",
      "loss.item()=-269.1593322753906, ablated_edges=643\n",
      "loss.item()=-269.5218505859375, ablated_edges=649\n",
      "loss.item()=-268.2799072265625, ablated_edges=635\n",
      "loss.item()=-273.46539306640625, ablated_edges=635\n",
      "loss.item()=-271.80877685546875, ablated_edges=627\n",
      "loss.item()=-276.06158447265625, ablated_edges=622\n",
      "loss.item()=-276.1129150390625, ablated_edges=628\n",
      "loss.item()=-278.3039245605469, ablated_edges=635\n",
      "Epochs trained:  150\n",
      "Loss: -278.3039\n",
      "Total preserved: 10921.6729\n",
      "Edges ablated:  635\n",
      "Toxic loss:  137.41433715820312\n",
      "OWT loss:  11.148551940917969\n",
      "Penalty:  tensor(140.8896, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' high'], P(Alicia) = 3.4432338225087733e-07, logit diff = 1.5141239166259766\n",
      "Best Token: [' the'], P(Alicia) = 1.6678909275924525e-07, logit diff = 0.9102363586425781\n",
      "\n",
      "\n",
      "loss.item()=-269.48358154296875, ablated_edges=864\n",
      "loss.item()=-275.4632263183594, ablated_edges=722\n",
      "loss.item()=-279.19140625, ablated_edges=671\n",
      "loss.item()=-280.1242980957031, ablated_edges=642\n",
      "loss.item()=-282.6241149902344, ablated_edges=637\n",
      "loss.item()=-284.3406677246094, ablated_edges=624\n",
      "loss.item()=-288.633056640625, ablated_edges=620\n",
      "loss.item()=-285.3250732421875, ablated_edges=616\n",
      "loss.item()=-287.7275390625, ablated_edges=598\n",
      "loss.item()=-283.66455078125, ablated_edges=596\n",
      "Epochs trained:  160\n",
      "Loss: -283.6646\n",
      "Total preserved: 10951.1963\n",
      "Edges ablated:  596\n",
      "Toxic loss:  131.44293212890625\n",
      "OWT loss:  9.464232444763184\n",
      "Penalty:  tensor(152.2216, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' cross'], P(Alicia) = 5.002398400489254e-29, logit diff = -19.203617095947266\n",
      "Best Token: [' cross'], P(Alicia) = 2.0631336486009096e-21, logit diff = 6.783259868621826\n",
      "\n",
      "\n",
      "loss.item()=-287.1382141113281, ablated_edges=589\n",
      "loss.item()=-287.796630859375, ablated_edges=584\n",
      "loss.item()=-284.0061950683594, ablated_edges=585\n",
      "loss.item()=-293.5887451171875, ablated_edges=577\n",
      "loss.item()=-294.8279113769531, ablated_edges=574\n",
      "loss.item()=-294.2454528808594, ablated_edges=574\n",
      "loss.item()=-296.08319091796875, ablated_edges=571\n",
      "loss.item()=-299.00762939453125, ablated_edges=569\n",
      "loss.item()=-299.33599853515625, ablated_edges=564\n",
      "loss.item()=-301.087890625, ablated_edges=572\n",
      "Epochs trained:  170\n",
      "Loss: -301.0879\n",
      "Total preserved: 10975.2949\n",
      "Edges ablated:  572\n",
      "Toxic loss:  137.55601501464844\n",
      "OWT loss:  10.431796073913574\n",
      "Penalty:  tensor(163.5319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' cross'], P(Alicia) = 7.990724746756446e-28, logit diff = -18.8807315826416\n",
      "Best Token: [' cross'], P(Alicia) = 1.1425076781719238e-21, logit diff = 2.1074819564819336\n",
      "\n",
      "\n",
      "loss.item()=-300.6142272949219, ablated_edges=563\n",
      "loss.item()=-299.4302978515625, ablated_edges=546\n",
      "loss.item()=-301.98052978515625, ablated_edges=547\n",
      "loss.item()=-303.68212890625, ablated_edges=548\n",
      "loss.item()=-305.47747802734375, ablated_edges=547\n",
      "loss.item()=-307.486083984375, ablated_edges=542\n",
      "loss.item()=-305.623291015625, ablated_edges=544\n",
      "loss.item()=-310.62005615234375, ablated_edges=542\n",
      "loss.item()=-310.4093017578125, ablated_edges=539\n",
      "loss.item()=-312.31732177734375, ablated_edges=527\n",
      "Epochs trained:  180\n",
      "Loss: -312.3173\n",
      "Total preserved: 11007.2002\n",
      "Edges ablated:  527\n",
      "Toxic loss:  137.30287170410156\n",
      "OWT loss:  10.309494972229004\n",
      "Penalty:  tensor(175.0145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' cross'], P(Alicia) = 4.726034097927994e-27, logit diff = -19.23622989654541\n",
      "Best Token: [' cross'], P(Alicia) = 5.383824469275093e-22, logit diff = -2.3058624267578125\n",
      "\n",
      "\n",
      "loss.item()=-312.067626953125, ablated_edges=528\n",
      "loss.item()=-314.34906005859375, ablated_edges=526\n",
      "loss.item()=-312.7939453125, ablated_edges=529\n",
      "loss.item()=-314.0882873535156, ablated_edges=523\n",
      "loss.item()=-317.386962890625, ablated_edges=514\n",
      "loss.item()=-315.8401794433594, ablated_edges=518\n",
      "loss.item()=-319.53399658203125, ablated_edges=511\n",
      "loss.item()=-320.23724365234375, ablated_edges=504\n",
      "loss.item()=-322.90545654296875, ablated_edges=510\n",
      "loss.item()=-320.6246032714844, ablated_edges=516\n",
      "Epochs trained:  190\n",
      "Loss: -320.6246\n",
      "Total preserved: 11020.1787\n",
      "Edges ablated:  516\n",
      "Toxic loss:  134.38357543945312\n",
      "OWT loss:  10.36072063446045\n",
      "Penalty:  tensor(186.2410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' cross'], P(Alicia) = 2.5609717553306968e-26, logit diff = -17.471498489379883\n",
      "Best Token: [' cross'], P(Alicia) = 1.045424030692214e-22, logit diff = -4.888051986694336\n",
      "\n",
      "\n",
      "loss.item()=-318.2962646484375, ablated_edges=516\n",
      "loss.item()=-322.8656311035156, ablated_edges=509\n",
      "loss.item()=-325.53472900390625, ablated_edges=502\n",
      "loss.item()=-328.2636413574219, ablated_edges=491\n",
      "loss.item()=-323.7899169921875, ablated_edges=498\n",
      "loss.item()=-330.286376953125, ablated_edges=495\n",
      "loss.item()=-328.4990234375, ablated_edges=489\n",
      "loss.item()=-330.45068359375, ablated_edges=495\n",
      "loss.item()=-329.603515625, ablated_edges=496\n",
      "loss.item()=-332.8333740234375, ablated_edges=488\n",
      "Epochs trained:  200\n",
      "Loss: -332.8334\n",
      "Total preserved: 11042.1387\n",
      "Edges ablated:  491\n",
      "Toxic loss:  135.17904663085938\n",
      "OWT loss:  10.406207084655762\n",
      "Penalty:  tensor(197.6543, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 9.230164188522849e-09, logit diff = -0.072540283203125\n",
      "Best Token: [' the'], P(Alicia) = 2.4760449690575115e-08, logit diff = 1.0843467712402344\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = -1 * (regularization_strength * penalty + alpha * tox_loss) #+ owt_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            \n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_ioi_sentences = [\"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\", \"While Joshua and Alicia were commuting to the restaurant, Joshua gave a snack to\"]\n",
    "                for test_ioi_sentence in test_ioi_sentences:\n",
    "                    correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "                    other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "                    test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "                    generation = model(test_ioi_tokens)[0][:, -1]\n",
    "                    probs = torch.softmax(generation, dim=-1)\n",
    "                    print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")\n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            print(\"\\n\")\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    # epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    # log_every = int(input('set log frequency'))\n",
    "    # edge_threshold = int(input('set edge threshold'))\n",
    "    epochs_left = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/alternative_necessary_masks_params_dict_lambda={regularization_strength}_{means_ioi=}_{template_type=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different alternative: sufficient but not necessary\n",
    "Trains with an inverted loss function. This loss function encourages sparsity (as opposed to discouraging) and wants model to ablate everything but the necessary circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_batch_size = 10 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 1\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "\n",
    "template_type = \"double\"\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None, template_type=template_type)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "# with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "#     means = pickle.load(f)[0][0]\n",
    "means_ioi = True\n",
    "if means_ioi:\n",
    "    with open(\"data/gpt2_ioi_abc_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "else:\n",
    "    with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "\n",
    "model = load_demo_gpt2(means=means)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 50 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 1 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 100\n",
    "max_steps_per_epoch = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_188436/2053811722.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f94b829f68457492cafee9b6789f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=3.0504297683364712e-05, ablated_edges=617\n",
      "loss.item()=1.0728716006269678e-05, ablated_edges=622\n",
      "loss.item()=2.3507487640017644e-05, ablated_edges=629\n",
      "loss.item()=1.5262060165405273, ablated_edges=1581\n",
      "loss.item()=5.200825398787856e-05, ablated_edges=4975\n",
      "loss.item()=0.007873818278312683, ablated_edges=4998\n",
      "loss.item()=0.00014682210166938603, ablated_edges=5002\n",
      "loss.item()=1.1312711649225093e-05, ablated_edges=5009\n",
      "loss.item()=5.888998930458911e-05, ablated_edges=5005\n",
      "loss.item()=7.152454145398224e-06, ablated_edges=5006\n",
      "Epochs trained:  10\n",
      "Loss: 0.0000\n",
      "Total preserved: 6498.6450\n",
      "Edges ablated:  5006\n",
      "Toxic loss:  7.152454145398224e-06\n",
      "OWT loss:  8.024084091186523\n",
      "Penalty:  0\n",
      "Best Token: [' Alicia'], P(Alicia) = 1.0, logit diff = 19.63433837890625\n",
      "Best Token: [' Alicia'], P(Alicia) = 1.0, logit diff = 20.033138275146484\n",
      "\n",
      "\n",
      "loss.item()=3.348397876834497e-05, ablated_edges=5007\n",
      "loss.item()=4.351118604972726e-06, ablated_edges=5011\n",
      "loss.item()=2.253034153909539e-06, ablated_edges=5010\n",
      "loss.item()=4.136532425036421e-06, ablated_edges=5010\n",
      "loss.item()=3.4689487620198634e-06, ablated_edges=5016\n",
      "loss.item()=3.1232643777912017e-06, ablated_edges=5017\n",
      "loss.item()=1.7523728956803097e-06, ablated_edges=5016\n",
      "loss.item()=9.57226802711375e-06, ablated_edges=5018\n",
      "loss.item()=2.133836915163556e-06, ablated_edges=5019\n",
      "loss.item()=8.249024176620878e-06, ablated_edges=5020\n",
      "Epochs trained:  20\n",
      "Loss: 0.0000\n",
      "Total preserved: 6496.4346\n",
      "Edges ablated:  5020\n",
      "Toxic loss:  8.249024176620878e-06\n",
      "OWT loss:  7.061918258666992\n",
      "Penalty:  0\n",
      "Best Token: [' Alicia'], P(Alicia) = 1.0, logit diff = 17.236949920654297\n",
      "Best Token: [' Alicia'], P(Alicia) = 1.0, logit diff = 19.099525451660156\n",
      "\n",
      "\n",
      "loss.item()=2.3923395929159597e-05, ablated_edges=5005\n",
      "loss.item()=0.17398755252361298, ablated_edges=10053\n",
      "loss.item()=0.5594539642333984, ablated_edges=8939\n",
      "loss.item()=0.528475821018219, ablated_edges=9974\n",
      "loss.item()=0.5578272938728333, ablated_edges=10286\n",
      "loss.item()=0.6925519108772278, ablated_edges=10262\n",
      "loss.item()=0.45985308289527893, ablated_edges=10865\n",
      "loss.item()=0.6276309490203857, ablated_edges=10720\n",
      "loss.item()=0.6935020685195923, ablated_edges=10849\n",
      "loss.item()=0.6919615864753723, ablated_edges=10886\n",
      "Epochs trained:  30\n",
      "Loss: 0.6920\n",
      "Total preserved: 768.7573\n",
      "Edges ablated:  10886\n",
      "Toxic loss:  8.000074740266427e-05\n",
      "OWT loss:  8.39330768585205\n",
      "Penalty:  tensor(0.6919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999995231628418, logit diff = 14.56707763671875\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.034295160323381424, logit diff = -3.337810516357422\n",
      "\n",
      "\n",
      "loss.item()=0.987838864326477, ablated_edges=10714\n",
      "loss.item()=0.5804678797721863, ablated_edges=11073\n",
      "loss.item()=2.309312105178833, ablated_edges=11259\n",
      "loss.item()=0.7627602219581604, ablated_edges=11038\n",
      "loss.item()=0.571688175201416, ablated_edges=11205\n",
      "loss.item()=0.8858848810195923, ablated_edges=11066\n",
      "loss.item()=0.8397508859634399, ablated_edges=11096\n",
      "loss.item()=0.5859495997428894, ablated_edges=11270\n",
      "loss.item()=0.46310511231422424, ablated_edges=11385\n",
      "loss.item()=0.7221208810806274, ablated_edges=11274\n",
      "Epochs trained:  40\n",
      "Loss: 0.7221\n",
      "Total preserved: 379.9035\n",
      "Edges ablated:  11274\n",
      "Toxic loss:  0.00030426541343331337\n",
      "OWT loss:  8.065912246704102\n",
      "Penalty:  tensor(0.7218, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999735355377197, logit diff = 13.374710083007812\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.999998927116394, logit diff = 18.63144016265869\n",
      "\n",
      "\n",
      "loss.item()=0.5407541990280151, ablated_edges=11370\n",
      "loss.item()=0.40449032187461853, ablated_edges=11428\n",
      "loss.item()=2.502159357070923, ablated_edges=11435\n",
      "loss.item()=0.5426741242408752, ablated_edges=11388\n",
      "loss.item()=0.7828854322433472, ablated_edges=11349\n",
      "loss.item()=0.5053468942642212, ablated_edges=11436\n",
      "loss.item()=0.7048131823539734, ablated_edges=11380\n",
      "loss.item()=0.6211170554161072, ablated_edges=11432\n",
      "loss.item()=0.5308440923690796, ablated_edges=11443\n",
      "loss.item()=0.9265244603157043, ablated_edges=11346\n",
      "Epochs trained:  50\n",
      "Loss: 0.9265\n",
      "Total preserved: 317.5619\n",
      "Edges ablated:  11348\n",
      "Toxic loss:  0.005594924092292786\n",
      "OWT loss:  9.822382926940918\n",
      "Penalty:  tensor(0.9209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.5479981899261475, logit diff = 8.81834602355957\n",
      "Best Token: [' Al'], P(Alicia) = 0.018685391172766685, logit diff = 3.1313247680664062\n",
      "\n",
      "\n",
      "loss.item()=0.7166503071784973, ablated_edges=11387\n",
      "loss.item()=0.49225613474845886, ablated_edges=11468\n",
      "loss.item()=0.5958720445632935, ablated_edges=11463\n",
      "loss.item()=0.9004855751991272, ablated_edges=11405\n",
      "loss.item()=0.7828322649002075, ablated_edges=11478\n",
      "loss.item()=0.46735095977783203, ablated_edges=11484\n",
      "loss.item()=0.7020240426063538, ablated_edges=11438\n",
      "loss.item()=0.5320097804069519, ablated_edges=11502\n",
      "loss.item()=0.5130903720855713, ablated_edges=11488\n",
      "loss.item()=0.6565840840339661, ablated_edges=11473\n",
      "Epochs trained:  60\n",
      "Loss: 0.6566\n",
      "Total preserved: 166.9232\n",
      "Edges ablated:  11473\n",
      "Toxic loss:  0.005583619698882103\n",
      "OWT loss:  9.232029914855957\n",
      "Penalty:  tensor(0.6510, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999949932098389, logit diff = 12.233685493469238\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9997592568397522, logit diff = 8.36999797821045\n",
      "\n",
      "\n",
      "loss.item()=0.6821994185447693, ablated_edges=11476\n",
      "loss.item()=0.5385717153549194, ablated_edges=11499\n",
      "loss.item()=1.2836743593215942, ablated_edges=11470\n",
      "loss.item()=0.7904183864593506, ablated_edges=11485\n",
      "loss.item()=0.649208128452301, ablated_edges=11492\n",
      "loss.item()=0.7064244747161865, ablated_edges=11482\n",
      "loss.item()=0.7608710527420044, ablated_edges=11470\n",
      "loss.item()=0.5891523957252502, ablated_edges=11498\n",
      "loss.item()=0.8513886332511902, ablated_edges=11458\n",
      "loss.item()=0.5939263105392456, ablated_edges=11496\n",
      "Epochs trained:  70\n",
      "Loss: 0.5939\n",
      "Total preserved: 120.3538\n",
      "Edges ablated:  11496\n",
      "Toxic loss:  0.004192802123725414\n",
      "OWT loss:  13.207061767578125\n",
      "Penalty:  tensor(0.5897, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999998807907104, logit diff = 21.948081970214844\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999936819076538, logit diff = 13.484237670898438\n",
      "\n",
      "\n",
      "loss.item()=0.680294930934906, ablated_edges=11498\n",
      "loss.item()=1.1755045652389526, ablated_edges=11514\n",
      "loss.item()=0.7043154239654541, ablated_edges=11488\n",
      "loss.item()=0.5949071049690247, ablated_edges=11513\n",
      "loss.item()=1.2401340007781982, ablated_edges=11411\n",
      "loss.item()=1.5754671096801758, ablated_edges=11442\n",
      "loss.item()=0.9383693933486938, ablated_edges=11471\n",
      "loss.item()=0.7144198417663574, ablated_edges=11511\n",
      "loss.item()=0.9623447060585022, ablated_edges=11476\n",
      "loss.item()=0.7601954340934753, ablated_edges=11518\n",
      "Epochs trained:  80\n",
      "Loss: 0.7602\n",
      "Total preserved: 128.1837\n",
      "Edges ablated:  11518\n",
      "Toxic loss:  0.003911274950951338\n",
      "OWT loss:  12.307639122009277\n",
      "Penalty:  tensor(0.7563, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999886751174927, logit diff = 28.07324981689453\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9974586367607117, logit diff = 17.833415985107422\n",
      "\n",
      "\n",
      "loss.item()=0.5846819281578064, ablated_edges=11527\n",
      "loss.item()=0.9236180186271667, ablated_edges=11475\n",
      "loss.item()=0.5950123071670532, ablated_edges=11530\n",
      "loss.item()=0.9559506177902222, ablated_edges=11495\n",
      "loss.item()=0.7198612689971924, ablated_edges=11524\n",
      "loss.item()=1.3006625175476074, ablated_edges=11486\n",
      "loss.item()=1.0195286273956299, ablated_edges=11512\n",
      "loss.item()=1.0490831136703491, ablated_edges=11492\n",
      "loss.item()=2.647153615951538, ablated_edges=11528\n",
      "loss.item()=1.175584316253662, ablated_edges=11474\n",
      "Epochs trained:  90\n",
      "Loss: 1.1756\n",
      "Total preserved: 170.3621\n",
      "Edges ablated:  11474\n",
      "Toxic loss:  8.60192667460069e-05\n",
      "OWT loss:  10.534825325012207\n",
      "Penalty:  tensor(1.1755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999996423721313, logit diff = 24.693950653076172\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999988079071045, logit diff = 24.879478454589844\n",
      "\n",
      "\n",
      "loss.item()=1.4363462924957275, ablated_edges=11514\n",
      "loss.item()=1.278296947479248, ablated_edges=11522\n",
      "loss.item()=0.7648888826370239, ablated_edges=11519\n",
      "loss.item()=0.7021269202232361, ablated_edges=11532\n",
      "loss.item()=0.771545946598053, ablated_edges=11533\n",
      "loss.item()=0.8697092533111572, ablated_edges=11521\n",
      "loss.item()=0.892496645450592, ablated_edges=11516\n",
      "loss.item()=1.5342378616333008, ablated_edges=11499\n",
      "loss.item()=0.6698748469352722, ablated_edges=11534\n",
      "loss.item()=0.8323975205421448, ablated_edges=11533\n",
      "Epochs trained:  100\n",
      "Loss: 0.8324\n",
      "Total preserved: 103.5808\n",
      "Edges ablated:  11533\n",
      "Toxic loss:  0.01410916168242693\n",
      "OWT loss:  8.636442184448242\n",
      "Penalty:  tensor(0.8183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9994508624076843, logit diff = 8.897517204284668\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.070487841963768, logit diff = -2.5733680725097656\n",
      "\n",
      "\n",
      "loss.item()=2.3086936473846436, ablated_edges=11355\n",
      "loss.item()=1.4049633741378784, ablated_edges=11446\n",
      "loss.item()=1.5729278326034546, ablated_edges=11492\n",
      "loss.item()=1.07839035987854, ablated_edges=11500\n",
      "loss.item()=1.533691644668579, ablated_edges=11477\n",
      "loss.item()=0.9987261891365051, ablated_edges=11511\n",
      "loss.item()=0.8981645703315735, ablated_edges=11525\n",
      "loss.item()=1.1330955028533936, ablated_edges=11497\n",
      "loss.item()=0.9761794805526733, ablated_edges=11517\n",
      "loss.item()=1.0063040256500244, ablated_edges=11535\n",
      "Epochs trained:  110\n",
      "Loss: 1.0063\n",
      "Total preserved: 112.4470\n",
      "Edges ablated:  11535\n",
      "Toxic loss:  0.005525506101548672\n",
      "OWT loss:  10.807186126708984\n",
      "Penalty:  tensor(1.0008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.99679034948349, logit diff = 5.826114654541016\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999427795410156, logit diff = 10.477134704589844\n",
      "\n",
      "\n",
      "loss.item()=1.6345804929733276, ablated_edges=11520\n",
      "loss.item()=1.079970359802246, ablated_edges=11526\n",
      "loss.item()=1.1748759746551514, ablated_edges=11491\n",
      "loss.item()=0.9562442302703857, ablated_edges=11516\n",
      "loss.item()=1.0353955030441284, ablated_edges=11533\n",
      "loss.item()=1.074641227722168, ablated_edges=11531\n",
      "loss.item()=1.568163275718689, ablated_edges=11525\n",
      "loss.item()=1.1033837795257568, ablated_edges=11522\n",
      "loss.item()=0.9593234062194824, ablated_edges=11536\n",
      "loss.item()=1.1191647052764893, ablated_edges=11538\n",
      "Epochs trained:  120\n",
      "Loss: 1.1192\n",
      "Total preserved: 112.8693\n",
      "Edges ablated:  11538\n",
      "Toxic loss:  0.0017587473848834634\n",
      "OWT loss:  12.436023712158203\n",
      "Penalty:  tensor(1.1174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.14891287684440613, logit diff = -1.52630615234375\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.01341601088643074, logit diff = -4.2134857177734375\n",
      "\n",
      "\n",
      "loss.item()=1.396270513534546, ablated_edges=11515\n",
      "loss.item()=0.9862658977508545, ablated_edges=11526\n",
      "loss.item()=1.190179705619812, ablated_edges=11530\n",
      "loss.item()=1.129488229751587, ablated_edges=11522\n",
      "loss.item()=1.0211626291275024, ablated_edges=11529\n",
      "loss.item()=1.002874732017517, ablated_edges=11540\n",
      "loss.item()=2.4899144172668457, ablated_edges=11518\n",
      "loss.item()=2.11415958404541, ablated_edges=11493\n",
      "loss.item()=1.500408411026001, ablated_edges=11517\n",
      "loss.item()=1.0137032270431519, ablated_edges=11538\n",
      "Epochs trained:  130\n",
      "Loss: 1.0137\n",
      "Total preserved: 90.8017\n",
      "Edges ablated:  11538\n",
      "Toxic loss:  0.02396484836935997\n",
      "OWT loss:  8.931882858276367\n",
      "Penalty:  tensor(0.9897, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9904961585998535, logit diff = 5.463775634765625\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9021298885345459, logit diff = 2.9188499450683594\n",
      "\n",
      "\n",
      "loss.item()=1.0197672843933105, ablated_edges=11542\n",
      "loss.item()=1.1240493059158325, ablated_edges=11539\n",
      "loss.item()=2.1887054443359375, ablated_edges=11533\n",
      "loss.item()=1.0809108018875122, ablated_edges=11539\n",
      "loss.item()=1.2513364553451538, ablated_edges=11555\n",
      "loss.item()=1.6085139513015747, ablated_edges=11533\n",
      "loss.item()=0.9102416038513184, ablated_edges=11546\n",
      "loss.item()=1.072023868560791, ablated_edges=11542\n",
      "loss.item()=2.977421283721924, ablated_edges=11379\n",
      "loss.item()=1.772498607635498, ablated_edges=11513\n",
      "Epochs trained:  140\n",
      "Loss: 1.7725\n",
      "Total preserved: 109.6221\n",
      "Edges ablated:  11513\n",
      "Toxic loss:  0.4679953455924988\n",
      "OWT loss:  15.390471458435059\n",
      "Penalty:  tensor(1.3045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9726325869560242, logit diff = 3.5960350036621094\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.48265090584754944, logit diff = -0.057952880859375\n",
      "\n",
      "\n",
      "loss.item()=0.9915135502815247, ablated_edges=11543\n",
      "loss.item()=1.1490050554275513, ablated_edges=11535\n",
      "loss.item()=0.9490958452224731, ablated_edges=11549\n",
      "loss.item()=1.2623894214630127, ablated_edges=11537\n",
      "loss.item()=1.2113237380981445, ablated_edges=11544\n",
      "loss.item()=1.1265099048614502, ablated_edges=11531\n",
      "loss.item()=0.8768659234046936, ablated_edges=11556\n",
      "loss.item()=1.6754629611968994, ablated_edges=11554\n",
      "loss.item()=1.4453679323196411, ablated_edges=11533\n",
      "loss.item()=1.0895334482192993, ablated_edges=11554\n",
      "Epochs trained:  150\n",
      "Loss: 1.0895\n",
      "Total preserved: 77.7008\n",
      "Edges ablated:  11554\n",
      "Toxic loss:  0.08719325810670853\n",
      "OWT loss:  11.582467079162598\n",
      "Penalty:  tensor(1.0023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.08351228386163712, logit diff = -1.6871185302734375\n",
      "Best Token: [' Joshua'], P(Alicia) = 1.4839622508588945e-06, logit diff = -12.5553138256073\n",
      "\n",
      "\n",
      "loss.item()=1.7642123699188232, ablated_edges=11479\n",
      "loss.item()=1.235959529876709, ablated_edges=11527\n",
      "loss.item()=1.9689606428146362, ablated_edges=11528\n",
      "loss.item()=1.180838942527771, ablated_edges=11538\n",
      "loss.item()=1.0850321054458618, ablated_edges=11542\n",
      "loss.item()=1.6115220785140991, ablated_edges=11516\n",
      "loss.item()=1.285523772239685, ablated_edges=11544\n",
      "loss.item()=1.4283206462860107, ablated_edges=11542\n",
      "loss.item()=1.4779062271118164, ablated_edges=11554\n",
      "loss.item()=0.9290322065353394, ablated_edges=11552\n",
      "Epochs trained:  160\n",
      "Loss: 0.9290\n",
      "Total preserved: 66.7487\n",
      "Edges ablated:  11552\n",
      "Toxic loss:  0.001225432613864541\n",
      "OWT loss:  17.932828903198242\n",
      "Penalty:  tensor(0.9278, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 1.0, logit diff = 24.50611114501953\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.0006887635681778193, logit diff = -7.2736616134643555\n",
      "\n",
      "\n",
      "loss.item()=1.0602823495864868, ablated_edges=11551\n",
      "loss.item()=1.248526692390442, ablated_edges=11556\n",
      "loss.item()=1.072791576385498, ablated_edges=11548\n",
      "loss.item()=1.0562388896942139, ablated_edges=11552\n",
      "loss.item()=1.0380702018737793, ablated_edges=11551\n",
      "loss.item()=0.806648850440979, ablated_edges=11562\n",
      "loss.item()=1.738633155822754, ablated_edges=11559\n",
      "loss.item()=1.5763707160949707, ablated_edges=11542\n",
      "loss.item()=1.1538991928100586, ablated_edges=11548\n",
      "loss.item()=0.8633016347885132, ablated_edges=11561\n",
      "Epochs trained:  170\n",
      "Loss: 0.8633\n",
      "Total preserved: 57.6966\n",
      "Edges ablated:  11561\n",
      "Toxic loss:  0.003621750045567751\n",
      "OWT loss:  11.590035438537598\n",
      "Penalty:  tensor(0.8597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999872446060181, logit diff = 17.035221099853516\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.93155437707901, logit diff = 6.1922607421875\n",
      "\n",
      "\n",
      "loss.item()=0.8551331162452698, ablated_edges=11565\n",
      "loss.item()=1.5927503108978271, ablated_edges=11529\n",
      "loss.item()=1.1801830530166626, ablated_edges=11549\n",
      "loss.item()=1.2625771760940552, ablated_edges=11544\n",
      "loss.item()=1.6816266775131226, ablated_edges=11544\n",
      "loss.item()=1.0314993858337402, ablated_edges=11557\n",
      "loss.item()=2.3684258460998535, ablated_edges=11545\n",
      "loss.item()=3.4032235145568848, ablated_edges=11556\n",
      "loss.item()=1.7169115543365479, ablated_edges=11560\n",
      "loss.item()=1.1044307947158813, ablated_edges=11556\n",
      "Epochs trained:  180\n",
      "Loss: 1.1044\n",
      "Total preserved: 69.1726\n",
      "Edges ablated:  11556\n",
      "Toxic loss:  0.0045863729901611805\n",
      "OWT loss:  10.713252067565918\n",
      "Penalty:  tensor(1.0998, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9996004700660706, logit diff = 14.144969940185547\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.456355482339859, logit diff = 1.3954658508300781\n",
      "\n",
      "\n",
      "loss.item()=1.2641592025756836, ablated_edges=11561\n",
      "loss.item()=1.1948111057281494, ablated_edges=11568\n",
      "loss.item()=0.9431160092353821, ablated_edges=11566\n",
      "loss.item()=1.6775532960891724, ablated_edges=11525\n",
      "loss.item()=1.0061196088790894, ablated_edges=11558\n",
      "loss.item()=1.2561269998550415, ablated_edges=11556\n",
      "loss.item()=1.1735142469406128, ablated_edges=11550\n",
      "loss.item()=1.046945333480835, ablated_edges=11559\n",
      "loss.item()=1.1391093730926514, ablated_edges=11561\n",
      "loss.item()=1.2035062313079834, ablated_edges=11565\n",
      "Epochs trained:  190\n",
      "Loss: 1.2035\n",
      "Total preserved: 49.9159\n",
      "Edges ablated:  11565\n",
      "Toxic loss:  0.35992729663848877\n",
      "OWT loss:  17.21310806274414\n",
      "Penalty:  tensor(0.8436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 1.0, logit diff = 25.68968391418457\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.22011639177799225, logit diff = -0.8987197875976562\n",
      "\n",
      "\n",
      "loss.item()=0.8770900964736938, ablated_edges=11565\n",
      "loss.item()=0.9068272709846497, ablated_edges=11565\n",
      "loss.item()=1.9978264570236206, ablated_edges=11520\n",
      "loss.item()=1.4568772315979004, ablated_edges=11558\n",
      "loss.item()=1.6506972312927246, ablated_edges=11559\n",
      "loss.item()=1.1249099969863892, ablated_edges=11562\n",
      "loss.item()=0.8094291687011719, ablated_edges=11570\n",
      "loss.item()=1.2312835454940796, ablated_edges=11555\n",
      "loss.item()=0.8460001349449158, ablated_edges=11570\n",
      "loss.item()=1.3838704824447632, ablated_edges=11556\n",
      "Epochs trained:  200\n",
      "Loss: 1.3839\n",
      "Total preserved: 65.2812\n",
      "Edges ablated:  11556\n",
      "Toxic loss:  0.21533605456352234\n",
      "OWT loss:  9.57421875\n",
      "Penalty:  tensor(1.1685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9056878089904785, logit diff = 2.4303483963012695\n",
      "Best Token: [' Joshua'], P(Alicia) = 2.5975340989248252e-08, logit diff = -17.38216209411621\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = (regularization_strength * penalty + alpha * tox_loss) #+ owt_loss\n",
    "            # loss = alpha * tox_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            \n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_ioi_sentences = [\"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\", \"While Joshua and Alicia were commuting to the restaurant, Joshua gave a snack to\"]\n",
    "                for test_ioi_sentence in test_ioi_sentences:\n",
    "                    correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "                    other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "                    test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "                    generation = model(test_ioi_tokens)[0][:, -1]\n",
    "                    probs = torch.softmax(generation, dim=-1)\n",
    "                    print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")\n",
    "            \n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            print(\"\\n\")\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    # epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    # log_every = int(input('set log frequency'))\n",
    "    # edge_threshold = int(input('set edge threshold'))\n",
    "    epochs_left = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/alternative_sufficient_masks_params_dict_lambda={regularization_strength}_{alpha=}_{means_ioi=}_{template_type=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train mask over known circuit\n",
    "Train mask over the circuit from the paper, as given by a run of ACDC++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed': tensor([]),\n",
       " 'a0.0': tensor([1.]),\n",
       " 'a0.1': tensor([0.]),\n",
       " 'a0.2': tensor([1.]),\n",
       " 'a0.3': tensor([0.]),\n",
       " 'a0.4': tensor([1.]),\n",
       " 'a0.5': tensor([0.]),\n",
       " 'a0.6': tensor([1.]),\n",
       " 'a0.7': tensor([1.]),\n",
       " 'a0.8': tensor([1.]),\n",
       " 'a0.9': tensor([1.]),\n",
       " 'a0.10': tensor([0.]),\n",
       " 'a0.11': tensor([1.]),\n",
       " 'm0': tensor([0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.]),\n",
       " 'a1.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm2': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1.]),\n",
       " 'a3.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0.]),\n",
       " 'a3.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'm3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "         1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.]),\n",
       " 'a5.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.]),\n",
       " 'a5.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         0., 1., 0., 0., 0., 1.]),\n",
       " 'a6.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 1.]),\n",
       " 'a6.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.9': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         0., 1., 1., 1., 1., 1., 0.]),\n",
       " 'a6.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
       "         1.]),\n",
       " 'a7.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.9': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0.]),\n",
       " 'a7.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'm7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.]),\n",
       " 'a8.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1.]),\n",
       " 'a9.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]),\n",
       " 'a9.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0.]),\n",
       " 'a9.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]),\n",
       " 'a9.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1.]),\n",
       " 'a10.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 1.]),\n",
       " 'a10.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.7': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 1.]),\n",
       " 'a10.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'm10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.10': tensor([1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.]),\n",
       " 'a11.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'output': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mask_utils import get_nodes_and_edges\n",
    "with open(\"models/acdcpp_mask_params.pkl\", \"rb\") as f:\n",
    "    acdc_mask_params = pickle.load(f)\n",
    "\n",
    "_, _, acdc_Edges, acdc_mask_dict = get_nodes_and_edges(mask_params=acdc_mask_params)\n",
    "acdc_mask_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([157])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acdc_mask_dict['output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22863/1329432646.py:48: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10673f0ad7234b04a3294330b8b5f2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=1.5056736469268799, ablated_edges=80\n",
      "loss.item()=-0.500084400177002, ablated_edges=87\n",
      "loss.item()=-0.0825948715209961, ablated_edges=87\n",
      "loss.item()=0.15455102920532227, ablated_edges=88\n",
      "loss.item()=0.7475423812866211, ablated_edges=95\n",
      "loss.item()=0.7848529815673828, ablated_edges=94\n",
      "loss.item()=0.06402826309204102, ablated_edges=98\n",
      "loss.item()=-0.9390511512756348, ablated_edges=96\n",
      "loss.item()=0.6501531600952148, ablated_edges=98\n",
      "loss.item()=0.626798152923584, ablated_edges=98\n",
      "Epochs trained:  10\n",
      "Loss: 0.6268\n",
      "Total preserved: 11512.8232\n",
      "Edges ablated:  98\n",
      "Toxic loss:  23.036602020263672\n",
      "OWT loss:  5.234118461608887\n",
      "Penalty:  0\n",
      "\n",
      "\n",
      "loss.item()=-3.0186614990234375, ablated_edges=103\n",
      "loss.item()=-0.8208189010620117, ablated_edges=93\n",
      "loss.item()=0.038724422454833984, ablated_edges=94\n",
      "loss.item()=-0.08931589126586914, ablated_edges=95\n",
      "loss.item()=-0.5814499855041504, ablated_edges=98\n",
      "loss.item()=-0.1508636474609375, ablated_edges=102\n",
      "loss.item()=2.3883941173553467, ablated_edges=98\n",
      "loss.item()=1.5119729042053223, ablated_edges=96\n",
      "loss.item()=-0.7001581192016602, ablated_edges=100\n",
      "loss.item()=-0.5811986923217773, ablated_edges=97\n",
      "Epochs trained:  20\n",
      "Loss: -0.5812\n",
      "Total preserved: 11511.8525\n",
      "Edges ablated:  97\n",
      "Toxic loss:  27.467458724975586\n",
      "OWT loss:  4.912292957305908\n",
      "Penalty:  0\n",
      "\n",
      "\n",
      "loss.item()=1.1091012954711914, ablated_edges=100\n",
      "loss.item()=-0.7198200225830078, ablated_edges=98\n",
      "loss.item()=-1.066877841949463, ablated_edges=100\n",
      "loss.item()=-4.244425296783447, ablated_edges=105\n",
      "loss.item()=-5.552443981170654, ablated_edges=98\n",
      "loss.item()=-6.534358024597168, ablated_edges=91\n",
      "loss.item()=-7.882603168487549, ablated_edges=89\n",
      "loss.item()=-8.790934562683105, ablated_edges=88\n",
      "loss.item()=-8.656744003295898, ablated_edges=87\n",
      "loss.item()=-8.970634460449219, ablated_edges=89\n",
      "Epochs trained:  30\n",
      "Loss: -8.9706\n",
      "Total preserved: 11524.6582\n",
      "Edges ablated:  89\n",
      "Toxic loss:  19.04596519470215\n",
      "OWT loss:  5.210751056671143\n",
      "Penalty:  tensor(10.3722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-10.965009689331055, ablated_edges=88\n",
      "loss.item()=-13.547128677368164, ablated_edges=90\n",
      "loss.item()=-13.717792510986328, ablated_edges=87\n",
      "loss.item()=-16.68755340576172, ablated_edges=86\n",
      "loss.item()=-16.444446563720703, ablated_edges=82\n",
      "loss.item()=-16.821895599365234, ablated_edges=82\n",
      "loss.item()=-17.48927116394043, ablated_edges=80\n",
      "loss.item()=-21.36284828186035, ablated_edges=81\n",
      "loss.item()=-21.52306365966797, ablated_edges=80\n",
      "loss.item()=-22.231990814208984, ablated_edges=77\n",
      "Epochs trained:  40\n",
      "Loss: -22.2320\n",
      "Total preserved: 11534.5127\n",
      "Edges ablated:  78\n",
      "Toxic loss:  26.378662109375\n",
      "OWT loss:  4.959319114685059\n",
      "Penalty:  tensor(21.9156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-22.69232940673828, ablated_edges=77\n",
      "loss.item()=-23.534677505493164, ablated_edges=78\n",
      "loss.item()=-25.442670822143555, ablated_edges=76\n",
      "loss.item()=-27.155630111694336, ablated_edges=79\n",
      "loss.item()=-28.782812118530273, ablated_edges=76\n",
      "loss.item()=-28.164342880249023, ablated_edges=77\n",
      "loss.item()=-29.027385711669922, ablated_edges=72\n",
      "loss.item()=-30.837371826171875, ablated_edges=80\n",
      "loss.item()=-32.40890121459961, ablated_edges=72\n",
      "loss.item()=-32.99948501586914, ablated_edges=72\n",
      "Epochs trained:  50\n",
      "Loss: -32.9995\n",
      "Total preserved: 11537.1387\n",
      "Edges ablated:  72\n",
      "Toxic loss:  25.70037269592285\n",
      "OWT loss:  5.598297119140625\n",
      "Penalty:  tensor(33.4577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-33.814735412597656, ablated_edges=70\n",
      "loss.item()=-34.977874755859375, ablated_edges=74\n",
      "loss.item()=-38.72045135498047, ablated_edges=76\n",
      "loss.item()=-38.59954833984375, ablated_edges=72\n",
      "loss.item()=-37.44197082519531, ablated_edges=75\n",
      "loss.item()=-40.775421142578125, ablated_edges=69\n",
      "loss.item()=-39.43312454223633, ablated_edges=69\n",
      "loss.item()=-43.922523498535156, ablated_edges=70\n",
      "loss.item()=-44.29679870605469, ablated_edges=63\n",
      "loss.item()=-44.6660270690918, ablated_edges=69\n",
      "Epochs trained:  60\n",
      "Loss: -44.6660\n",
      "Total preserved: 11543.3457\n",
      "Edges ablated:  68\n",
      "Toxic loss:  24.17945098876953\n",
      "OWT loss:  5.188914775848389\n",
      "Penalty:  tensor(45.0191, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-45.43226623535156, ablated_edges=68\n",
      "loss.item()=-47.554931640625, ablated_edges=67\n",
      "loss.item()=-48.48405456542969, ablated_edges=70\n",
      "loss.item()=-49.8265495300293, ablated_edges=67\n",
      "loss.item()=-51.74678421020508, ablated_edges=66\n",
      "loss.item()=-51.73463821411133, ablated_edges=69\n",
      "loss.item()=-52.265323638916016, ablated_edges=70\n",
      "loss.item()=-54.762516021728516, ablated_edges=71\n",
      "loss.item()=-55.7078971862793, ablated_edges=68\n",
      "loss.item()=-56.24128723144531, ablated_edges=67\n",
      "Epochs trained:  70\n",
      "Loss: -56.2413\n",
      "Total preserved: 11541.6113\n",
      "Edges ablated:  67\n",
      "Toxic loss:  23.93475341796875\n",
      "OWT loss:  5.099553108215332\n",
      "Penalty:  tensor(56.5539, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-57.00583267211914, ablated_edges=69\n",
      "loss.item()=-57.7839469909668, ablated_edges=70\n",
      "loss.item()=-58.821163177490234, ablated_edges=67\n",
      "loss.item()=-61.0757942199707, ablated_edges=66\n",
      "loss.item()=-63.463558197021484, ablated_edges=66\n",
      "loss.item()=-64.74162292480469, ablated_edges=62\n",
      "loss.item()=-65.59223175048828, ablated_edges=61\n",
      "loss.item()=-65.37863159179688, ablated_edges=66\n",
      "loss.item()=-66.93404388427734, ablated_edges=61\n",
      "loss.item()=-67.35196685791016, ablated_edges=66\n",
      "Epochs trained:  80\n",
      "Loss: -67.3520\n",
      "Total preserved: 11547.4160\n",
      "Edges ablated:  64\n",
      "Toxic loss:  22.907915115356445\n",
      "OWT loss:  5.359364986419678\n",
      "Penalty:  tensor(68.1298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-70.22889709472656, ablated_edges=59\n",
      "loss.item()=-69.60752868652344, ablated_edges=62\n",
      "loss.item()=-71.21782684326172, ablated_edges=64\n",
      "loss.item()=-72.4986801147461, ablated_edges=64\n",
      "loss.item()=-74.5802001953125, ablated_edges=66\n",
      "loss.item()=-74.36848449707031, ablated_edges=65\n",
      "loss.item()=-76.47356414794922, ablated_edges=63\n",
      "loss.item()=-78.4028549194336, ablated_edges=63\n",
      "loss.item()=-77.62654876708984, ablated_edges=66\n",
      "loss.item()=-80.140625, ablated_edges=64\n",
      "Epochs trained:  90\n",
      "Loss: -80.1406\n",
      "Total preserved: 11547.1406\n",
      "Edges ablated:  64\n",
      "Toxic loss:  23.180822372436523\n",
      "OWT loss:  4.170817852020264\n",
      "Penalty:  tensor(79.6753, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-82.50818634033203, ablated_edges=66\n",
      "loss.item()=-81.01814270019531, ablated_edges=64\n",
      "loss.item()=-83.3720474243164, ablated_edges=66\n",
      "loss.item()=-83.98469543457031, ablated_edges=58\n",
      "loss.item()=-87.44886016845703, ablated_edges=61\n",
      "loss.item()=-86.34146881103516, ablated_edges=58\n",
      "loss.item()=-85.54612731933594, ablated_edges=57\n",
      "loss.item()=-90.238525390625, ablated_edges=54\n",
      "loss.item()=-90.92143249511719, ablated_edges=54\n",
      "loss.item()=-92.84722137451172, ablated_edges=57\n",
      "Epochs trained:  100\n",
      "Loss: -92.8472\n",
      "Total preserved: 11554.1641\n",
      "Edges ablated:  57\n",
      "Toxic loss:  33.43305587768555\n",
      "OWT loss:  5.11730432510376\n",
      "Penalty:  tensor(91.2779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-91.27748107910156, ablated_edges=59\n",
      "loss.item()=-93.88461303710938, ablated_edges=54\n",
      "loss.item()=-94.77798461914062, ablated_edges=50\n",
      "loss.item()=-94.69718933105469, ablated_edges=52\n",
      "loss.item()=-95.52283477783203, ablated_edges=53\n",
      "loss.item()=-97.91181182861328, ablated_edges=58\n",
      "loss.item()=-100.29170989990234, ablated_edges=53\n",
      "loss.item()=-101.28544616699219, ablated_edges=52\n",
      "loss.item()=-99.9444808959961, ablated_edges=54\n",
      "loss.item()=-102.86627197265625, ablated_edges=53\n",
      "Epochs trained:  110\n",
      "Loss: -102.8663\n",
      "Total preserved: 11557.9971\n",
      "Edges ablated:  53\n",
      "Toxic loss:  28.86589813232422\n",
      "OWT loss:  5.773064613342285\n",
      "Penalty:  tensor(102.8662, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-103.58384704589844, ablated_edges=52\n",
      "loss.item()=-105.84606170654297, ablated_edges=52\n",
      "loss.item()=-107.2335433959961, ablated_edges=53\n",
      "loss.item()=-106.85431671142578, ablated_edges=53\n",
      "loss.item()=-109.91695404052734, ablated_edges=52\n",
      "loss.item()=-110.24185943603516, ablated_edges=54\n",
      "loss.item()=-111.40071105957031, ablated_edges=50\n",
      "loss.item()=-112.56986999511719, ablated_edges=50\n",
      "loss.item()=-113.40050506591797, ablated_edges=53\n",
      "loss.item()=-113.72864532470703, ablated_edges=52\n",
      "Epochs trained:  120\n",
      "Loss: -113.7286\n",
      "Total preserved: 11557.8770\n",
      "Edges ablated:  52\n",
      "Toxic loss:  28.943449020385742\n",
      "OWT loss:  6.483026027679443\n",
      "Penalty:  tensor(114.4230, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-116.54255676269531, ablated_edges=53\n",
      "loss.item()=-116.79327392578125, ablated_edges=51\n",
      "loss.item()=-115.19482421875, ablated_edges=51\n",
      "loss.item()=-118.75401306152344, ablated_edges=53\n",
      "loss.item()=-122.14277648925781, ablated_edges=49\n",
      "loss.item()=-121.06953430175781, ablated_edges=48\n",
      "loss.item()=-122.05601501464844, ablated_edges=49\n",
      "loss.item()=-123.47509002685547, ablated_edges=48\n",
      "loss.item()=-124.31399536132812, ablated_edges=50\n",
      "loss.item()=-127.36456298828125, ablated_edges=46\n",
      "Epochs trained:  130\n",
      "Loss: -127.3646\n",
      "Total preserved: 11562.5859\n",
      "Edges ablated:  46\n",
      "Toxic loss:  32.48900604248047\n",
      "OWT loss:  5.165423393249512\n",
      "Penalty:  tensor(126.0322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-126.47238159179688, ablated_edges=47\n",
      "loss.item()=-128.5781707763672, ablated_edges=49\n",
      "loss.item()=-130.5258331298828, ablated_edges=46\n",
      "loss.item()=-129.74996948242188, ablated_edges=47\n",
      "loss.item()=-132.48599243164062, ablated_edges=48\n",
      "loss.item()=-133.76837158203125, ablated_edges=47\n",
      "loss.item()=-132.95957946777344, ablated_edges=46\n",
      "loss.item()=-133.96261596679688, ablated_edges=45\n",
      "loss.item()=-135.52638244628906, ablated_edges=47\n",
      "loss.item()=-137.492431640625, ablated_edges=50\n",
      "Epochs trained:  140\n",
      "Loss: -137.4924\n",
      "Total preserved: 11560.9385\n",
      "Edges ablated:  50\n",
      "Toxic loss:  27.15222930908203\n",
      "OWT loss:  5.513188362121582\n",
      "Penalty:  tensor(137.5752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-138.09088134765625, ablated_edges=46\n",
      "loss.item()=-139.6414031982422, ablated_edges=46\n",
      "loss.item()=-140.9293975830078, ablated_edges=45\n",
      "loss.item()=-141.51370239257812, ablated_edges=49\n",
      "loss.item()=-144.06930541992188, ablated_edges=46\n",
      "loss.item()=-143.3538055419922, ablated_edges=47\n",
      "loss.item()=-146.1291046142578, ablated_edges=44\n",
      "loss.item()=-146.28192138671875, ablated_edges=47\n",
      "loss.item()=-148.14944458007812, ablated_edges=48\n",
      "loss.item()=-150.17247009277344, ablated_edges=50\n",
      "Epochs trained:  150\n",
      "Loss: -150.1725\n",
      "Total preserved: 11561.4805\n",
      "Edges ablated:  50\n",
      "Toxic loss:  33.0401725769043\n",
      "OWT loss:  5.578667640686035\n",
      "Penalty:  tensor(149.1431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-152.55580139160156, ablated_edges=49\n",
      "loss.item()=-151.85586547851562, ablated_edges=49\n",
      "loss.item()=-153.99522399902344, ablated_edges=46\n",
      "loss.item()=-153.05007934570312, ablated_edges=47\n",
      "loss.item()=-155.26657104492188, ablated_edges=45\n",
      "loss.item()=-155.52505493164062, ablated_edges=49\n",
      "loss.item()=-158.27174377441406, ablated_edges=46\n",
      "loss.item()=-158.82046508789062, ablated_edges=43\n",
      "loss.item()=-159.7530059814453, ablated_edges=48\n",
      "loss.item()=-161.92526245117188, ablated_edges=46\n",
      "Epochs trained:  160\n",
      "Loss: -161.9253\n",
      "Total preserved: 11562.2822\n",
      "Edges ablated:  46\n",
      "Toxic loss:  33.64940643310547\n",
      "OWT loss:  5.520321846008301\n",
      "Penalty:  tensor(160.7157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-161.7630157470703, ablated_edges=43\n",
      "loss.item()=-162.72824096679688, ablated_edges=43\n",
      "loss.item()=-163.37709045410156, ablated_edges=47\n",
      "loss.item()=-164.8791961669922, ablated_edges=44\n",
      "loss.item()=-167.94241333007812, ablated_edges=45\n",
      "loss.item()=-168.17257690429688, ablated_edges=46\n",
      "loss.item()=-168.0404052734375, ablated_edges=47\n",
      "loss.item()=-169.7171630859375, ablated_edges=49\n",
      "loss.item()=-171.64768981933594, ablated_edges=46\n",
      "loss.item()=-174.03814697265625, ablated_edges=43\n",
      "Epochs trained:  170\n",
      "Loss: -174.0381\n",
      "Total preserved: 11564.3203\n",
      "Edges ablated:  43\n",
      "Toxic loss:  39.49759292602539\n",
      "OWT loss:  6.16975736618042\n",
      "Penalty:  tensor(172.3084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-174.89840698242188, ablated_edges=45\n",
      "loss.item()=-174.6654510498047, ablated_edges=48\n",
      "loss.item()=-174.72088623046875, ablated_edges=45\n",
      "loss.item()=-178.4546661376953, ablated_edges=46\n",
      "loss.item()=-177.6720428466797, ablated_edges=47\n",
      "loss.item()=-179.65830993652344, ablated_edges=48\n",
      "loss.item()=-179.35630798339844, ablated_edges=41\n",
      "loss.item()=-183.06385803222656, ablated_edges=44\n",
      "loss.item()=-184.6969757080078, ablated_edges=46\n",
      "loss.item()=-184.05078125, ablated_edges=44\n",
      "Epochs trained:  180\n",
      "Loss: -184.0508\n",
      "Total preserved: 11566.2373\n",
      "Edges ablated:  44\n",
      "Toxic loss:  28.797325134277344\n",
      "OWT loss:  5.611840724945068\n",
      "Penalty:  tensor(183.9032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-184.4897003173828, ablated_edges=47\n",
      "loss.item()=-185.37271118164062, ablated_edges=46\n",
      "loss.item()=-186.9844207763672, ablated_edges=43\n",
      "loss.item()=-188.4407501220703, ablated_edges=46\n",
      "loss.item()=-189.3105010986328, ablated_edges=44\n",
      "loss.item()=-191.02035522460938, ablated_edges=44\n",
      "loss.item()=-191.65814208984375, ablated_edges=43\n",
      "loss.item()=-191.11868286132812, ablated_edges=45\n",
      "loss.item()=-195.22610473632812, ablated_edges=44\n",
      "loss.item()=-195.29010009765625, ablated_edges=44\n",
      "Epochs trained:  190\n",
      "Loss: -195.2901\n",
      "Total preserved: 11565.8291\n",
      "Edges ablated:  44\n",
      "Toxic loss:  25.891029357910156\n",
      "OWT loss:  5.3506364822387695\n",
      "Penalty:  tensor(195.4625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-197.07337951660156, ablated_edges=46\n",
      "loss.item()=-197.45147705078125, ablated_edges=47\n",
      "loss.item()=-199.31268310546875, ablated_edges=44\n",
      "loss.item()=-198.38023376464844, ablated_edges=43\n",
      "loss.item()=-201.58419799804688, ablated_edges=44\n",
      "loss.item()=-199.99661254882812, ablated_edges=44\n",
      "loss.item()=-202.88490295410156, ablated_edges=46\n",
      "loss.item()=-203.29649353027344, ablated_edges=44\n",
      "loss.item()=-205.46502685546875, ablated_edges=41\n",
      "loss.item()=-207.99468994140625, ablated_edges=43\n",
      "Epochs trained:  200\n",
      "Loss: -207.9947\n",
      "Total preserved: 11567.3525\n",
      "Edges ablated:  43\n",
      "Toxic loss:  30.689029693603516\n",
      "OWT loss:  5.198721885681152\n",
      "Penalty:  tensor(207.0556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m     prev_params \u001b[39m=\u001b[39m mask_params\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m epochs_left \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39minput\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mcontinue training for this number of epochs: \u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m log_every \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(\u001b[39minput\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mset log frequency\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m edge_threshold \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39minput\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mset edge threshold\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "toxic_batch_size = 1 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 5\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "    means = pickle.load(f)[0][0]\n",
    "\n",
    "model = load_demo_gpt2(means=means, mask_dict_superset=acdc_mask_dict)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 20 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 0.2 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 0\n",
    "max_steps_per_epoch = 100\n",
    "\n",
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "            # print(f\"{ablated_edges=}\")\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = -1 * (regularization_strength * penalty + alpha * tox_loss) + owt_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            print(\"\\n\")\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    log_every = int(input('set log frequency'))\n",
    "    edge_threshold = int(input('set edge threshold'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/alternative_acdc_cb_subset_mask_params_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model before and after circuit breaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/ioi_sentences_test.pkl\", \"rb\") as f:\n",
    "    ioi_sentences_test = pickle.load(f)\n",
    "    # ioi_sentences_test = [t[2] for t in ioi_sentences_test]\n",
    "\n",
    "with open(\"data/eval_uniform.pkl\", \"rb\") as f:\n",
    "    uniform_samples = pickle.load(f)\n",
    "    uniform_sentences = [t[2] for t in uniform_samples]\n",
    "\n",
    "original_model = load_demo_gpt2(means=False)\n",
    "\n",
    "# with open(\"models/masked_gpt2_mean_ablation_v6.pkl\", \"rb\") as f:\n",
    "#     model.state_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on an ioi_sentence\n",
    "ioi_sentence = ioi_sentences_test[0]\n",
    "print(ioi_sentence)\n",
    "# ioi_tokens = tokenizer(ioi_sentence, return_tensors='pt').input_ids.to('cuda')\n",
    "\n",
    "original_model.eval()\n",
    "original_model.to('cuda')\n",
    "def get_last_token(model, prompt, topk=5):\n",
    "    # generate last token\n",
    "    tokens = tokenizer(prompt, return_tensors='pt').input_ids[:, :-1]\n",
    "\n",
    "    # generate one token, decode original_model(ioi_tokens[:, :-1])\n",
    "    model_outputs = model(tokens)[0]\n",
    "    model_outputs = model_outputs.squeeze(0)[-1]\n",
    "    probs = torch.nn.functional.softmax(model_outputs, dim=-1)\n",
    "\n",
    "    topk_outputs = torch.topk(model_outputs, topk)\n",
    "    topk_tokens = topk_outputs.indices\n",
    "    topk_probs = probs[topk_outputs.indices]\n",
    "    \n",
    "    # decode tokens\n",
    "    for i in range(topk):\n",
    "        print(f\"{tokenizer.decode(topk_tokens[i].unsqueeze(0))}, probability of {topk_probs[i]}\")\n",
    "    topk_tokens_decoded = tokenizer.batch_decode(topk_tokens)\n",
    "    return topk_tokens_decoded, topk_probs\n",
    "\n",
    "print(\"Before ablation\")\n",
    "_ = get_last_token(original_model, ioi_sentence)\n",
    "print()\n",
    "print()\n",
    "print(\"After ablation\")\n",
    "_ = get_last_token(model, ioi_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try on uniform samples\n",
    "for idx in range(3):\n",
    "    print(uniform_sentences[idx])\n",
    "    print(\"Before ablation\")\n",
    "    _ = get_last_token(original_model, uniform_sentences[idx])\n",
    "    print()\n",
    "    print(\"After ablation\")\n",
    "    _ = get_last_token(model, uniform_sentences[idx])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize mask\n",
    "Create the computational graphs in edge attribution patching paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate which nodes will be in the graph\n",
    "connected_nodes = set()\n",
    "# add embed node at position\n",
    "# connected_nodes.add((-1, \"embed\"))\n",
    "n_heads = 12\n",
    "n_layers = 12\n",
    "\n",
    "# associate each node with a position\n",
    "all_possible_nodes = [(-1, \"embed\")]\n",
    "mask_dict = {}\n",
    "# empty tensor\n",
    "mask_dict[\"embed\"] = torch.zeros(size=(0,))\n",
    "for idx in range(len(mask_params)):\n",
    "    if \"attention\" in param_names[idx]:\n",
    "        layer = int(param_names[idx].split(\".\")[1])\n",
    "        for i in range(n_heads):\n",
    "            all_possible_nodes.append((layer, f\"a{layer}.{i}\"))\n",
    "            mask_dict[f\"a{layer}.{i}\"] = mask_params[idx][:,i].detach().cpu()\n",
    "    elif \"mlp\" in param_names[idx]:\n",
    "        layer = int(param_names[idx].split(\".\")[1])\n",
    "        all_possible_nodes.append((layer, f\"m{layer}\"))\n",
    "        mask_dict[f\"m{layer}\"] = mask_params[idx].detach().cpu()\n",
    "all_possible_nodes.append((n_heads, \"output\"))\n",
    "mask_dict[\"output\"] = mask_params[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate where edges are based on the mask\n",
    "# Edge between node i and node j if mask_dict[i][all_possible_nodes.index(j)] == 0\n",
    "sufficient = True\n",
    "\n",
    "edges = []\n",
    "for i in range(len(all_possible_nodes)):\n",
    "    for j in range(len(all_possible_nodes)):\n",
    "        j_index = all_possible_nodes.index(all_possible_nodes[j])\n",
    "        if j_index < len(mask_dict[all_possible_nodes[i][1]]) and mask_dict[all_possible_nodes[i][1]][all_possible_nodes.index(all_possible_nodes[j])] == (1 if sufficient else 0):\n",
    "            edges.append((all_possible_nodes[i], all_possible_nodes[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aligned_graph(all_possible_nodes, edges):\n",
    "    G = pgv.AGraph(strict=False, directed=True)\n",
    "\n",
    "    # Find the maximum layer number for adjusting the graph\n",
    "    max_layer = max(layer for layer, _ in all_possible_nodes if isinstance(layer, int))\n",
    "    nodes_with_edges = set([node for edge in edges for node in edge])\n",
    "    print(nodes_with_edges)\n",
    "    # Add nodes and edges to the graph\n",
    "    # for node in all_possible_nodes:\n",
    "    #     if node in [edge[0] for edge in edges] or node in [edge[1] for edge in edges]:\n",
    "    #         G.add_node(node[1], layer=str(max_layer - node[0]))\n",
    "\n",
    "    for edge in edges:\n",
    "        G.add_edge(edge[1][1], edge[0][1])\n",
    "\n",
    "    # Create subgraphs to ensure nodes of the same layer have the same rank\n",
    "    for layer in range(max_layer, -2, -1):\n",
    "        with G.subgraph(name=f'cluster_{layer}') as s:\n",
    "            s.graph_attr['rank'] = 'same'\n",
    "            for node in nodes_with_edges:\n",
    "                if node[0] == layer:\n",
    "                    s.add_node(node[1])\n",
    "\n",
    "    # Apply layout and render the graph\n",
    "    G.layout(prog='dot')\n",
    "    G.draw('aligned_graph.png')\n",
    "    return Image('aligned_graph.png')\n",
    "\n",
    "# Call the function with your nodes and edges\n",
    "flipped_graph_image = create_aligned_graph(all_possible_nodes, edges)\n",
    "\n",
    "# To display the graph in Jupyter Notebook\n",
    "flipped_graph_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "unlrn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
