{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train different kinds of masks over IOI edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir(\"/data/phillip_guo/circuit-breaking/ioi/\")\n",
    "from models import load_gpt2_weights, load_demo_gpt2, tokenizer\n",
    "from data import retrieve_toxic_data, retrieve_owt_data, retrieve_toxic_data_low_loss, retrieve_toxic_filtered_data, FILTER_DEMO_LEN, CONTEXT_LENGTH\n",
    "from inference import infer_batch_with_owt, infer_batch, prepare_fixed_demo, criterion\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import pickle\n",
    "import datasets\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from itertools import cycle\n",
    "# from eval import evaluate_model\n",
    "from data import batch_text_to_tokens\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train params of mask\n",
    "Train without the original D_train loss term (only mask loss and IOI data loss)\n",
    "Finds necessary (but not sufficient) edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_batch_size = 10 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 1\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "\n",
    "template_type = \"single\"\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None, template_type=template_type)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "# with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "#     means = pickle.load(f)[0][0]\n",
    "means_ioi = True\n",
    "if means_ioi:\n",
    "    with open(\"data/gpt2_ioi_abc_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "else:\n",
    "    with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "\n",
    "model = load_demo_gpt2(means=means)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 50 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 1 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 100\n",
    "max_steps_per_epoch = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Then, Jeremy and Jacob went to the restaurant. Jeremy gave a basketball to Jacob', 'Then, Tiffany and Michelle went to the school. Tiffany gave a ring to Michelle', 'Then, Tiffany and Samuel went to the restaurant. Tiffany gave a drink to Samuel', 'Then, Sarah and Joshua went to the hospital. Sarah gave a drink to Joshua', 'Then, Alexander and Katie went to the school. Alexander gave a kiss to Katie', 'Then, Nicholas and Steven went to the office. Nicholas gave a drink to Steven', 'Then, Patrick and Kristen went to the school. Patrick gave a computer to Kristen', 'Then, Jesse and Joseph went to the house. Jesse gave a computer to Joseph', 'Then, Jeffrey and Lisa went to the hospital. Jeffrey gave a computer to Lisa', 'Then, Richard and Elizabeth went to the house. Richard gave a drink to Elizabeth']}\n",
      "{'text': ['Then, Emily and Kelly went to the office. Emily gave a kiss to Kelly', 'Then, Cody and Matthew went to the station. Cody gave a snack to Matthew', 'Then, Brandon and David went to the office. Brandon gave a bone to David', 'Then, Stephen and Melissa went to the hospital. Stephen gave a ring to Melissa', 'Then, Nicole and Kimberly went to the store. Nicole gave a snack to Kimberly', 'Then, Kyle and Emily went to the station. Kyle gave a necklace to Emily', 'Then, Paul and Dustin went to the restaurant. Paul gave a bone to Dustin', 'Then, Thomas and Ashley went to the office. Thomas gave a kiss to Ashley', 'Then, Nicholas and Kenneth went to the hospital. Nicholas gave a necklace to Kenneth', 'Then, Alexander and Emily went to the hospital. Alexander gave a snack to Emily']}\n",
      "{'text': ['Then, Sean and William went to the house. Sean gave a ring to William', 'Then, Daniel and Brandon went to the house. Daniel gave a computer to Brandon', 'Then, Andrew and Sara went to the school. Andrew gave a basketball to Sara', 'Then, Katherine and Kevin went to the garden. Katherine gave a kiss to Kevin', 'Then, Amber and Christine went to the house. Amber gave a ring to Christine', 'Then, Christopher and Steven went to the office. Christopher gave a basketball to Steven', 'Then, Aaron and Allison went to the station. Aaron gave a computer to Allison', 'Then, Mary and Rachel went to the station. Mary gave a basketball to Rachel', 'Then, Andrea and Andrew went to the house. Andrea gave a drink to Andrew', 'Then, Christina and Erin went to the office. Christina gave a ring to Erin']}\n",
      "{'text': ['Then, Jeffrey and Samantha went to the store. Jeffrey gave a drink to Samantha', 'Then, James and Lauren went to the house. James gave a basketball to Lauren', 'Then, Daniel and Kenneth went to the station. Daniel gave a necklace to Kenneth', 'Then, Andrew and James went to the restaurant. Andrew gave a drink to James', 'Then, Amy and Timothy went to the store. Amy gave a computer to Timothy', 'Then, Shannon and Stephen went to the restaurant. Shannon gave a kiss to Stephen', 'Then, Danielle and Jeremy went to the restaurant. Danielle gave a ring to Jeremy', 'Then, James and Andrew went to the restaurant. James gave a drink to Andrew', 'Then, Michelle and Amy went to the office. Michelle gave a bone to Amy', 'Then, Adam and Ashley went to the hospital. Adam gave a bone to Ashley']}\n",
      "{'text': ['Then, Anthony and Jesse went to the garden. Anthony gave a basketball to Jesse', 'Then, Katherine and Jason went to the restaurant. Katherine gave a ring to Jason', 'Then, Kristen and Adam went to the restaurant. Kristen gave a ring to Adam', 'Then, Michael and Steven went to the hospital. Michael gave a necklace to Steven', 'Then, Christina and Kelly went to the garden. Christina gave a snack to Kelly', 'Then, Emily and Anthony went to the school. Emily gave a necklace to Anthony', 'Then, Amanda and Michelle went to the restaurant. Amanda gave a snack to Michelle', 'Then, Jason and Kimberly went to the restaurant. Jason gave a computer to Kimberly', 'Then, Ryan and Katherine went to the house. Ryan gave a computer to Katherine', 'Then, Erin and Timothy went to the school. Erin gave a bone to Timothy']}\n",
      "{'text': ['Then, Lauren and Richard went to the office. Lauren gave a ring to Richard', 'Then, Stephanie and Melissa went to the garden. Stephanie gave a bone to Melissa', 'Then, Alexander and Bryan went to the restaurant. Alexander gave a computer to Bryan', 'Then, Daniel and Vanessa went to the school. Daniel gave a computer to Vanessa', 'Then, Tyler and Lisa went to the garden. Tyler gave a ring to Lisa', 'Then, James and Kimberly went to the hospital. James gave a computer to Kimberly', 'Then, Kimberly and Katie went to the station. Kimberly gave a kiss to Katie', 'Then, Megan and Timothy went to the hospital. Megan gave a snack to Timothy', 'Then, Lindsay and Heather went to the garden. Lindsay gave a basketball to Heather', 'Then, Megan and Sarah went to the station. Megan gave a drink to Sarah']}\n"
     ]
    }
   ],
   "source": [
    "for c, batch in enumerate(toxic_data_loader):\n",
    "    if c > 5:\n",
    "        break\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_239414/2478911762.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db06e828c9046a6a415afad3dbc3a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=-105.22795104980469, ablated_edges=3682\n",
      "loss.item()=-109.21211242675781, ablated_edges=4294\n",
      "loss.item()=-107.7078857421875, ablated_edges=4723\n",
      "loss.item()=-111.7557144165039, ablated_edges=5102\n",
      "loss.item()=-113.11031341552734, ablated_edges=5304\n",
      "loss.item()=-113.76216125488281, ablated_edges=5468\n",
      "loss.item()=-115.1998291015625, ablated_edges=5565\n",
      "loss.item()=-113.5888900756836, ablated_edges=5658\n",
      "loss.item()=-115.18148040771484, ablated_edges=5765\n",
      "loss.item()=-114.0505599975586, ablated_edges=5763\n",
      "Epochs trained:  10\n",
      "Loss: -114.0506\n",
      "Total preserved: 5810.1855\n",
      "Edges ablated:  5763\n",
      "Toxic loss:  114.0505599975586\n",
      "OWT loss:  10.377520561218262\n",
      "Penalty:  0\n",
      "Best Token: [' it'], P(Alicia) = 7.461976434631875e-26, logit diff = -1.4965858459472656\n",
      "Best Token: [' it'], P(Alicia) = 2.1077113065753974e-35, logit diff = -58.636775493621826\n",
      "\n",
      "\n",
      "loss.item()=-116.17030334472656, ablated_edges=5832\n",
      "loss.item()=-116.09373474121094, ablated_edges=5848\n",
      "loss.item()=-115.07530212402344, ablated_edges=5857\n",
      "loss.item()=-113.7989273071289, ablated_edges=5880\n",
      "loss.item()=-114.3002700805664, ablated_edges=5896\n",
      "loss.item()=-117.05613708496094, ablated_edges=5896\n",
      "loss.item()=-115.81672668457031, ablated_edges=5919\n",
      "loss.item()=-118.58927917480469, ablated_edges=5952\n",
      "loss.item()=-116.61746978759766, ablated_edges=5959\n",
      "loss.item()=-116.6511001586914, ablated_edges=5951\n",
      "Epochs trained:  20\n",
      "Loss: -116.6511\n",
      "Total preserved: 5659.2983\n",
      "Edges ablated:  5951\n",
      "Toxic loss:  116.6511001586914\n",
      "OWT loss:  13.232407569885254\n",
      "Penalty:  0\n",
      "Best Token: [' it'], P(Alicia) = 3.1396401619841347e-29, logit diff = 5.089717864990234\n",
      "Best Token: [' it'], P(Alicia) = 5.678822282510287e-39, logit diff = -50.10035705566406\n",
      "\n",
      "\n",
      "loss.item()=-116.31449890136719, ablated_edges=5999\n",
      "loss.item()=-116.38764190673828, ablated_edges=5914\n",
      "loss.item()=-118.29852294921875, ablated_edges=5754\n",
      "loss.item()=-118.32965850830078, ablated_edges=5565\n",
      "loss.item()=-120.72006225585938, ablated_edges=5317\n",
      "loss.item()=-120.72425079345703, ablated_edges=5086\n",
      "loss.item()=-120.7771224975586, ablated_edges=4796\n",
      "loss.item()=-120.50503540039062, ablated_edges=4525\n",
      "loss.item()=-123.05567932128906, ablated_edges=4307\n",
      "loss.item()=-123.64910888671875, ablated_edges=4062\n",
      "Epochs trained:  30\n",
      "Loss: -123.6491\n",
      "Total preserved: 7545.9771\n",
      "Edges ablated:  4062\n",
      "Toxic loss:  116.85772705078125\n",
      "OWT loss:  11.329231262207031\n",
      "Penalty:  tensor(6.7914, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' it'], P(Alicia) = 1.4074039011055484e-23, logit diff = 6.052814483642578\n",
      "Best Token: [' it'], P(Alicia) = 3.392307087791155e-36, logit diff = -49.44533729553223\n",
      "\n",
      "\n",
      "loss.item()=-125.8162612915039, ablated_edges=3863\n",
      "loss.item()=-126.76332092285156, ablated_edges=3682\n",
      "loss.item()=-123.59192657470703, ablated_edges=3478\n",
      "loss.item()=-126.05734252929688, ablated_edges=3348\n",
      "loss.item()=-126.09983825683594, ablated_edges=3169\n",
      "loss.item()=-130.46095275878906, ablated_edges=3003\n",
      "loss.item()=-130.0868682861328, ablated_edges=2877\n",
      "loss.item()=-128.32254028320312, ablated_edges=2720\n",
      "loss.item()=-132.39059448242188, ablated_edges=2620\n",
      "loss.item()=-133.39166259765625, ablated_edges=2490\n",
      "Epochs trained:  40\n",
      "Loss: -133.3917\n",
      "Total preserved: 9007.7568\n",
      "Edges ablated:  2490\n",
      "Toxic loss:  116.27691650390625\n",
      "OWT loss:  12.250164031982422\n",
      "Penalty:  tensor(17.1147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' it'], P(Alicia) = 1.2458314930909705e-22, logit diff = 11.604011535644531\n",
      "Best Token: [' it'], P(Alicia) = 2.3756692042511097e-35, logit diff = -49.44961166381836\n",
      "\n",
      "\n",
      "loss.item()=-133.34251403808594, ablated_edges=2407\n",
      "loss.item()=-133.5218505859375, ablated_edges=2309\n",
      "loss.item()=-135.3507080078125, ablated_edges=2244\n",
      "loss.item()=-139.88079833984375, ablated_edges=2164\n",
      "loss.item()=-139.74110412597656, ablated_edges=2086\n",
      "loss.item()=-139.81341552734375, ablated_edges=2055\n",
      "loss.item()=-138.97604370117188, ablated_edges=1974\n",
      "loss.item()=-144.12777709960938, ablated_edges=1920\n",
      "loss.item()=-144.38587951660156, ablated_edges=1848\n",
      "loss.item()=-142.21182250976562, ablated_edges=1830\n",
      "Epochs trained:  50\n",
      "Loss: -142.2118\n",
      "Total preserved: 9683.4404\n",
      "Edges ablated:  1831\n",
      "Toxic loss:  114.12984466552734\n",
      "OWT loss:  11.623367309570312\n",
      "Penalty:  tensor(28.0820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['\\n'], P(Alicia) = 3.755135646876795e-10, logit diff = -5.425514221191406\n",
      "Best Token: [':'], P(Alicia) = 9.554953495083396e-10, logit diff = -4.579383850097656\n",
      "\n",
      "\n",
      "loss.item()=-133.96730041503906, ablated_edges=1975\n",
      "loss.item()=-147.1227569580078, ablated_edges=1676\n",
      "loss.item()=-154.7083740234375, ablated_edges=1482\n",
      "loss.item()=-156.69862365722656, ablated_edges=1326\n",
      "loss.item()=-155.41903686523438, ablated_edges=1247\n",
      "loss.item()=-157.20361328125, ablated_edges=1196\n",
      "loss.item()=-161.85540771484375, ablated_edges=1183\n",
      "loss.item()=-159.5000457763672, ablated_edges=1139\n",
      "loss.item()=-162.7687225341797, ablated_edges=1099\n",
      "loss.item()=-164.81857299804688, ablated_edges=1091\n",
      "Epochs trained:  60\n",
      "Loss: -164.8186\n",
      "Total preserved: 10440.1602\n",
      "Edges ablated:  1091\n",
      "Toxic loss:  124.10194396972656\n",
      "OWT loss:  22.853742599487305\n",
      "Penalty:  tensor(40.7166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['.'], P(Alicia) = 6.711693204308796e-36, logit diff = 27.85028839111328\n",
      "Best Token: ['.'], P(Alicia) = 0.0, logit diff = -38.60600280761719\n",
      "\n",
      "\n",
      "loss.item()=-165.96783447265625, ablated_edges=1071\n",
      "loss.item()=-169.5971221923828, ablated_edges=1041\n",
      "loss.item()=-167.33502197265625, ablated_edges=1026\n",
      "loss.item()=-162.61984252929688, ablated_edges=1011\n",
      "loss.item()=-167.1485137939453, ablated_edges=1256\n",
      "loss.item()=-169.1515655517578, ablated_edges=1065\n",
      "loss.item()=-170.054931640625, ablated_edges=1004\n",
      "loss.item()=-171.1484375, ablated_edges=967\n",
      "loss.item()=-171.56381225585938, ablated_edges=946\n",
      "loss.item()=-173.72738647460938, ablated_edges=926\n",
      "Epochs trained:  70\n",
      "Loss: -173.7274\n",
      "Total preserved: 10610.3154\n",
      "Edges ablated:  926\n",
      "Toxic loss:  121.73685455322266\n",
      "OWT loss:  24.4600772857666\n",
      "Penalty:  tensor(51.9905, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['.'], P(Alicia) = 2.311768487601789e-37, logit diff = 30.038734436035156\n",
      "Best Token: ['.'], P(Alicia) = 0.0, logit diff = -46.98979949951172\n",
      "\n",
      "\n",
      "loss.item()=-175.40037536621094, ablated_edges=912\n",
      "loss.item()=-173.73695373535156, ablated_edges=894\n",
      "loss.item()=-178.06582641601562, ablated_edges=882\n",
      "loss.item()=-179.44662475585938, ablated_edges=855\n",
      "loss.item()=-181.02667236328125, ablated_edges=837\n",
      "loss.item()=-181.71646118164062, ablated_edges=830\n",
      "loss.item()=-183.34381103515625, ablated_edges=815\n",
      "loss.item()=-183.18614196777344, ablated_edges=817\n",
      "loss.item()=-183.3199462890625, ablated_edges=801\n",
      "loss.item()=-185.04673767089844, ablated_edges=793\n",
      "Epochs trained:  80\n",
      "Loss: -185.0467\n",
      "Total preserved: 10734.1396\n",
      "Edges ablated:  793\n",
      "Toxic loss:  121.71531677246094\n",
      "OWT loss:  27.003128051757812\n",
      "Penalty:  tensor(63.3314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['.'], P(Alicia) = 2.1638987652833943e-37, logit diff = 25.495254516601562\n",
      "Best Token: ['.'], P(Alicia) = 0.0, logit diff = -44.27381896972656\n",
      "\n",
      "\n",
      "loss.item()=-185.760009765625, ablated_edges=794\n",
      "loss.item()=-189.6227264404297, ablated_edges=769\n",
      "loss.item()=-189.10086059570312, ablated_edges=761\n",
      "loss.item()=-190.12477111816406, ablated_edges=760\n",
      "loss.item()=-188.56915283203125, ablated_edges=756\n",
      "loss.item()=-193.86904907226562, ablated_edges=756\n",
      "loss.item()=-193.9090576171875, ablated_edges=745\n",
      "loss.item()=-194.50265502929688, ablated_edges=727\n",
      "loss.item()=-197.20440673828125, ablated_edges=727\n",
      "loss.item()=-195.15696716308594, ablated_edges=711\n",
      "Epochs trained:  90\n",
      "Loss: -195.1570\n",
      "Total preserved: 10800.5410\n",
      "Edges ablated:  711\n",
      "Toxic loss:  120.63323974609375\n",
      "OWT loss:  17.365108489990234\n",
      "Penalty:  tensor(74.5237, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['.'], P(Alicia) = 1.0748514001038567e-35, logit diff = 31.292282104492188\n",
      "Best Token: ['.'], P(Alicia) = 0.0, logit diff = -45.77799987792969\n",
      "\n",
      "\n",
      "loss.item()=-196.4091033935547, ablated_edges=725\n",
      "loss.item()=-197.66192626953125, ablated_edges=706\n",
      "loss.item()=-198.3797607421875, ablated_edges=688\n",
      "loss.item()=-200.76278686523438, ablated_edges=691\n",
      "loss.item()=-202.64312744140625, ablated_edges=687\n",
      "loss.item()=-203.65438842773438, ablated_edges=678\n",
      "loss.item()=-201.52761840820312, ablated_edges=683\n",
      "loss.item()=-206.11512756347656, ablated_edges=682\n",
      "loss.item()=-205.96231079101562, ablated_edges=676\n",
      "loss.item()=-204.134521484375, ablated_edges=679\n",
      "Epochs trained:  100\n",
      "Loss: -204.1345\n",
      "Total preserved: 10854.3145\n",
      "Edges ablated:  679\n",
      "Toxic loss:  118.38545227050781\n",
      "OWT loss:  24.76450538635254\n",
      "Penalty:  tensor(85.7491, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['ing'], P(Alicia) = 3.860695763080457e-09, logit diff = -1.7248077392578125\n",
      "Best Token: ['ing'], P(Alicia) = 1.8973678184153187e-09, logit diff = -1.51824951171875\n",
      "\n",
      "\n",
      "loss.item()=-211.63204956054688, ablated_edges=939\n",
      "loss.item()=-217.730224609375, ablated_edges=747\n",
      "loss.item()=-210.58966064453125, ablated_edges=710\n",
      "loss.item()=-220.1268768310547, ablated_edges=684\n",
      "loss.item()=-224.51150512695312, ablated_edges=674\n",
      "loss.item()=-225.81375122070312, ablated_edges=656\n",
      "loss.item()=-230.19032287597656, ablated_edges=663\n",
      "loss.item()=-231.2166290283203, ablated_edges=652\n",
      "loss.item()=-233.3267822265625, ablated_edges=640\n",
      "loss.item()=-230.23873901367188, ablated_edges=637\n",
      "Epochs trained:  110\n",
      "Loss: -230.2387\n",
      "Total preserved: 10888.2549\n",
      "Edges ablated:  637\n",
      "Toxic loss:  133.33328247070312\n",
      "OWT loss:  29.98503875732422\n",
      "Penalty:  tensor(96.9055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['ing'], P(Alicia) = 2.2420775429197073e-44, logit diff = 7.636688232421875\n",
      "Best Token: ['ing'], P(Alicia) = 0.0, logit diff = -17.176651000976562\n",
      "\n",
      "\n",
      "loss.item()=-234.9921875, ablated_edges=647\n",
      "loss.item()=-237.63893127441406, ablated_edges=639\n",
      "loss.item()=-236.5364990234375, ablated_edges=646\n",
      "loss.item()=-239.05001831054688, ablated_edges=661\n",
      "loss.item()=-242.54486083984375, ablated_edges=667\n",
      "loss.item()=-242.42337036132812, ablated_edges=669\n",
      "loss.item()=-242.13656616210938, ablated_edges=637\n",
      "loss.item()=-248.06881713867188, ablated_edges=631\n",
      "loss.item()=-247.91722106933594, ablated_edges=638\n",
      "loss.item()=-249.92373657226562, ablated_edges=639\n",
      "Epochs trained:  120\n",
      "Loss: -249.9237\n",
      "Total preserved: 10885.2930\n",
      "Edges ablated:  639\n",
      "Toxic loss:  142.15933227539062\n",
      "OWT loss:  32.450984954833984\n",
      "Penalty:  tensor(107.7644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['ing'], P(Alicia) = 0.0, logit diff = 15.282821655273438\n",
      "Best Token: ['ing'], P(Alicia) = 0.0, logit diff = -25.338390350341797\n",
      "\n",
      "\n",
      "loss.item()=-247.7645721435547, ablated_edges=647\n",
      "loss.item()=-248.3267822265625, ablated_edges=662\n",
      "loss.item()=-252.19833374023438, ablated_edges=655\n",
      "loss.item()=-253.02792358398438, ablated_edges=648\n",
      "loss.item()=-255.14035034179688, ablated_edges=640\n",
      "loss.item()=-256.2718505859375, ablated_edges=629\n",
      "loss.item()=-258.0471496582031, ablated_edges=612\n",
      "loss.item()=-257.3164978027344, ablated_edges=603\n",
      "loss.item()=-257.648193359375, ablated_edges=608\n",
      "loss.item()=-261.7375183105469, ablated_edges=608\n",
      "Epochs trained:  130\n",
      "Loss: -261.7375\n",
      "Total preserved: 10930.7920\n",
      "Edges ablated:  608\n",
      "Toxic loss:  142.59188842773438\n",
      "OWT loss:  30.218549728393555\n",
      "Penalty:  tensor(119.1456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['ing'], P(Alicia) = 0.0, logit diff = 20.160133361816406\n",
      "Best Token: ['ing'], P(Alicia) = 0.0, logit diff = -22.4227294921875\n",
      "\n",
      "\n",
      "loss.item()=-264.5054931640625, ablated_edges=590\n",
      "loss.item()=-263.3113708496094, ablated_edges=593\n",
      "loss.item()=-263.7192687988281, ablated_edges=586\n",
      "loss.item()=-263.2762145996094, ablated_edges=584\n",
      "loss.item()=-265.84075927734375, ablated_edges=576\n",
      "loss.item()=-268.1306457519531, ablated_edges=565\n",
      "loss.item()=-269.0431213378906, ablated_edges=567\n",
      "loss.item()=-269.8200378417969, ablated_edges=578\n",
      "loss.item()=-271.03387451171875, ablated_edges=574\n",
      "loss.item()=-272.78118896484375, ablated_edges=575\n",
      "Epochs trained:  140\n",
      "Loss: -272.7812\n",
      "Total preserved: 10971.9531\n",
      "Edges ablated:  575\n",
      "Toxic loss:  142.21493530273438\n",
      "OWT loss:  31.22026824951172\n",
      "Penalty:  tensor(130.5662, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['ing'], P(Alicia) = 0.0, logit diff = 21.4534912109375\n",
      "Best Token: ['ing'], P(Alicia) = 0.0, logit diff = -21.066242218017578\n",
      "\n",
      "\n",
      "loss.item()=-272.4288635253906, ablated_edges=572\n",
      "loss.item()=-275.02886962890625, ablated_edges=571\n",
      "loss.item()=-276.7781982421875, ablated_edges=568\n",
      "loss.item()=-277.7083435058594, ablated_edges=557\n",
      "loss.item()=-279.9417724609375, ablated_edges=554\n",
      "loss.item()=-277.9381103515625, ablated_edges=558\n",
      "loss.item()=-280.40704345703125, ablated_edges=556\n",
      "loss.item()=-282.8526611328125, ablated_edges=554\n",
      "loss.item()=-282.9175720214844, ablated_edges=547\n",
      "loss.item()=-284.0160217285156, ablated_edges=546\n",
      "Epochs trained:  150\n",
      "Loss: -284.0160\n",
      "Total preserved: 11006.7520\n",
      "Edges ablated:  544\n",
      "Toxic loss:  142.02890014648438\n",
      "OWT loss:  31.80817413330078\n",
      "Penalty:  tensor(141.9871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['-'], P(Alicia) = 4.0481632396270964e-14, logit diff = -2.7609405517578125\n",
      "Best Token: ['-'], P(Alicia) = 1.0896874223266517e-13, logit diff = -1.7152557373046875\n",
      "\n",
      "\n",
      "loss.item()=-248.35833740234375, ablated_edges=625\n",
      "loss.item()=-254.11990356445312, ablated_edges=588\n",
      "loss.item()=-259.44927978515625, ablated_edges=609\n",
      "loss.item()=-263.9761962890625, ablated_edges=611\n",
      "loss.item()=-262.2230224609375, ablated_edges=596\n",
      "loss.item()=-265.29248046875, ablated_edges=599\n",
      "loss.item()=-267.6518859863281, ablated_edges=594\n",
      "loss.item()=-270.6974182128906, ablated_edges=586\n",
      "loss.item()=-272.4508056640625, ablated_edges=562\n",
      "loss.item()=-271.8554382324219, ablated_edges=570\n",
      "Epochs trained:  160\n",
      "Loss: -271.8554\n",
      "Total preserved: 10967.9785\n",
      "Edges ablated:  570\n",
      "Toxic loss:  119.4005355834961\n",
      "OWT loss:  20.99715232849121\n",
      "Penalty:  tensor(152.4549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['-'], P(Alicia) = 1.9522890204973351e-41, logit diff = 16.457550048828125\n",
      "Best Token: ['-'], P(Alicia) = 0.0, logit diff = -23.68970489501953\n",
      "\n",
      "\n",
      "loss.item()=-274.3192138671875, ablated_edges=575\n",
      "loss.item()=-275.8363952636719, ablated_edges=555\n",
      "loss.item()=-276.6923828125, ablated_edges=548\n",
      "loss.item()=-276.71734619140625, ablated_edges=541\n",
      "loss.item()=-278.4800109863281, ablated_edges=563\n",
      "loss.item()=-278.6476745605469, ablated_edges=549\n",
      "loss.item()=-280.8092956542969, ablated_edges=542\n",
      "loss.item()=-280.58514404296875, ablated_edges=539\n",
      "loss.item()=-285.72412109375, ablated_edges=535\n",
      "loss.item()=-282.98651123046875, ablated_edges=524\n",
      "Epochs trained:  170\n",
      "Loss: -282.9865\n",
      "Total preserved: 11018.6494\n",
      "Edges ablated:  524\n",
      "Toxic loss:  118.80863952636719\n",
      "OWT loss:  21.79449462890625\n",
      "Penalty:  tensor(164.1779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['-'], P(Alicia) = 4.205274270663347e-39, logit diff = 20.30840301513672\n",
      "Best Token: ['-'], P(Alicia) = 0.0, logit diff = -27.489990234375\n",
      "\n",
      "\n",
      "loss.item()=-287.0790710449219, ablated_edges=521\n",
      "loss.item()=-286.58636474609375, ablated_edges=521\n",
      "loss.item()=-287.0751037597656, ablated_edges=513\n",
      "loss.item()=-286.2633361816406, ablated_edges=509\n",
      "loss.item()=-290.6695251464844, ablated_edges=495\n",
      "loss.item()=-289.1636047363281, ablated_edges=506\n",
      "loss.item()=-294.78509521484375, ablated_edges=504\n",
      "loss.item()=-294.15142822265625, ablated_edges=502\n",
      "loss.item()=-294.76416015625, ablated_edges=498\n",
      "loss.item()=-295.2433166503906, ablated_edges=494\n",
      "Epochs trained:  180\n",
      "Loss: -295.2433\n",
      "Total preserved: 11052.3994\n",
      "Edges ablated:  494\n",
      "Toxic loss:  119.5101547241211\n",
      "OWT loss:  23.71285057067871\n",
      "Penalty:  tensor(175.7332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['-'], P(Alicia) = 1.0712716564993578e-39, logit diff = 20.886749267578125\n",
      "Best Token: ['-'], P(Alicia) = 0.0, logit diff = -30.20086669921875\n",
      "\n",
      "\n",
      "loss.item()=-298.4069519042969, ablated_edges=494\n",
      "loss.item()=-299.5738830566406, ablated_edges=489\n",
      "loss.item()=-301.46826171875, ablated_edges=498\n",
      "loss.item()=-302.04364013671875, ablated_edges=490\n",
      "loss.item()=-304.7057800292969, ablated_edges=496\n",
      "loss.item()=-305.3751220703125, ablated_edges=492\n",
      "loss.item()=-306.21075439453125, ablated_edges=497\n",
      "loss.item()=-305.15692138671875, ablated_edges=486\n",
      "loss.item()=-305.19317626953125, ablated_edges=496\n",
      "loss.item()=-306.6537780761719, ablated_edges=486\n",
      "Epochs trained:  190\n",
      "Loss: -306.6538\n",
      "Total preserved: 11063.1240\n",
      "Edges ablated:  486\n",
      "Toxic loss:  119.68697357177734\n",
      "OWT loss:  19.004390716552734\n",
      "Penalty:  tensor(186.9668, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['-'], P(Alicia) = 1.5507158367191508e-38, logit diff = 18.01959228515625\n",
      "Best Token: ['-'], P(Alicia) = 0.0, logit diff = -34.038787841796875\n",
      "\n",
      "\n",
      "loss.item()=-306.0862731933594, ablated_edges=484\n",
      "loss.item()=-312.1082763671875, ablated_edges=490\n",
      "loss.item()=-310.4936218261719, ablated_edges=493\n",
      "loss.item()=-310.2020263671875, ablated_edges=500\n",
      "loss.item()=-284.55462646484375, ablated_edges=529\n",
      "loss.item()=-312.94012451171875, ablated_edges=538\n",
      "loss.item()=-314.47210693359375, ablated_edges=503\n",
      "loss.item()=-315.0995788574219, ablated_edges=475\n",
      "loss.item()=-318.8719787597656, ablated_edges=468\n",
      "loss.item()=-319.2048645019531, ablated_edges=473\n",
      "Epochs trained:  200\n",
      "Loss: -319.2049\n",
      "Total preserved: 11042.7422\n",
      "Edges ablated:  474\n",
      "Toxic loss:  121.53977966308594\n",
      "OWT loss:  22.135292053222656\n",
      "Penalty:  tensor(197.6651, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['.'], P(Alicia) = 1.9595560051444312e-14, logit diff = -1.9424896240234375\n",
      "Best Token: ['.'], P(Alicia) = 1.4112930083202881e-14, logit diff = -1.1424942016601562\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = -1 * (regularization_strength * penalty + alpha * tox_loss) #+ owt_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            \n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_ioi_sentences = [\"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\", \"While Joshua and Alicia were commuting to the restaurant, Joshua gave a snack to\"]\n",
    "                for test_ioi_sentence in test_ioi_sentences:\n",
    "                    correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "                    other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "                    test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "                    generation = model(test_ioi_tokens)[0][:, -1]\n",
    "                    probs = torch.softmax(generation, dim=-1)\n",
    "                    print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")\n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            print(\"\\n\")\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    # epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    # log_every = int(input('set log frequency'))\n",
    "    # edge_threshold = int(input('set edge threshold'))\n",
    "    epochs_left = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/alternative_necessary_masks_params_dict_lambda={regularization_strength}_{alpha=}_{means_ioi=}_{template_type=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different alternative: sufficient but not necessary\n",
    "Trains with an inverted loss function. This loss function encourages sparsity (as opposed to discouraging) and wants model to ablate everything but the necessary circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_batch_size = 10 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 1\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "\n",
    "template_type = \"single\"\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None, template_type=template_type)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "# with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "#     means = pickle.load(f)[0][0]\n",
    "means_ioi = True\n",
    "if means_ioi:\n",
    "    with open(\"data/gpt2_ioi_abc_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "else:\n",
    "    with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "        means = pickle.load(f)[0]\n",
    "\n",
    "model = load_demo_gpt2(means=means)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 50 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 1 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 100\n",
    "max_steps_per_epoch = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_239414/2053811722.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2145d045ccd64e2ea4e70d505f33b8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=1.6247709936578758e-05, ablated_edges=122\n",
      "loss.item()=6.13922475167783e-06, ablated_edges=141\n",
      "loss.item()=1.0323410606360994e-05, ablated_edges=168\n",
      "loss.item()=2.9802256449329434e-06, ablated_edges=181\n",
      "loss.item()=2.717954657782684e-06, ablated_edges=194\n",
      "loss.item()=5.483623795043968e-07, ablated_edges=199\n",
      "loss.item()=3.814694480297476e-07, ablated_edges=205\n",
      "loss.item()=9.179104267786897e-07, ablated_edges=213\n",
      "loss.item()=9.41752830385667e-07, ablated_edges=220\n",
      "loss.item()=1.1563283806026448e-06, ablated_edges=226\n",
      "Epochs trained:  10\n",
      "Loss: 0.0000\n",
      "Total preserved: 9732.8271\n",
      "Edges ablated:  226\n",
      "Toxic loss:  1.1563283806026448e-06\n",
      "OWT loss:  4.824834823608398\n",
      "Penalty:  0\n",
      "Best Token: [' Joshua'], P(Alicia) = 1.8541905788538315e-08, logit diff = -17.80322265625\n",
      "Best Token: [' Alicia'], P(Alicia) = 1.0, logit diff = 33.295623779296875\n",
      "\n",
      "\n",
      "loss.item()=1.1801693062807317e-06, ablated_edges=226\n",
      "loss.item()=2.8610210733859276e-07, ablated_edges=228\n",
      "loss.item()=4.2915289100164955e-07, ablated_edges=231\n",
      "loss.item()=3.576278473360617e-08, ablated_edges=234\n",
      "loss.item()=3.457067521139834e-07, ablated_edges=234\n",
      "loss.item()=2.5033941142282856e-07, ablated_edges=234\n",
      "loss.item()=2.8610219260372105e-07, ablated_edges=234\n",
      "loss.item()=2.3841843699301535e-07, ablated_edges=242\n",
      "loss.item()=1.311302071371756e-07, ablated_edges=244\n",
      "loss.item()=9.53674117454284e-08, ablated_edges=247\n",
      "Epochs trained:  20\n",
      "Loss: 0.0000\n",
      "Total preserved: 9708.9854\n",
      "Edges ablated:  247\n",
      "Toxic loss:  9.53674117454284e-08\n",
      "OWT loss:  5.172268390655518\n",
      "Penalty:  0\n",
      "Best Token: [' Joshua'], P(Alicia) = 2.789935837910207e-09, logit diff = -19.697242736816406\n",
      "Best Token: [' Alicia'], P(Alicia) = 1.0, logit diff = 34.71345520019531\n",
      "\n",
      "\n",
      "loss.item()=2.0265572686639644e-07, ablated_edges=252\n",
      "loss.item()=0.319571852684021, ablated_edges=8295\n",
      "loss.item()=0.3013332188129425, ablated_edges=10063\n",
      "loss.item()=0.2249298393726349, ablated_edges=10849\n",
      "loss.item()=0.2232503890991211, ablated_edges=11099\n",
      "loss.item()=0.6940387487411499, ablated_edges=10235\n",
      "loss.item()=0.36072269082069397, ablated_edges=10985\n",
      "loss.item()=0.31025227904319763, ablated_edges=11186\n",
      "loss.item()=0.22073915600776672, ablated_edges=11336\n",
      "loss.item()=0.17448922991752625, ablated_edges=11430\n",
      "Epochs trained:  30\n",
      "Loss: 0.1745\n",
      "Total preserved: 193.6635\n",
      "Edges ablated:  11430\n",
      "Toxic loss:  0.00019208001322112978\n",
      "OWT loss:  10.41128158569336\n",
      "Penalty:  tensor(0.1743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 3.696067974248707e-13, logit diff = -28.626169204711914\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9857199788093567, logit diff = 6.157489776611328\n",
      "\n",
      "\n",
      "loss.item()=0.15618930757045746, ablated_edges=11458\n",
      "loss.item()=0.11998365074396133, ablated_edges=11512\n",
      "loss.item()=0.679938018321991, ablated_edges=11016\n",
      "loss.item()=0.4472084641456604, ablated_edges=11250\n",
      "loss.item()=0.3375406265258789, ablated_edges=11367\n",
      "loss.item()=0.27404746413230896, ablated_edges=11429\n",
      "loss.item()=0.22486995160579681, ablated_edges=11474\n",
      "loss.item()=0.19097691774368286, ablated_edges=11506\n",
      "loss.item()=0.5155377388000488, ablated_edges=11337\n",
      "loss.item()=0.2851220369338989, ablated_edges=11462\n",
      "Epochs trained:  40\n",
      "Loss: 0.2851\n",
      "Total preserved: 150.0127\n",
      "Edges ablated:  11462\n",
      "Toxic loss:  9.800413681659847e-05\n",
      "OWT loss:  12.392659187316895\n",
      "Penalty:  tensor(0.2850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 3.248705837677335e-08, logit diff = -17.133920669555664\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.8300368189811707, logit diff = 5.292264938354492\n",
      "\n",
      "\n",
      "loss.item()=0.2103620022535324, ablated_edges=11517\n",
      "loss.item()=0.17335496842861176, ablated_edges=11538\n",
      "loss.item()=0.4709155857563019, ablated_edges=11548\n",
      "loss.item()=0.5811716318130493, ablated_edges=11357\n",
      "loss.item()=0.35417330265045166, ablated_edges=11465\n",
      "loss.item()=0.25779417157173157, ablated_edges=11505\n",
      "loss.item()=0.22201459109783173, ablated_edges=11525\n",
      "loss.item()=0.17747370898723602, ablated_edges=11553\n",
      "loss.item()=0.14414098858833313, ablated_edges=11568\n",
      "loss.item()=0.961333155632019, ablated_edges=11272\n",
      "Epochs trained:  50\n",
      "Loss: 0.9613\n",
      "Total preserved: 330.3491\n",
      "Edges ablated:  11278\n",
      "Toxic loss:  0.0033207752276211977\n",
      "OWT loss:  9.570430755615234\n",
      "Penalty:  tensor(0.9580, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.0002307159302290529, logit diff = -8.33016586303711\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.0007131856400519609, logit diff = -7.205219268798828\n",
      "\n",
      "\n",
      "loss.item()=0.6887408494949341, ablated_edges=11387\n",
      "loss.item()=0.4042663872241974, ablated_edges=11481\n",
      "loss.item()=0.2978704571723938, ablated_edges=11512\n",
      "loss.item()=0.24320121109485626, ablated_edges=11537\n",
      "loss.item()=0.44940730929374695, ablated_edges=11507\n",
      "loss.item()=0.22561323642730713, ablated_edges=11557\n",
      "loss.item()=0.16877765953540802, ablated_edges=11570\n",
      "loss.item()=0.6173847317695618, ablated_edges=11449\n",
      "loss.item()=0.3425728678703308, ablated_edges=11518\n",
      "loss.item()=0.3265089988708496, ablated_edges=11538\n",
      "Epochs trained:  60\n",
      "Loss: 0.3265\n",
      "Total preserved: 83.3040\n",
      "Edges ablated:  11538\n",
      "Toxic loss:  0.0016232287744060159\n",
      "OWT loss:  13.478694915771484\n",
      "Penalty:  tensor(0.3249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 3.173047762039527e-13, logit diff = -28.778591632843018\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9933813810348511, logit diff = 5.22246789932251\n",
      "\n",
      "\n",
      "loss.item()=0.2271868884563446, ablated_edges=11564\n",
      "loss.item()=0.4619322121143341, ablated_edges=11570\n",
      "loss.item()=0.19210273027420044, ablated_edges=11572\n",
      "loss.item()=0.13515160977840424, ablated_edges=11584\n",
      "loss.item()=0.1185600608587265, ablated_edges=11591\n",
      "loss.item()=1.8433539867401123, ablated_edges=11211\n",
      "loss.item()=0.6249874234199524, ablated_edges=11478\n",
      "loss.item()=0.6243516802787781, ablated_edges=11504\n",
      "loss.item()=0.30660444498062134, ablated_edges=11557\n",
      "loss.item()=0.23257824778556824, ablated_edges=11576\n",
      "Epochs trained:  70\n",
      "Loss: 0.2326\n",
      "Total preserved: 47.0105\n",
      "Edges ablated:  11576\n",
      "Toxic loss:  0.0022268083412200212\n",
      "OWT loss:  9.87032699584961\n",
      "Penalty:  tensor(0.2304, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 1.0207854933509328e-14, logit diff = -32.21539306640625\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9998552799224854, logit diff = 13.314144134521484\n",
      "\n",
      "\n",
      "loss.item()=0.20331047475337982, ablated_edges=11582\n",
      "loss.item()=0.15693813562393188, ablated_edges=11587\n",
      "loss.item()=0.3155769109725952, ablated_edges=11586\n",
      "loss.item()=0.5548756718635559, ablated_edges=11511\n",
      "loss.item()=0.31566354632377625, ablated_edges=11568\n",
      "loss.item()=0.21006228029727936, ablated_edges=11581\n",
      "loss.item()=0.1732494831085205, ablated_edges=11592\n",
      "loss.item()=0.13453400135040283, ablated_edges=11594\n",
      "loss.item()=1.2180767059326172, ablated_edges=11414\n",
      "loss.item()=0.39940574765205383, ablated_edges=11549\n",
      "Epochs trained:  80\n",
      "Loss: 0.3994\n",
      "Total preserved: 67.5816\n",
      "Edges ablated:  11549\n",
      "Toxic loss:  0.0006746620638296008\n",
      "OWT loss:  16.024730682373047\n",
      "Penalty:  tensor(0.3987, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 2.171756698088334e-11, logit diff = -24.552465438842773\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9999849796295166, logit diff = 16.07460117340088\n",
      "\n",
      "\n",
      "loss.item()=0.393204927444458, ablated_edges=11549\n",
      "loss.item()=0.8507313132286072, ablated_edges=11537\n",
      "loss.item()=0.48170387744903564, ablated_edges=11535\n",
      "loss.item()=0.740312933921814, ablated_edges=11499\n",
      "loss.item()=0.37205520272254944, ablated_edges=11554\n",
      "loss.item()=0.25861266255378723, ablated_edges=11581\n",
      "loss.item()=0.19804739952087402, ablated_edges=11592\n",
      "loss.item()=1.1825722455978394, ablated_edges=11515\n",
      "loss.item()=0.604996919631958, ablated_edges=11529\n",
      "loss.item()=1.0751445293426514, ablated_edges=11477\n",
      "Epochs trained:  90\n",
      "Loss: 1.0751\n",
      "Total preserved: 155.6732\n",
      "Edges ablated:  11477\n",
      "Toxic loss:  0.0009993573185056448\n",
      "OWT loss:  9.796628952026367\n",
      "Penalty:  tensor(1.0741, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 6.082291337250467e-10, logit diff = -21.21923828125\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9936608076095581, logit diff = 9.99691390991211\n",
      "\n",
      "\n",
      "loss.item()=0.38777902722358704, ablated_edges=11564\n",
      "loss.item()=0.24337808787822723, ablated_edges=11579\n",
      "loss.item()=0.22821803390979767, ablated_edges=11590\n",
      "loss.item()=1.6269639730453491, ablated_edges=11417\n",
      "loss.item()=0.5572272539138794, ablated_edges=11548\n",
      "loss.item()=0.2730368971824646, ablated_edges=11587\n",
      "loss.item()=1.0617988109588623, ablated_edges=11496\n",
      "loss.item()=0.38513511419296265, ablated_edges=11561\n",
      "loss.item()=0.31466904282569885, ablated_edges=11580\n",
      "loss.item()=0.18690092861652374, ablated_edges=11590\n",
      "Epochs trained:  100\n",
      "Loss: 0.1869\n",
      "Total preserved: 23.4576\n",
      "Edges ablated:  11590\n",
      "Toxic loss:  0.001586264232173562\n",
      "OWT loss:  16.11726951599121\n",
      "Penalty:  tensor(0.1853, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: ['Rap'], P(Alicia) = 3.552860783351264e-11, logit diff = -19.52667760848999\n",
      "Best Token: [' bol'], P(Alicia) = 1.0909881211773609e-06, logit diff = 0.015157222747802734\n",
      "\n",
      "\n",
      "loss.item()=3.2068636417388916, ablated_edges=11325\n",
      "loss.item()=1.6501730680465698, ablated_edges=11395\n",
      "loss.item()=1.3499364852905273, ablated_edges=11436\n",
      "loss.item()=1.159066915512085, ablated_edges=11462\n",
      "loss.item()=1.4400815963745117, ablated_edges=11441\n",
      "loss.item()=1.122125267982483, ablated_edges=11481\n",
      "loss.item()=1.7144488096237183, ablated_edges=11421\n",
      "loss.item()=0.9053667783737183, ablated_edges=11509\n",
      "loss.item()=0.7731857895851135, ablated_edges=11524\n",
      "loss.item()=1.4700313806533813, ablated_edges=11463\n",
      "Epochs trained:  110\n",
      "Loss: 1.4700\n",
      "Total preserved: 162.5984\n",
      "Edges ablated:  11463\n",
      "Toxic loss:  0.02290564589202404\n",
      "OWT loss:  9.67537784576416\n",
      "Penalty:  tensor(1.4471, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 1.3806369452140643e-06, logit diff = -13.470836639404297\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9635933041572571, logit diff = 5.405998229980469\n",
      "\n",
      "\n",
      "loss.item()=0.7750483155250549, ablated_edges=11527\n",
      "loss.item()=0.6755251288414001, ablated_edges=11537\n",
      "loss.item()=0.5586079955101013, ablated_edges=11555\n",
      "loss.item()=0.5321057438850403, ablated_edges=11559\n",
      "loss.item()=0.7706779837608337, ablated_edges=11552\n",
      "loss.item()=0.6042822003364563, ablated_edges=11562\n",
      "loss.item()=0.45224758982658386, ablated_edges=11570\n",
      "loss.item()=0.4208683669567108, ablated_edges=11574\n",
      "loss.item()=0.5494689345359802, ablated_edges=11571\n",
      "loss.item()=0.3786466121673584, ablated_edges=11576\n",
      "Epochs trained:  120\n",
      "Loss: 0.3786\n",
      "Total preserved: 37.7699\n",
      "Edges ablated:  11576\n",
      "Toxic loss:  0.004724876023828983\n",
      "OWT loss:  12.739434242248535\n",
      "Penalty:  tensor(0.3739, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.003874473972246051, logit diff = -5.365657806396484\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.07641198486089706, logit diff = -1.9694366455078125\n",
      "\n",
      "\n",
      "loss.item()=0.37407293915748596, ablated_edges=11581\n",
      "loss.item()=7.740462303161621, ablated_edges=11582\n",
      "loss.item()=0.7082314491271973, ablated_edges=11549\n",
      "loss.item()=0.4713258147239685, ablated_edges=11563\n",
      "loss.item()=0.410091370344162, ablated_edges=11571\n",
      "loss.item()=0.3653189241886139, ablated_edges=11576\n",
      "loss.item()=0.3242238163948059, ablated_edges=11582\n",
      "loss.item()=0.6517142057418823, ablated_edges=11549\n",
      "loss.item()=0.4886111915111542, ablated_edges=11572\n",
      "loss.item()=0.3934555947780609, ablated_edges=11576\n",
      "Epochs trained:  130\n",
      "Loss: 0.3935\n",
      "Total preserved: 36.0080\n",
      "Edges ablated:  11576\n",
      "Toxic loss:  0.0009678475325927138\n",
      "OWT loss:  10.519150733947754\n",
      "Penalty:  tensor(0.3925, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 5.0081677471780495e-08, logit diff = -16.807544708251953\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9817383885383606, logit diff = 6.6682634353637695\n",
      "\n",
      "\n",
      "loss.item()=0.3242437541484833, ablated_edges=11584\n",
      "loss.item()=5.438139915466309, ablated_edges=11429\n",
      "loss.item()=0.8652603030204773, ablated_edges=11547\n",
      "loss.item()=0.64545738697052, ablated_edges=11566\n",
      "loss.item()=0.4232422709465027, ablated_edges=11578\n",
      "loss.item()=0.4834759831428528, ablated_edges=11575\n",
      "loss.item()=0.4564094543457031, ablated_edges=11573\n",
      "loss.item()=0.3448494076728821, ablated_edges=11586\n",
      "loss.item()=0.42927423119544983, ablated_edges=11579\n",
      "loss.item()=0.28952664136886597, ablated_edges=11592\n",
      "Epochs trained:  140\n",
      "Loss: 0.2895\n",
      "Total preserved: 24.0253\n",
      "Edges ablated:  11592\n",
      "Toxic loss:  0.003625444369390607\n",
      "OWT loss:  11.008721351623535\n",
      "Penalty:  tensor(0.2859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 3.056105128962372e-07, logit diff = -14.902595043182373\n",
      "Best Token: [' Josh'], P(Alicia) = 0.026396479457616806, logit diff = -2.666161298751831\n",
      "\n",
      "\n",
      "loss.item()=0.23483148217201233, ablated_edges=11594\n",
      "loss.item()=0.21687330305576324, ablated_edges=11597\n",
      "loss.item()=0.4386284649372101, ablated_edges=11580\n",
      "loss.item()=0.31080949306488037, ablated_edges=11589\n",
      "loss.item()=0.9003008604049683, ablated_edges=11548\n",
      "loss.item()=0.47966518998146057, ablated_edges=11572\n",
      "loss.item()=0.404467910528183, ablated_edges=11583\n",
      "loss.item()=0.33508163690567017, ablated_edges=11585\n",
      "loss.item()=0.7432845830917358, ablated_edges=11565\n",
      "loss.item()=0.36927545070648193, ablated_edges=11587\n",
      "Epochs trained:  150\n",
      "Loss: 0.3693\n",
      "Total preserved: 27.4272\n",
      "Edges ablated:  11587\n",
      "Toxic loss:  0.015464536845684052\n",
      "OWT loss:  9.290637016296387\n",
      "Penalty:  tensor(0.3538, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Rebecca'], P(Alicia) = 0.0006466594059020281, logit diff = 1.0358333587646484\n",
      "Best Token: [' Rebecca'], P(Alicia) = 0.0006466594059020281, logit diff = 1.0358333587646484\n",
      "\n",
      "\n",
      "loss.item()=7.952764511108398, ablated_edges=11378\n",
      "loss.item()=2.214264392852783, ablated_edges=11438\n",
      "loss.item()=1.8681726455688477, ablated_edges=11473\n",
      "loss.item()=1.586867094039917, ablated_edges=11490\n",
      "loss.item()=1.4604147672653198, ablated_edges=11504\n",
      "loss.item()=2.297705888748169, ablated_edges=11513\n",
      "loss.item()=1.6076699495315552, ablated_edges=11498\n",
      "loss.item()=1.255759835243225, ablated_edges=11524\n",
      "loss.item()=1.1108698844909668, ablated_edges=11535\n",
      "loss.item()=1.155150294303894, ablated_edges=11534\n",
      "Epochs trained:  160\n",
      "Loss: 1.1552\n",
      "Total preserved: 80.2208\n",
      "Edges ablated:  11534\n",
      "Toxic loss:  0.04008099064230919\n",
      "OWT loss:  11.2357816696167\n",
      "Penalty:  tensor(1.1151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 2.9911431664864097e-10, logit diff = -21.738204956054688\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.8386899828910828, logit diff = 11.361824035644531\n",
      "\n",
      "\n",
      "loss.item()=0.9697514176368713, ablated_edges=11544\n",
      "loss.item()=0.8812726140022278, ablated_edges=11557\n",
      "loss.item()=0.8008664846420288, ablated_edges=11562\n",
      "loss.item()=1.1368324756622314, ablated_edges=11550\n",
      "loss.item()=10.227934837341309, ablated_edges=11537\n",
      "loss.item()=1.140871524810791, ablated_edges=11546\n",
      "loss.item()=1.8745945692062378, ablated_edges=11496\n",
      "loss.item()=1.0560941696166992, ablated_edges=11544\n",
      "loss.item()=0.8088220357894897, ablated_edges=11565\n",
      "loss.item()=0.664933979511261, ablated_edges=11573\n",
      "Epochs trained:  170\n",
      "Loss: 0.6649\n",
      "Total preserved: 44.0477\n",
      "Edges ablated:  11573\n",
      "Toxic loss:  0.008623731322586536\n",
      "OWT loss:  11.160869598388672\n",
      "Penalty:  tensor(0.6563, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 1.005485833482922e-10, logit diff = -22.903529167175293\n",
      "Best Token: [' Joshua'], P(Alicia) = 0.0023426313418895006, logit diff = -5.458438873291016\n",
      "\n",
      "\n",
      "loss.item()=2.0289626121520996, ablated_edges=11488\n",
      "loss.item()=1.302978754043579, ablated_edges=11557\n",
      "loss.item()=0.8447887897491455, ablated_edges=11561\n",
      "loss.item()=0.6674566864967346, ablated_edges=11575\n",
      "loss.item()=0.6680594682693481, ablated_edges=11579\n",
      "loss.item()=0.572058916091919, ablated_edges=11584\n",
      "loss.item()=0.4789956510066986, ablated_edges=11587\n",
      "loss.item()=3.3264124393463135, ablated_edges=11422\n",
      "loss.item()=1.1467397212982178, ablated_edges=11561\n",
      "loss.item()=0.626824140548706, ablated_edges=11575\n",
      "Epochs trained:  180\n",
      "Loss: 0.6268\n",
      "Total preserved: 38.2843\n",
      "Edges ablated:  11575\n",
      "Toxic loss:  0.01810305006802082\n",
      "OWT loss:  13.798935890197754\n",
      "Penalty:  tensor(0.6087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 4.805317602163406e-11, logit diff = -23.671403884887695\n",
      "Best Token: [' Alicia'], P(Alicia) = 0.9946331977844238, logit diff = 13.290856838226318\n",
      "\n",
      "\n",
      "loss.item()=0.48964259028434753, ablated_edges=11584\n",
      "loss.item()=5.9502434730529785, ablated_edges=11426\n",
      "loss.item()=1.3199518918991089, ablated_edges=11542\n",
      "loss.item()=0.8629011511802673, ablated_edges=11566\n",
      "loss.item()=1.6400808095932007, ablated_edges=11517\n",
      "loss.item()=1.8433305025100708, ablated_edges=11512\n",
      "loss.item()=1.7652515172958374, ablated_edges=11512\n",
      "loss.item()=1.465708613395691, ablated_edges=11541\n",
      "loss.item()=0.7586110830307007, ablated_edges=11571\n",
      "loss.item()=0.6187514066696167, ablated_edges=11579\n",
      "Epochs trained:  190\n",
      "Loss: 0.6188\n",
      "Total preserved: 36.4742\n",
      "Edges ablated:  11579\n",
      "Toxic loss:  0.0023381581995636225\n",
      "OWT loss:  14.12374496459961\n",
      "Penalty:  tensor(0.6164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' Joshua'], P(Alicia) = 5.0104580207044336e-11, logit diff = -23.71303367614746\n",
      "Best Token: [' Joshua'], P(Alicia) = 2.879820648615805e-08, logit diff = -17.359039306640625\n",
      "\n",
      "\n",
      "loss.item()=1.0060046911239624, ablated_edges=11576\n",
      "loss.item()=0.9298745393753052, ablated_edges=11576\n",
      "loss.item()=0.5202821493148804, ablated_edges=11588\n",
      "loss.item()=0.6002644300460815, ablated_edges=11588\n",
      "loss.item()=0.4651264250278473, ablated_edges=11592\n",
      "loss.item()=6.538536071777344, ablated_edges=11492\n",
      "loss.item()=1.03706693649292, ablated_edges=11568\n",
      "loss.item()=2.572803497314453, ablated_edges=11480\n",
      "loss.item()=0.8491005897521973, ablated_edges=11573\n",
      "loss.item()=7.128297805786133, ablated_edges=11432\n",
      "Epochs trained:  200\n",
      "Loss: 7.1283\n",
      "Total preserved: 205.7822\n",
      "Edges ablated:  11433\n",
      "Toxic loss:  3.4447968006134033\n",
      "OWT loss:  11.805241584777832\n",
      "Penalty:  tensor(3.6835, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Best Token: [' the'], P(Alicia) = 5.1743656513281167e-05, logit diff = -1.1193275451660156\n",
      "Best Token: [' the'], P(Alicia) = 7.252733485074714e-05, logit diff = -0.8684349060058594\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = (regularization_strength * penalty + alpha * tox_loss) #+ owt_loss\n",
    "            # loss = alpha * tox_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            \n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_ioi_sentences = [\"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\", \"While Joshua and Alicia were commuting to the restaurant, Joshua gave a snack to\"]\n",
    "                for test_ioi_sentence in test_ioi_sentences:\n",
    "                    correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "                    other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "                    test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "                    generation = model(test_ioi_tokens)[0][:, -1]\n",
    "                    probs = torch.softmax(generation, dim=-1)\n",
    "                    print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")\n",
    "            \n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            print(\"\\n\")\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    # epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    # log_every = int(input('set log frequency'))\n",
    "    # edge_threshold = int(input('set edge threshold'))\n",
    "    epochs_left = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"models/alternative_sufficient_masks_params_dict_lambda={regularization_strength}_{alpha=}_{means_ioi=}_{template_type=}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
       " tensor([1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0.]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 1., 0.]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_mask_params[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Token: [' Alicia'], P(Alicia) = 0.9056878089904785, logit diff = 2.4303483963012695\n",
      "Best Token: [' Joshua'], P(Alicia) = 2.5975340989248252e-08, logit diff = -17.38216209411621\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_ioi_sentences = [\"While Alicia and Joshua were commuting to the restaurant, Joshua gave a snack to\", \"While Joshua and Alicia were commuting to the restaurant, Joshua gave a snack to\"]\n",
    "    for test_ioi_sentence in test_ioi_sentences:\n",
    "        correct_token_id = tokenizer.encode(\" Alicia\", return_tensors=\"pt\").squeeze().item()\n",
    "        other_token_id = tokenizer.encode(\" Joshua\", return_tensors=\"pt\").squeeze().item()\n",
    "        test_ioi_tokens = tokenizer.encode(test_ioi_sentence, return_tensors=\"pt\").to('cuda')\n",
    "        generation = model(test_ioi_tokens)[0][:, -1]\n",
    "        probs = torch.softmax(generation, dim=-1)\n",
    "        print(f\"Best Token: {tokenizer.batch_decode(torch.argmax(generation, dim=-1))}, P(Alicia) = {probs[:,correct_token_id].item()}, logit diff = {generation[:,correct_token_id].item() - generation[:,other_token_id].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train mask over known circuit\n",
    "Train mask over the circuit from the paper, as given by a run of ACDC++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed': tensor([]),\n",
       " 'a0.0': tensor([1.]),\n",
       " 'a0.1': tensor([0.]),\n",
       " 'a0.2': tensor([1.]),\n",
       " 'a0.3': tensor([0.]),\n",
       " 'a0.4': tensor([1.]),\n",
       " 'a0.5': tensor([0.]),\n",
       " 'a0.6': tensor([1.]),\n",
       " 'a0.7': tensor([1.]),\n",
       " 'a0.8': tensor([1.]),\n",
       " 'a0.9': tensor([1.]),\n",
       " 'a0.10': tensor([0.]),\n",
       " 'a0.11': tensor([1.]),\n",
       " 'm0': tensor([0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.]),\n",
       " 'a1.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a1.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a2.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm2': tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1.]),\n",
       " 'a3.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0.]),\n",
       " 'a3.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'a3.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]),\n",
       " 'm3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a4.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "         1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.]),\n",
       " 'a5.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.]),\n",
       " 'a5.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a5.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         0., 1., 0., 0., 0., 1.]),\n",
       " 'a6.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 1.]),\n",
       " 'a6.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.9': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         0., 1., 1., 1., 1., 1., 0.]),\n",
       " 'a6.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a6.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
       "         1.]),\n",
       " 'a7.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.9': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0.]),\n",
       " 'a7.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'a7.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1.]),\n",
       " 'm7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.]),\n",
       " 'a8.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a8.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1.]),\n",
       " 'a9.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]),\n",
       " 'a9.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0.]),\n",
       " 'a9.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]),\n",
       " 'a9.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a9.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1.]),\n",
       " 'a10.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 1.]),\n",
       " 'a10.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.7': tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 1.]),\n",
       " 'a10.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'a10.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]),\n",
       " 'm10': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.0': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.1': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.2': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.3': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.4': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.5': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.6': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.7': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.8': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.9': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'a11.10': tensor([1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.]),\n",
       " 'a11.11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'm11': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'output': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mask_utils import get_nodes_and_edges\n",
    "with open(\"models/acdcpp_mask_params.pkl\", \"rb\") as f:\n",
    "    acdc_mask_params = pickle.load(f)\n",
    "\n",
    "_, _, acdc_Edges, acdc_mask_dict = get_nodes_and_edges(mask_params=acdc_mask_params)\n",
    "acdc_mask_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([157])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acdc_mask_dict['output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22863/1329432646.py:48: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for e in tqdm(range(epochs_left)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10673f0ad7234b04a3294330b8b5f2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=1.5056736469268799, ablated_edges=80\n",
      "loss.item()=-0.500084400177002, ablated_edges=87\n",
      "loss.item()=-0.0825948715209961, ablated_edges=87\n",
      "loss.item()=0.15455102920532227, ablated_edges=88\n",
      "loss.item()=0.7475423812866211, ablated_edges=95\n",
      "loss.item()=0.7848529815673828, ablated_edges=94\n",
      "loss.item()=0.06402826309204102, ablated_edges=98\n",
      "loss.item()=-0.9390511512756348, ablated_edges=96\n",
      "loss.item()=0.6501531600952148, ablated_edges=98\n",
      "loss.item()=0.626798152923584, ablated_edges=98\n",
      "Epochs trained:  10\n",
      "Loss: 0.6268\n",
      "Total preserved: 11512.8232\n",
      "Edges ablated:  98\n",
      "Toxic loss:  23.036602020263672\n",
      "OWT loss:  5.234118461608887\n",
      "Penalty:  0\n",
      "\n",
      "\n",
      "loss.item()=-3.0186614990234375, ablated_edges=103\n",
      "loss.item()=-0.8208189010620117, ablated_edges=93\n",
      "loss.item()=0.038724422454833984, ablated_edges=94\n",
      "loss.item()=-0.08931589126586914, ablated_edges=95\n",
      "loss.item()=-0.5814499855041504, ablated_edges=98\n",
      "loss.item()=-0.1508636474609375, ablated_edges=102\n",
      "loss.item()=2.3883941173553467, ablated_edges=98\n",
      "loss.item()=1.5119729042053223, ablated_edges=96\n",
      "loss.item()=-0.7001581192016602, ablated_edges=100\n",
      "loss.item()=-0.5811986923217773, ablated_edges=97\n",
      "Epochs trained:  20\n",
      "Loss: -0.5812\n",
      "Total preserved: 11511.8525\n",
      "Edges ablated:  97\n",
      "Toxic loss:  27.467458724975586\n",
      "OWT loss:  4.912292957305908\n",
      "Penalty:  0\n",
      "\n",
      "\n",
      "loss.item()=1.1091012954711914, ablated_edges=100\n",
      "loss.item()=-0.7198200225830078, ablated_edges=98\n",
      "loss.item()=-1.066877841949463, ablated_edges=100\n",
      "loss.item()=-4.244425296783447, ablated_edges=105\n",
      "loss.item()=-5.552443981170654, ablated_edges=98\n",
      "loss.item()=-6.534358024597168, ablated_edges=91\n",
      "loss.item()=-7.882603168487549, ablated_edges=89\n",
      "loss.item()=-8.790934562683105, ablated_edges=88\n",
      "loss.item()=-8.656744003295898, ablated_edges=87\n",
      "loss.item()=-8.970634460449219, ablated_edges=89\n",
      "Epochs trained:  30\n",
      "Loss: -8.9706\n",
      "Total preserved: 11524.6582\n",
      "Edges ablated:  89\n",
      "Toxic loss:  19.04596519470215\n",
      "OWT loss:  5.210751056671143\n",
      "Penalty:  tensor(10.3722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-10.965009689331055, ablated_edges=88\n",
      "loss.item()=-13.547128677368164, ablated_edges=90\n",
      "loss.item()=-13.717792510986328, ablated_edges=87\n",
      "loss.item()=-16.68755340576172, ablated_edges=86\n",
      "loss.item()=-16.444446563720703, ablated_edges=82\n",
      "loss.item()=-16.821895599365234, ablated_edges=82\n",
      "loss.item()=-17.48927116394043, ablated_edges=80\n",
      "loss.item()=-21.36284828186035, ablated_edges=81\n",
      "loss.item()=-21.52306365966797, ablated_edges=80\n",
      "loss.item()=-22.231990814208984, ablated_edges=77\n",
      "Epochs trained:  40\n",
      "Loss: -22.2320\n",
      "Total preserved: 11534.5127\n",
      "Edges ablated:  78\n",
      "Toxic loss:  26.378662109375\n",
      "OWT loss:  4.959319114685059\n",
      "Penalty:  tensor(21.9156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-22.69232940673828, ablated_edges=77\n",
      "loss.item()=-23.534677505493164, ablated_edges=78\n",
      "loss.item()=-25.442670822143555, ablated_edges=76\n",
      "loss.item()=-27.155630111694336, ablated_edges=79\n",
      "loss.item()=-28.782812118530273, ablated_edges=76\n",
      "loss.item()=-28.164342880249023, ablated_edges=77\n",
      "loss.item()=-29.027385711669922, ablated_edges=72\n",
      "loss.item()=-30.837371826171875, ablated_edges=80\n",
      "loss.item()=-32.40890121459961, ablated_edges=72\n",
      "loss.item()=-32.99948501586914, ablated_edges=72\n",
      "Epochs trained:  50\n",
      "Loss: -32.9995\n",
      "Total preserved: 11537.1387\n",
      "Edges ablated:  72\n",
      "Toxic loss:  25.70037269592285\n",
      "OWT loss:  5.598297119140625\n",
      "Penalty:  tensor(33.4577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-33.814735412597656, ablated_edges=70\n",
      "loss.item()=-34.977874755859375, ablated_edges=74\n",
      "loss.item()=-38.72045135498047, ablated_edges=76\n",
      "loss.item()=-38.59954833984375, ablated_edges=72\n",
      "loss.item()=-37.44197082519531, ablated_edges=75\n",
      "loss.item()=-40.775421142578125, ablated_edges=69\n",
      "loss.item()=-39.43312454223633, ablated_edges=69\n",
      "loss.item()=-43.922523498535156, ablated_edges=70\n",
      "loss.item()=-44.29679870605469, ablated_edges=63\n",
      "loss.item()=-44.6660270690918, ablated_edges=69\n",
      "Epochs trained:  60\n",
      "Loss: -44.6660\n",
      "Total preserved: 11543.3457\n",
      "Edges ablated:  68\n",
      "Toxic loss:  24.17945098876953\n",
      "OWT loss:  5.188914775848389\n",
      "Penalty:  tensor(45.0191, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-45.43226623535156, ablated_edges=68\n",
      "loss.item()=-47.554931640625, ablated_edges=67\n",
      "loss.item()=-48.48405456542969, ablated_edges=70\n",
      "loss.item()=-49.8265495300293, ablated_edges=67\n",
      "loss.item()=-51.74678421020508, ablated_edges=66\n",
      "loss.item()=-51.73463821411133, ablated_edges=69\n",
      "loss.item()=-52.265323638916016, ablated_edges=70\n",
      "loss.item()=-54.762516021728516, ablated_edges=71\n",
      "loss.item()=-55.7078971862793, ablated_edges=68\n",
      "loss.item()=-56.24128723144531, ablated_edges=67\n",
      "Epochs trained:  70\n",
      "Loss: -56.2413\n",
      "Total preserved: 11541.6113\n",
      "Edges ablated:  67\n",
      "Toxic loss:  23.93475341796875\n",
      "OWT loss:  5.099553108215332\n",
      "Penalty:  tensor(56.5539, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-57.00583267211914, ablated_edges=69\n",
      "loss.item()=-57.7839469909668, ablated_edges=70\n",
      "loss.item()=-58.821163177490234, ablated_edges=67\n",
      "loss.item()=-61.0757942199707, ablated_edges=66\n",
      "loss.item()=-63.463558197021484, ablated_edges=66\n",
      "loss.item()=-64.74162292480469, ablated_edges=62\n",
      "loss.item()=-65.59223175048828, ablated_edges=61\n",
      "loss.item()=-65.37863159179688, ablated_edges=66\n",
      "loss.item()=-66.93404388427734, ablated_edges=61\n",
      "loss.item()=-67.35196685791016, ablated_edges=66\n",
      "Epochs trained:  80\n",
      "Loss: -67.3520\n",
      "Total preserved: 11547.4160\n",
      "Edges ablated:  64\n",
      "Toxic loss:  22.907915115356445\n",
      "OWT loss:  5.359364986419678\n",
      "Penalty:  tensor(68.1298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-70.22889709472656, ablated_edges=59\n",
      "loss.item()=-69.60752868652344, ablated_edges=62\n",
      "loss.item()=-71.21782684326172, ablated_edges=64\n",
      "loss.item()=-72.4986801147461, ablated_edges=64\n",
      "loss.item()=-74.5802001953125, ablated_edges=66\n",
      "loss.item()=-74.36848449707031, ablated_edges=65\n",
      "loss.item()=-76.47356414794922, ablated_edges=63\n",
      "loss.item()=-78.4028549194336, ablated_edges=63\n",
      "loss.item()=-77.62654876708984, ablated_edges=66\n",
      "loss.item()=-80.140625, ablated_edges=64\n",
      "Epochs trained:  90\n",
      "Loss: -80.1406\n",
      "Total preserved: 11547.1406\n",
      "Edges ablated:  64\n",
      "Toxic loss:  23.180822372436523\n",
      "OWT loss:  4.170817852020264\n",
      "Penalty:  tensor(79.6753, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-82.50818634033203, ablated_edges=66\n",
      "loss.item()=-81.01814270019531, ablated_edges=64\n",
      "loss.item()=-83.3720474243164, ablated_edges=66\n",
      "loss.item()=-83.98469543457031, ablated_edges=58\n",
      "loss.item()=-87.44886016845703, ablated_edges=61\n",
      "loss.item()=-86.34146881103516, ablated_edges=58\n",
      "loss.item()=-85.54612731933594, ablated_edges=57\n",
      "loss.item()=-90.238525390625, ablated_edges=54\n",
      "loss.item()=-90.92143249511719, ablated_edges=54\n",
      "loss.item()=-92.84722137451172, ablated_edges=57\n",
      "Epochs trained:  100\n",
      "Loss: -92.8472\n",
      "Total preserved: 11554.1641\n",
      "Edges ablated:  57\n",
      "Toxic loss:  33.43305587768555\n",
      "OWT loss:  5.11730432510376\n",
      "Penalty:  tensor(91.2779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-91.27748107910156, ablated_edges=59\n",
      "loss.item()=-93.88461303710938, ablated_edges=54\n",
      "loss.item()=-94.77798461914062, ablated_edges=50\n",
      "loss.item()=-94.69718933105469, ablated_edges=52\n",
      "loss.item()=-95.52283477783203, ablated_edges=53\n",
      "loss.item()=-97.91181182861328, ablated_edges=58\n",
      "loss.item()=-100.29170989990234, ablated_edges=53\n",
      "loss.item()=-101.28544616699219, ablated_edges=52\n",
      "loss.item()=-99.9444808959961, ablated_edges=54\n",
      "loss.item()=-102.86627197265625, ablated_edges=53\n",
      "Epochs trained:  110\n",
      "Loss: -102.8663\n",
      "Total preserved: 11557.9971\n",
      "Edges ablated:  53\n",
      "Toxic loss:  28.86589813232422\n",
      "OWT loss:  5.773064613342285\n",
      "Penalty:  tensor(102.8662, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-103.58384704589844, ablated_edges=52\n",
      "loss.item()=-105.84606170654297, ablated_edges=52\n",
      "loss.item()=-107.2335433959961, ablated_edges=53\n",
      "loss.item()=-106.85431671142578, ablated_edges=53\n",
      "loss.item()=-109.91695404052734, ablated_edges=52\n",
      "loss.item()=-110.24185943603516, ablated_edges=54\n",
      "loss.item()=-111.40071105957031, ablated_edges=50\n",
      "loss.item()=-112.56986999511719, ablated_edges=50\n",
      "loss.item()=-113.40050506591797, ablated_edges=53\n",
      "loss.item()=-113.72864532470703, ablated_edges=52\n",
      "Epochs trained:  120\n",
      "Loss: -113.7286\n",
      "Total preserved: 11557.8770\n",
      "Edges ablated:  52\n",
      "Toxic loss:  28.943449020385742\n",
      "OWT loss:  6.483026027679443\n",
      "Penalty:  tensor(114.4230, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-116.54255676269531, ablated_edges=53\n",
      "loss.item()=-116.79327392578125, ablated_edges=51\n",
      "loss.item()=-115.19482421875, ablated_edges=51\n",
      "loss.item()=-118.75401306152344, ablated_edges=53\n",
      "loss.item()=-122.14277648925781, ablated_edges=49\n",
      "loss.item()=-121.06953430175781, ablated_edges=48\n",
      "loss.item()=-122.05601501464844, ablated_edges=49\n",
      "loss.item()=-123.47509002685547, ablated_edges=48\n",
      "loss.item()=-124.31399536132812, ablated_edges=50\n",
      "loss.item()=-127.36456298828125, ablated_edges=46\n",
      "Epochs trained:  130\n",
      "Loss: -127.3646\n",
      "Total preserved: 11562.5859\n",
      "Edges ablated:  46\n",
      "Toxic loss:  32.48900604248047\n",
      "OWT loss:  5.165423393249512\n",
      "Penalty:  tensor(126.0322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-126.47238159179688, ablated_edges=47\n",
      "loss.item()=-128.5781707763672, ablated_edges=49\n",
      "loss.item()=-130.5258331298828, ablated_edges=46\n",
      "loss.item()=-129.74996948242188, ablated_edges=47\n",
      "loss.item()=-132.48599243164062, ablated_edges=48\n",
      "loss.item()=-133.76837158203125, ablated_edges=47\n",
      "loss.item()=-132.95957946777344, ablated_edges=46\n",
      "loss.item()=-133.96261596679688, ablated_edges=45\n",
      "loss.item()=-135.52638244628906, ablated_edges=47\n",
      "loss.item()=-137.492431640625, ablated_edges=50\n",
      "Epochs trained:  140\n",
      "Loss: -137.4924\n",
      "Total preserved: 11560.9385\n",
      "Edges ablated:  50\n",
      "Toxic loss:  27.15222930908203\n",
      "OWT loss:  5.513188362121582\n",
      "Penalty:  tensor(137.5752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-138.09088134765625, ablated_edges=46\n",
      "loss.item()=-139.6414031982422, ablated_edges=46\n",
      "loss.item()=-140.9293975830078, ablated_edges=45\n",
      "loss.item()=-141.51370239257812, ablated_edges=49\n",
      "loss.item()=-144.06930541992188, ablated_edges=46\n",
      "loss.item()=-143.3538055419922, ablated_edges=47\n",
      "loss.item()=-146.1291046142578, ablated_edges=44\n",
      "loss.item()=-146.28192138671875, ablated_edges=47\n",
      "loss.item()=-148.14944458007812, ablated_edges=48\n",
      "loss.item()=-150.17247009277344, ablated_edges=50\n",
      "Epochs trained:  150\n",
      "Loss: -150.1725\n",
      "Total preserved: 11561.4805\n",
      "Edges ablated:  50\n",
      "Toxic loss:  33.0401725769043\n",
      "OWT loss:  5.578667640686035\n",
      "Penalty:  tensor(149.1431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-152.55580139160156, ablated_edges=49\n",
      "loss.item()=-151.85586547851562, ablated_edges=49\n",
      "loss.item()=-153.99522399902344, ablated_edges=46\n",
      "loss.item()=-153.05007934570312, ablated_edges=47\n",
      "loss.item()=-155.26657104492188, ablated_edges=45\n",
      "loss.item()=-155.52505493164062, ablated_edges=49\n",
      "loss.item()=-158.27174377441406, ablated_edges=46\n",
      "loss.item()=-158.82046508789062, ablated_edges=43\n",
      "loss.item()=-159.7530059814453, ablated_edges=48\n",
      "loss.item()=-161.92526245117188, ablated_edges=46\n",
      "Epochs trained:  160\n",
      "Loss: -161.9253\n",
      "Total preserved: 11562.2822\n",
      "Edges ablated:  46\n",
      "Toxic loss:  33.64940643310547\n",
      "OWT loss:  5.520321846008301\n",
      "Penalty:  tensor(160.7157, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-161.7630157470703, ablated_edges=43\n",
      "loss.item()=-162.72824096679688, ablated_edges=43\n",
      "loss.item()=-163.37709045410156, ablated_edges=47\n",
      "loss.item()=-164.8791961669922, ablated_edges=44\n",
      "loss.item()=-167.94241333007812, ablated_edges=45\n",
      "loss.item()=-168.17257690429688, ablated_edges=46\n",
      "loss.item()=-168.0404052734375, ablated_edges=47\n",
      "loss.item()=-169.7171630859375, ablated_edges=49\n",
      "loss.item()=-171.64768981933594, ablated_edges=46\n",
      "loss.item()=-174.03814697265625, ablated_edges=43\n",
      "Epochs trained:  170\n",
      "Loss: -174.0381\n",
      "Total preserved: 11564.3203\n",
      "Edges ablated:  43\n",
      "Toxic loss:  39.49759292602539\n",
      "OWT loss:  6.16975736618042\n",
      "Penalty:  tensor(172.3084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-174.89840698242188, ablated_edges=45\n",
      "loss.item()=-174.6654510498047, ablated_edges=48\n",
      "loss.item()=-174.72088623046875, ablated_edges=45\n",
      "loss.item()=-178.4546661376953, ablated_edges=46\n",
      "loss.item()=-177.6720428466797, ablated_edges=47\n",
      "loss.item()=-179.65830993652344, ablated_edges=48\n",
      "loss.item()=-179.35630798339844, ablated_edges=41\n",
      "loss.item()=-183.06385803222656, ablated_edges=44\n",
      "loss.item()=-184.6969757080078, ablated_edges=46\n",
      "loss.item()=-184.05078125, ablated_edges=44\n",
      "Epochs trained:  180\n",
      "Loss: -184.0508\n",
      "Total preserved: 11566.2373\n",
      "Edges ablated:  44\n",
      "Toxic loss:  28.797325134277344\n",
      "OWT loss:  5.611840724945068\n",
      "Penalty:  tensor(183.9032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-184.4897003173828, ablated_edges=47\n",
      "loss.item()=-185.37271118164062, ablated_edges=46\n",
      "loss.item()=-186.9844207763672, ablated_edges=43\n",
      "loss.item()=-188.4407501220703, ablated_edges=46\n",
      "loss.item()=-189.3105010986328, ablated_edges=44\n",
      "loss.item()=-191.02035522460938, ablated_edges=44\n",
      "loss.item()=-191.65814208984375, ablated_edges=43\n",
      "loss.item()=-191.11868286132812, ablated_edges=45\n",
      "loss.item()=-195.22610473632812, ablated_edges=44\n",
      "loss.item()=-195.29010009765625, ablated_edges=44\n",
      "Epochs trained:  190\n",
      "Loss: -195.2901\n",
      "Total preserved: 11565.8291\n",
      "Edges ablated:  44\n",
      "Toxic loss:  25.891029357910156\n",
      "OWT loss:  5.3506364822387695\n",
      "Penalty:  tensor(195.4625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "loss.item()=-197.07337951660156, ablated_edges=46\n",
      "loss.item()=-197.45147705078125, ablated_edges=47\n",
      "loss.item()=-199.31268310546875, ablated_edges=44\n",
      "loss.item()=-198.38023376464844, ablated_edges=43\n",
      "loss.item()=-201.58419799804688, ablated_edges=44\n",
      "loss.item()=-199.99661254882812, ablated_edges=44\n",
      "loss.item()=-202.88490295410156, ablated_edges=46\n",
      "loss.item()=-203.29649353027344, ablated_edges=44\n",
      "loss.item()=-205.46502685546875, ablated_edges=41\n",
      "loss.item()=-207.99468994140625, ablated_edges=43\n",
      "Epochs trained:  200\n",
      "Loss: -207.9947\n",
      "Total preserved: 11567.3525\n",
      "Edges ablated:  43\n",
      "Toxic loss:  30.689029693603516\n",
      "OWT loss:  5.198721885681152\n",
      "Penalty:  tensor(207.0556, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m     prev_params \u001b[39m=\u001b[39m mask_params\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m epochs_left \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39minput\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mcontinue training for this number of epochs: \u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m log_every \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(\u001b[39minput\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mset log frequency\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/phil/deep_learning/mechanistic-unlearning/circuit-breaking/ioi/alternative_masks.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m edge_threshold \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39minput\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mset edge threshold\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "toxic_batch_size = 1 # so that we can just access the last sequence position without worrying about padding\n",
    "owt_batch_size = 5\n",
    "context_length = CONTEXT_LENGTH\n",
    "\n",
    "toxic_data_loader = retrieve_toxic_data(toxic_batch_size, context_length, tokenizer, tokenize=False, num_points=None)\n",
    "# toxic_data_loader = retrieve_toxic_filtered_data(toxic_batch_size)\n",
    "owt_data_loader = retrieve_owt_data(owt_batch_size)\n",
    "\n",
    "with open(\"data/gpt2_means.pkl\", \"rb\") as f:\n",
    "    means = pickle.load(f)[0][0]\n",
    "\n",
    "model = load_demo_gpt2(means=means, mask_dict_superset=acdc_mask_dict)\n",
    "epochs_left = 200\n",
    "log_every = 10\n",
    "lr = .05 # free\n",
    "weight_decay = 0\n",
    "clamp_every = 20 # 5 # free\n",
    "threshold = 0.5\n",
    "epochs_trained = 0\n",
    "regularization_strength = 1 # free\n",
    "\n",
    "mask_params = []\n",
    "param_names = []\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        param_names.append(name)\n",
    "        mask_params.append(p)\n",
    "optimizer = AdamW(mask_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "losses = []\n",
    "num_ablated_edges = []\n",
    "alpha = 0.2 # free\n",
    "batch_size = toxic_batch_size + owt_batch_size\n",
    "demos = prepare_fixed_demo(tokenizer, batch_size, demo=\"\")\n",
    "owt_iter = cycle(owt_data_loader)\n",
    "edge_threshold = 0\n",
    "max_steps_per_epoch = 100\n",
    "\n",
    "old_mask_params = {}\n",
    "def duplicate_mask_params(mask_params):\n",
    "    new_mask_params = []\n",
    "    for p in mask_params:\n",
    "        new_mask_params.append(p.data.cpu())\n",
    "    return new_mask_params\n",
    "\n",
    "prev_params = None\n",
    "while epochs_left >= 0:\n",
    "    for e in tqdm(range(epochs_left)):\n",
    "        for c, batch in enumerate(toxic_data_loader):\n",
    "            if c > max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "            # print(batch[\"text\"])\n",
    "            total_preserving = 0\n",
    "            ablated_edges = 0\n",
    "            penalty = 0\n",
    "            for p in mask_params:\n",
    "                total_preserving += p.sum()\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "                penalty += max(0, p.sum() * (epochs_trained-20) / 10000) # why 2000? free\n",
    "            # print(f\"{ablated_edges=}\")\n",
    "            # demos = batch[:, :FILTER_DEMO_LEN]\n",
    "            # completions = batch[:, FILTER_DEMO_LEN:]\n",
    "\n",
    "            # tox_loss = infer_batch(model, criterion, completions, toxic_batch_size, demos)\n",
    "            # owt_loss = infer_batch(model, criterion, next(owt_iter)['tokens'], owt_batch_size, fixed_demos)\n",
    "            tox_loss, owt_loss = infer_batch_with_owt(model, criterion, batch, next(owt_iter), batch_size, demos, access_toxic_pos=-1)\n",
    "            # print(f\"{tox_loss=}, {owt_loss=}\")\n",
    "            loss = -1 * (regularization_strength * penalty + alpha * tox_loss) + owt_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            num_ablated_edges.append(ablated_edges)\n",
    "            for p in mask_params:\n",
    "                p.data.clamp_(0,1)\n",
    "        print(f\"{loss.item()=}, {ablated_edges=}\")\n",
    "        epochs_trained += 1\n",
    "        if epochs_trained % clamp_every == 0:\n",
    "            ablated_edges = 0\n",
    "            for p in mask_params:\n",
    "                p.data[p.data < threshold] = 0\n",
    "                p.data[p.data >= threshold] = 1\n",
    "                ablated_edges += p[p.data < 0.5].shape[0]\n",
    "        if epochs_trained % log_every == 0:\n",
    "            print(\"Epochs trained: \", epochs_trained)\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Total preserved: {total_preserving:.4f}\")\n",
    "            print(\"Edges ablated: \", ablated_edges)\n",
    "            print(\"Toxic loss: \", tox_loss.item())\n",
    "            print(\"OWT loss: \", owt_loss.item())\n",
    "            print(\"Penalty: \", penalty)\n",
    "            # if input('evaluate? (y)') == 'y':\n",
    "            #     evaluate_model(model, toxic_batches=1, owt_batches=1)\n",
    "            print(\"\\n\")\n",
    "            old_mask_params[epochs_trained] = duplicate_mask_params(mask_params)\n",
    "                \n",
    "        if epochs_trained > 50 and ablated_edges < edge_threshold:\n",
    "            break\n",
    "        prev_params = mask_params\n",
    "    epochs_left = int(input('continue training for this number of epochs: '))\n",
    "    log_every = int(input('set log frequency'))\n",
    "    edge_threshold = int(input('set edge threshold'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/alternative_acdc_cb_subset_mask_params_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(old_mask_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model before and after circuit breaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/ioi_sentences_test.pkl\", \"rb\") as f:\n",
    "    ioi_sentences_test = pickle.load(f)\n",
    "    # ioi_sentences_test = [t[2] for t in ioi_sentences_test]\n",
    "\n",
    "with open(\"data/eval_uniform.pkl\", \"rb\") as f:\n",
    "    uniform_samples = pickle.load(f)\n",
    "    uniform_sentences = [t[2] for t in uniform_samples]\n",
    "\n",
    "original_model = load_demo_gpt2(means=False)\n",
    "\n",
    "# with open(\"models/masked_gpt2_mean_ablation_v6.pkl\", \"rb\") as f:\n",
    "#     model.state_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on an ioi_sentence\n",
    "ioi_sentence = ioi_sentences_test[0]\n",
    "print(ioi_sentence)\n",
    "# ioi_tokens = tokenizer(ioi_sentence, return_tensors='pt').input_ids.to('cuda')\n",
    "\n",
    "original_model.eval()\n",
    "original_model.to('cuda')\n",
    "def get_last_token(model, prompt, topk=5):\n",
    "    # generate last token\n",
    "    tokens = tokenizer(prompt, return_tensors='pt').input_ids[:, :-1]\n",
    "\n",
    "    # generate one token, decode original_model(ioi_tokens[:, :-1])\n",
    "    model_outputs = model(tokens)[0]\n",
    "    model_outputs = model_outputs.squeeze(0)[-1]\n",
    "    probs = torch.nn.functional.softmax(model_outputs, dim=-1)\n",
    "\n",
    "    topk_outputs = torch.topk(model_outputs, topk)\n",
    "    topk_tokens = topk_outputs.indices\n",
    "    topk_probs = probs[topk_outputs.indices]\n",
    "    \n",
    "    # decode tokens\n",
    "    for i in range(topk):\n",
    "        print(f\"{tokenizer.decode(topk_tokens[i].unsqueeze(0))}, probability of {topk_probs[i]}\")\n",
    "    topk_tokens_decoded = tokenizer.batch_decode(topk_tokens)\n",
    "    return topk_tokens_decoded, topk_probs\n",
    "\n",
    "print(\"Before ablation\")\n",
    "_ = get_last_token(original_model, ioi_sentence)\n",
    "print()\n",
    "print()\n",
    "print(\"After ablation\")\n",
    "_ = get_last_token(model, ioi_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try on uniform samples\n",
    "for idx in range(3):\n",
    "    print(uniform_sentences[idx])\n",
    "    print(\"Before ablation\")\n",
    "    _ = get_last_token(original_model, uniform_sentences[idx])\n",
    "    print()\n",
    "    print(\"After ablation\")\n",
    "    _ = get_last_token(model, uniform_sentences[idx])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize mask\n",
    "Create the computational graphs in edge attribution patching paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate which nodes will be in the graph\n",
    "connected_nodes = set()\n",
    "# add embed node at position\n",
    "# connected_nodes.add((-1, \"embed\"))\n",
    "n_heads = 12\n",
    "n_layers = 12\n",
    "\n",
    "# associate each node with a position\n",
    "all_possible_nodes = [(-1, \"embed\")]\n",
    "mask_dict = {}\n",
    "# empty tensor\n",
    "mask_dict[\"embed\"] = torch.zeros(size=(0,))\n",
    "for idx in range(len(mask_params)):\n",
    "    if \"attention\" in param_names[idx]:\n",
    "        layer = int(param_names[idx].split(\".\")[1])\n",
    "        for i in range(n_heads):\n",
    "            all_possible_nodes.append((layer, f\"a{layer}.{i}\"))\n",
    "            mask_dict[f\"a{layer}.{i}\"] = mask_params[idx][:,i].detach().cpu()\n",
    "    elif \"mlp\" in param_names[idx]:\n",
    "        layer = int(param_names[idx].split(\".\")[1])\n",
    "        all_possible_nodes.append((layer, f\"m{layer}\"))\n",
    "        mask_dict[f\"m{layer}\"] = mask_params[idx].detach().cpu()\n",
    "all_possible_nodes.append((n_heads, \"output\"))\n",
    "mask_dict[\"output\"] = mask_params[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate where edges are based on the mask\n",
    "# Edge between node i and node j if mask_dict[i][all_possible_nodes.index(j)] == 0\n",
    "sufficient = True\n",
    "\n",
    "edges = []\n",
    "for i in range(len(all_possible_nodes)):\n",
    "    for j in range(len(all_possible_nodes)):\n",
    "        j_index = all_possible_nodes.index(all_possible_nodes[j])\n",
    "        if j_index < len(mask_dict[all_possible_nodes[i][1]]) and mask_dict[all_possible_nodes[i][1]][all_possible_nodes.index(all_possible_nodes[j])] == (1 if sufficient else 0):\n",
    "            edges.append((all_possible_nodes[i], all_possible_nodes[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aligned_graph(all_possible_nodes, edges):\n",
    "    G = pgv.AGraph(strict=False, directed=True)\n",
    "\n",
    "    # Find the maximum layer number for adjusting the graph\n",
    "    max_layer = max(layer for layer, _ in all_possible_nodes if isinstance(layer, int))\n",
    "    nodes_with_edges = set([node for edge in edges for node in edge])\n",
    "    print(nodes_with_edges)\n",
    "    # Add nodes and edges to the graph\n",
    "    # for node in all_possible_nodes:\n",
    "    #     if node in [edge[0] for edge in edges] or node in [edge[1] for edge in edges]:\n",
    "    #         G.add_node(node[1], layer=str(max_layer - node[0]))\n",
    "\n",
    "    for edge in edges:\n",
    "        G.add_edge(edge[1][1], edge[0][1])\n",
    "\n",
    "    # Create subgraphs to ensure nodes of the same layer have the same rank\n",
    "    for layer in range(max_layer, -2, -1):\n",
    "        with G.subgraph(name=f'cluster_{layer}') as s:\n",
    "            s.graph_attr['rank'] = 'same'\n",
    "            for node in nodes_with_edges:\n",
    "                if node[0] == layer:\n",
    "                    s.add_node(node[1])\n",
    "\n",
    "    # Apply layout and render the graph\n",
    "    G.layout(prog='dot')\n",
    "    G.draw('aligned_graph.png')\n",
    "    return Image('aligned_graph.png')\n",
    "\n",
    "# Call the function with your nodes and edges\n",
    "flipped_graph_image = create_aligned_graph(all_possible_nodes, edges)\n",
    "\n",
    "# To display the graph in Jupyter Notebook\n",
    "flipped_graph_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "unlrn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
